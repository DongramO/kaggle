import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from catboost import CatBoostClassifier
from category_encoders import CatBoostEncoder
import lightgbm as lgb
import xgboost as xgb
from scipy.optimize import minimize
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
 
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix
 
import optuna
from sklearn.model_selection import StratifiedKFold, cross_val_score
 
# ìƒëŒ€ ê²½ë¡œ ì‚¬ìš© (ê°™ì€ í´ë”ì— ìˆëŠ” íŒŒì¼ ì°¸ì¡°)
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
df_sub = pd.read_csv('sample_submission.csv')

optuna.logging.set_verbosity(optuna.logging.WARNING)

def eda(df_train, df_test, df_sub):
    print(df_train.head())
    
    print('df_train.shape', df_train.shape)
    print('df_test.shape', df_test.shape)
    
    print(df_train.info())
    print(df_test.info())
    print(df_train.describe())
    
    
    print('df_train.shape:', df_train.shape)
    print('df_test.shape:', df_test.shape)
    
    
    print(df_train.info())
    print(df_test.info())
    print(df_train.describe())
    
    cols = ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score',
        'loan_amount', 'interest_rate', 'gender', 'marital_status',
        'education_level', 'employment_status', 'loan_purpose',
        'grade_subgrade', 'loan_paid_back']
    
    for col in cols:
        print(col, df_train[col].nunique())
    
    
    num_cols = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']

    cat_cols = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']
    
    # Target Distribution Visualization
    counts = df_train['loan_paid_back'].value_counts()
    labels = counts.index
    values = counts.values
    
    plt.figure(figsize=(15,5.5))
    
    bars = plt.barh(labels, values, color = 'crimson')
    plt.ylabel("loan_paid_back")
    plt.xlabel("Frequency")
    plt.title("The Distribution of the Target Column 'loan_paid_back'")
    plt.yticks([1, 0])
    
    total = values.sum()
    for bar, count in zip(bars, values):
        width = bar.get_width()
        pct = count / total * 100
        plt.text(width, bar.get_y() + bar.get_height()/2,
                f"{count}\n({pct:.1f}%)",
                ha='left', va='center')
    # plt.show()
    
    
    # Data dirtribution visualization
    n_vars = len(num_cols)
    fig, axes = plt.subplots(n_vars, 2, figsize=(12, n_vars*3))
    for i, col in enumerate(num_cols):
        axes[i,0].hist(df_train[col], bins=50, edgecolor='black', color='crimson')
        axes[i,0].set_title(f'{col} histogram')
        axes[i,1].boxplot(df_train[col], vert=False)
        axes[i,1].set_title(f'{col} boxplot')
    
    plt.tight_layout()
    # plt.show()
    
    
    # outliers
    
    n_vars = len(num_cols)
    n_cols = 2
    n_rows = (n_vars + 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_vars * 3))
    
    for i, col in enumerate(num_cols):
        row = i // 2
        col_idx = i % 2
        sns.boxplot(x='loan_paid_back', y=col, data=df_train, ax=axes[row, col_idx], palette='pastel')
        axes[row, col_idx].set_title(f'{col} by loan_paid_back')
    
    if n_vars % 2 !=0:
        fig.delaxes(axes[n_rows-1, 1])
    
    plt.tight_layout()
    # plt.show()
    
    
    
    n_vars = len(cat_cols)
    fig, axes = plt.subplots(n_vars, 2, figsize=(14, n_vars*8))
    
    for i, col in enumerate(cat_cols):
        sns.countplot(x=df_train[col], ax=axes[i,0], order=df_train[col].value_counts().index, palette='pastel')
        axes[i,0].set_title(f'{col} countplot')
        axes[i,0].set_xlabel('')
        axes[i,0].set_ylabel('Count')
        axes[i,0].tick_params(axis='x', rotation=45)
    
        df_train[col].value_counts().plot.pie(
            ax=axes[i, 1],
            autopct='%1.1f%%',
            startangle=90,
            colors=sns.color_palette('pastel'),
            legend=False,
            ylabel=''
        )
        axes[i,1].set_title(f'{col} Pie chart')
    
    plt.tight_layout()
    # plt.show()
    
    n_vars = len(cat_cols)
    n_cols = 2
    n_rows = (n_vars + 1) // 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))
    
    for i, col in enumerate(cat_cols):
        row  = i // 2
        col_idx = i % 2
        sns.countplot(x=col, hue='loan_paid_back', data=df_train, ax=axes[row, col_idx], palette='pastel')
        axes[row, col_idx].set_title(f"{col} by loan_paid_back")
        axes[row, col_idx].tick_params(axis='x', rotation=45)
        axes[row, col_idx].set_xlabel(col)
        axes[row, col_idx].set_ylabel('Count')
    
    if n_vars % 2 != 0:
        fig.delaxes(axes[n_rows - 1, 1])
    
    plt.tight_layout()
    # plt.show()
    
    
    n_vars = len(cat_cols)
    n_cols = 2
    n_rows = (n_vars + 1) // 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))
    
    for i, col in enumerate(cat_cols):
        row = i // 2
        col_idx = i % 2
    
        ratio = (
            df_train.groupby(col)['loan_paid_back']
            .value_counts(normalize=True)
            .rename('ratio')
            .mul(100)
            .reset_index()
        )
    
        ratio = ratio[ratio['loan_paid_back'] == 0]
    
        sns.barplot(
            data=ratio,
            x=col,
            y='ratio',
            ax=axes[row, col_idx],
            palette='pastel'
        )
    
        axes[row, col_idx].set_title(f"{col}: % Not Paid Back")
        axes[row, col_idx].set_xlabel(col)
        axes[row, col_idx].set_ylabel('% Not Paid Back')
        axes[row, col_idx].tick_params(axis='x', rotation=45)
        axes[row, col_idx].bar_label(axes[row, col_idx].containers[0], fmt='%.1f%%', label_type='edge', fontsize=9)
    
    if n_vars % 2 != 0:
        fig.delaxes(axes[n_rows - 1, 1])
    
    plt.tight_layout()
    # plt.show()

rs = 42
model_types = ["catboost", "lgbm", "xgb"]

def data_preprocessing(df_array, target_col, df_num_cols, df_cat_cols):
    
    def log_regularization(df_array):
        
        for df in df_array:

            df["loan_amount"] = np.log1p(df["loan_amount"])
            # df["interest_rate"] = np.log1p(df["interest_rate"])
            df["annual_income"] = np.log1p(df["annual_income"])    
        
        return df_array
    
    def remove_outliers(df_array, cols):

        for df in df_array:

            for col in cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3- Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                # lowerì™€ upperë¥¼ êµ¬í•˜ê³  ì‚¬ì´ì˜ ê°’ì„ ì‚¬ìš©í•¨ ì •ìƒì ì¸ ê°’ë“¤ë§Œ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ì˜ì§€
                df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
                
        return df_array

    def remove_data_identify(df_array):
        
        for df in df_array:
            df.drop(columns=['id'], inplace=True)
        
        return df_array

    def feature_engineering(df_array):
        
        eps = 1e-6
        clip_rules = {
            "annual_income": 0.995,
            "loan_amount": 0.995,
            "debt_to_income_ratio": 0.99,
            "interest_rate": 0.995,
            "credit_score": 0.99
        }

        for df in df_array:
            for col, q in clip_rules.items():
                upper = df[col].quantile(q)

                if col == "credit_score":
                    lower = df[col].quantile(1 - q)
                    df[col] = df[col].clip(lower=lower, upper=upper)
                else:
                    df[col] = df[col].clip(upper=upper)

            df["annual_income"] = df["annual_income"].clip(lower=eps)
            df["loan_amount"]   = df["loan_amount"].clip(lower=eps)
            df["interest_rate"] = df["interest_rate"].clip(lower=eps)
            df["credit_score"]  = df["credit_score"].clip(lower=300)
            # 1. interest_rate / debt_to_income_ratio
            df["dti_to_interest_rate"] = (df["debt_to_income_ratio"] / (df["interest_rate"]+ eps))
            
            # 2. education_level & loan_purpose
            # df["loan_purpose_interest_rate"] = df["loan_purpose"].astype(str) + "_" + np.log1p(df["interest_rate"].round(1)).astype(str)
            
            # 3. employment_status & loan_purpose
            # df['employment_status_grade_subgrade'] = df['employment_status'].astype(str) + '_' + df['grade_subgrade'].astype(str)
            df["employment_loan_purpose"] = df["employment_status"].astype(str) + "_" + df["loan_purpose"].astype(str)
            # df["education_loan_purpose"] = df["education_level"].astype(str) + "_" + df["loan_purpose"].astype(str)

            # 4. monthly_income 
            df["monthly_income"] = df["annual_income"] / 12
            df["debt_to_monthly_income"] = df["debt_to_income_ratio"] / (df["monthly_income"] + eps)
            # df["monthly_income_interest_amount"] = df["monthly_income"] / ( df["interest_rate"] * df["loan_amount"] / 12)
            df["estimated_monthly_payment"] = (df["loan_amount"] * df["interest_rate"]) / 12
            df["pti_ratio"] = df["estimated_monthly_payment"] / (df["monthly_income"] + eps)
            
            # 5. education_level & grade_subgrade
            # df["education_grade_subgrade"] = df["education_level"].astype(str) + "_" + df["grade_subgrade"].astype(str)
            df["head_grade"] = df["grade_subgrade"].astype(str).str.split('_').str[0]
            
            # 6. loan_amount_div
            # df["loan_amount_credit"] = df["loan_amount"].astype(float) / (df["credit_score"].astype(float)+ eps)
            # df["loan_amount_div_income"] = df["loan_amount"].astype(int) / (df["annual_income"].astype(float)+ eps)
            # df["loan_amount_div_ratio"] = df["loan_amount"].astype(float) / (df["debt_to_income_ratio"].astype(float)+ eps)
            
            # 7. creadit
            df["credit_div_ratio"] = df["credit_score"].astype(float) / (df["debt_to_income_ratio"].astype(float)+ eps)

            ratio_cols = [
                "dti_to_interest_rate",
                "debt_to_monthly_income",
                "pti_ratio",
                "credit_div_ratio",
            ]

            for col in ratio_cols:
                upper = df[col].quantile(0.995)
                df[col] = df[col].clip(lower=0, upper=upper)

        return df_array

    df_array = remove_data_identify(df_array)
    df_array = log_regularization(df_array)
    df_array = feature_engineering(df_array)
    # df_array = remove_outliers(df_array, df_num_cols)
    return df_array

def encoding_data(df_train, target_col, model_type, X, y,X_train, X_val, y_train, y_val):

    df_num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    df_cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    
    education_order = [
        "High School",
        "Other",
        "Bachelor's",
        "Master's",
        "PhD",
    ]

    grade_order = [
        'G5','G4','G3','G2','G1',
        'F5','F4','F3','F2','F1',
        'E5','E4','E3','E2','E1',
        'D5','D4','D3','D2','D1',
        'C5','C4','C3','C2','C1',
        'B5','B4','B3','B2','B1',
        'A5','A4','A3','A2','A1',
    ]

    head_grade_order = ['G', 'F', 'E', 'D', 'C', 'B', 'A']

  
    # encoder.fit(X_train, y_train)

    
    # num_transformer = 'passthrough'
    num_transformer = StandardScaler()
    
    if model_type == "catboost":
        cat_cat_transformer = OrdinalEncoder(
            handle_unknown='use_encoded_value',
            unknown_value=-1)
        cat_cat_cols = [
            'education_level',
            'grade_subgrade',
            'head_grade',
            'gender',
            'marital_status',
            'loan_purpose',
            'employment_status',    
            'employment_loan_purpose',
            # 'employment_status_grade_subgrade',
        ]
    elif model_type == "lgbm":
        lgbm_onehot_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        lgbm_one_cols = [
            'gender',
            'marital_status',
            'loan_purpose',
            # 'employment_status',    
            'employment_loan_purpose',
        ]
        lgbm_ordinal_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', 
          categories=[
                education_order,
                grade_order,
                head_grade_order,
            ],
            unknown_value=-1)
        lgbm_ordinal_cols = [
            'education_level',
            'grade_subgrade',
            'head_grade',
        ]
        lgbm_cb_cols = [
            'employment_status',
            'employment_loan_purpose',
            # 'employment_status_grade_subgrade',
        ]
        lgbm_cb_encoder = CatBoostEncoder(
            return_df=False,
            random_state=rs,
            cols=lgbm_cb_cols,
        )
       
    elif model_type == "xgb":
        xgb_cat_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        
        xgb_cat_cols = [
            'gender',
            'marital_status',
            'loan_purpose',
            # 'employment_status',    
            'employment_loan_purpose',
        ]

        xgb_cb_cols = [
            'employment_status',
            'employment_loan_purpose',
            # 'employment_status_grade_subgrade',
        ]
        xgb_cb_encoder = CatBoostEncoder(
            return_df=False,
            random_state=rs,
            cols=xgb_cb_cols,
        )
      

    if model_type == "catboost":
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', num_transformer, df_num_cols),
                ('cat', cat_cat_transformer, cat_cat_cols),
                
            ],
            remainder='drop'
        )
    elif model_type == "lgbm":
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', num_transformer, df_num_cols),
                ('onehot', lgbm_onehot_transformer, lgbm_one_cols),
                ('ordinal', lgbm_ordinal_transformer, lgbm_ordinal_cols),
                ('catboost', lgbm_cb_encoder, lgbm_cb_cols),
            ],
            remainder='drop'  
        )
    elif model_type == "xgb":
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', num_transformer, df_num_cols),
                ('cat', xgb_cat_transformer, xgb_cat_cols),
                ('catboost', xgb_cb_encoder, xgb_cb_cols),
            ],
            remainder='drop'  
        )
   

    X_train_processed = preprocessor.fit_transform(X_train, y_train)
    X_val_processed = preprocessor.transform(X_val)
 
    return [X_train_processed, X_val_processed, y_train, y_val, preprocessor]


def optimize_models(X_train, y_train, model_type, cat_features=None):

    pos = (df_train['loan_paid_back'] == 1).sum()
    neg = (df_train['loan_paid_back'] == 0).sum()
    spw = neg / pos

    def objective(trial):
        if model_type == "catboost":
            params = {
                "iterations": trial.suggest_int("iterations", 100, 500),
                "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 100.0, log=True),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True),
                "random_strength": trial.suggest_float("random_strength", 0.0, 1.0),
                "depth": trial.suggest_int("depth", 4, 10),
                "random_seed": rs,
                "loss_function": "Logloss",      # 2ì§„ ë¶„ë¥˜ ê¸°ë³¸
                "eval_metric": "AUC",            # ëª¨ë‹ˆí„°ë§í•  ì§€í‘œ
                "verbose": False,   
            }
            model = CatBoostClassifier(**params)
        
        elif model_type == "lgbm":
            params = {
                "objective": "binary",
                "boosting_type": "gbdt",
                "n_estimators": trial.suggest_int("n_estimators", 300, 900),
                "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.1, log=True),
                "num_leaves": trial.suggest_int("num_leaves", 31, 255),
                "max_depth": trial.suggest_int("max_depth", -1, 12),
                "min_child_samples": trial.suggest_int("min_child_samples", 20, 200),
                "min_split_gain": trial.suggest_float("min_split_gain", 0.0, 1.0),
                "subsample": trial.suggest_float("subsample", 0.6, 1.0),
                "subsample_freq": trial.suggest_int("subsample_freq", 0, 5),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 10.0, log=True),
                "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
                "random_state": rs,
                "n_jobs": -1,
                "metric": "auc",
            }
            model = lgb.LGBMClassifier(**params)
        
        else:  
            params = {
                "n_estimators": trial.suggest_int("n_estimators", 300, 800),
                "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.05, log=True),
                "max_depth": trial.suggest_int("max_depth", 3, 12),
                "subsample": trial.suggest_float("subsample", 0.6, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
                "gamma": trial.suggest_float("gamma", 0.0, 1.0),
                "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 10.0, log=True),
                "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
                "random_state": rs,
                'scale_pos_weight': spw,
                # "use_label_encoder": False,
                "eval_metric": "auc",
            }
            model = xgb.XGBClassifier(**params)
        
        # k-foldë¥¼ í†µí•´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ê°ì²´ ìƒì„±
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=rs)

        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring="roc_auc", n_jobs=-1)
        return np.mean(scores)

    # optunaë¥¼ í†µí•´ ìµœì ì˜ íŒŒë¼ë©”í„°ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •
    sampler = optuna.samplers.TPESampler(seed=rs)
    study = optuna.create_study(direction="maximize", sampler=sampler)
    # ì‹¤ì œë¡œ ëª¨ë¸ì„ ëŒë ¤ë³´ë©´ì„œ ìµœì ì˜ íŒŒë¼ë©”í„°ë¥¼ ì°¾ëŠ” ê³¼ì •
    study.optimize(objective, n_trials=50)
    
    print("\nBest trial:" , model_type)
    print(study.best_trial.params)
    return study.best_trial.params

def create_models_with_optuna(X_train, y_train, model_type, use_fixed_params, cat_features=None):
    """Optuna ìµœì í™” ë˜ëŠ” ê³ ì • íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±"""
    pos = (df_train['loan_paid_back'] == 1).sum()
    neg = (df_train['loan_paid_back'] == 0).sum()
    spw = neg / pos

    if use_fixed_params:
        # ê³ ì • íŒŒë¼ë¯¸í„° ì‚¬ìš©
        if model_type == "catboost":
            fixed_params = {
                "loss_function": "Logloss",
                "eval_metric": "AUC",
                "depth": 8,
                "iterations": 477,
                "l2_leaf_reg": 77.30156271338825,
                "learning_rate": 0.08217342500676761,
                "random_strength": 0.6335140795029303,
                "random_seed": rs,
                "verbose": False,
            }
            model = CatBoostClassifier(**fixed_params)
            
        elif model_type == "lgbm":
            fixed_params = {
                "objective": "binary",
                "boosting_type": "gbdt",
                "colsample_bytree": 0.7689967650165342,
                "learning_rate": 0.0903227448273408,
                "max_depth": 7,
                "min_child_samples": 105,
                "min_split_gain": 0.8904577433341123,
                "n_estimators": 841,
                "num_leaves": 84,
                "reg_alpha": 0.01946164724781516,
                "reg_lambda": 3.665810648369544,
                "subsample": 0.9127034245997915,
                "subsample_freq": 1,
                "random_state": rs,
                "n_jobs": -1,
                "metric": "auc",
                "verbose": -1,
            }
            model = lgb.LGBMClassifier(**fixed_params)
            
        elif model_type == "xgb":
            fixed_params = {
                "colsample_bytree": 0.6059606379994678,
                "gamma": 0.861757820514575,
                "learning_rate": 0.04114729921642164,
                "max_depth": 7,
                "min_child_weight": 8,
                "n_estimators": 639,
                "reg_alpha": 3.568274751351765,
                "reg_lambda": 0.014158740787660969,
                "subsample": 0.8489898860887624,
                "random_state": rs,
                "scale_pos_weight": spw,
                "eval_metric": "auc",
            }
            model = xgb.XGBClassifier(**fixed_params)
            
        print(f"âœ… {model_type.upper()} ëª¨ë¸ ìƒì„± ì™„ë£Œ (ê³ ì • íŒŒë¼ë¯¸í„° ì‚¬ìš©)")
        return model
    
    else:
        try:
            print(f"\nğŸš€ {model_type.upper()} ìµœì í™” ì‹œì‘...")
            best_params = optimize_models(X_train, y_train, model_type, cat_features=cat_features)
            
            # ìµœì  íŒŒë¼ë¯¸í„° ì¬í™•ì¸ ì¶œë ¥
            print(f"âœ… {model_type.upper()} ìµœì¢… ì‚¬ìš© íŒŒë¼ë¯¸í„°:")
            print("-" * 80)
            for key, value in sorted(best_params.items()):
                print(f"  {key:25s}: {value}")
            print("-" * 80)
            
            # ëª¨ë¸ ìƒì„±
            if model_type == "catboost":
                model = CatBoostClassifier(**best_params)
            elif model_type == "lgbm":
                model = lgb.LGBMClassifier(**best_params)
            elif model_type == "xgb":
                model = xgb.XGBClassifier(**best_params)
            
            print(f"âœ… {model_type.upper()} ëª¨ë¸ ìƒì„± ì™„ë£Œ\n")
            return model
            
        except Exception as e:
            print(f"\nâŒ {model_type.upper()} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            raise

def find_optimal_weights(models, preprocessor_list):
    """ê° ëª¨ë¸ë³„ ì „ì²˜ë¦¬ëœ validation setì„ ì‚¬ìš©í•˜ì—¬ optimal weights ì°¾ê¸°"""
    
    cb_model = models['catboost']
    lgb_model = models['lgbm']
    xgb_model = models['xgb']
    
    # ê° ëª¨ë¸ì˜ ì „ì²˜ë¦¬ëœ validation set ê°€ì ¸ì˜¤ê¸°
    # preprocessor_list êµ¬ì¡°: [X_train, X_val, y_train, y_val, preprocessor]
    X_val_lgbm = preprocessor_list[0][1]      # lgbmìš© ì „ì²˜ë¦¬ëœ validation set
    X_val_xgb = preprocessor_list[1][1]       # xgbìš© ì „ì²˜ë¦¬ëœ validation set
    X_val_catboost = preprocessor_list[2][1]  # catboostìš© ì „ì²˜ë¦¬ëœ validation set
    y_val = preprocessor_list[0][3]           # ëª¨ë“  ëª¨ë¸ì˜ y_valì€ ë™ì¼
    
    # ê° ëª¨ë¸ì— ë§ëŠ” ì „ì²˜ë¦¬ëœ validation setìœ¼ë¡œ ì˜ˆì¸¡
    cb_pred = cb_model.predict_proba(X_val_catboost)[:, 1]
    lgb_pred = lgb_model.predict_proba(X_val_lgbm)[:, 1]
    xgb_pred = xgb_model.predict_proba(X_val_xgb)[:, 1]
    
    # ê° ëª¨ë¸ì˜ ê°œë³„ ì„±ëŠ¥ í™•ì¸
    cb_loss = log_loss(y_val, cb_pred)
    lgb_loss = log_loss(y_val, lgb_pred)
    xgb_loss = log_loss(y_val, xgb_pred)
    
    cb_auc = roc_auc_score(y_val, cb_pred)
    lgb_auc = roc_auc_score(y_val, lgb_pred)
    xgb_auc = roc_auc_score(y_val, xgb_pred)
    
    print("\n" + "="*80)
    print("ğŸ“Š ê° ëª¨ë¸ì˜ ê°œë³„ ì„±ëŠ¥ (Validation Set)")
    print("="*80)
    print(f"CatBoost  - LogLoss: {cb_loss:.6f}, AUC: {cb_auc:.6f}")
    print(f"LightGBM  - LogLoss: {lgb_loss:.6f}, AUC: {lgb_auc:.6f}")
    print(f"XGBoost   - LogLoss: {xgb_loss:.6f}, AUC: {xgb_auc:.6f}")
    
    # ëª¨ë¸ë“¤ ê°„ì˜ ì˜ˆì¸¡ ìƒê´€ê´€ê³„ í™•ì¸
    preds_dict = {
        'CatBoost': cb_pred,
        'LightGBM': lgb_pred,
        'XGBoost': xgb_pred
    }
    preds_df = pd.DataFrame(preds_dict)
    corr_matrix = preds_df.corr()
    
    print("\n" + "="*80)
    print("ğŸ“ˆ ëª¨ë¸ë“¤ ê°„ì˜ ì˜ˆì¸¡ ìƒê´€ê´€ê³„")
    print("="*80)
    print(corr_matrix.round(4))
    
    preds = np.vstack([cb_pred, lgb_pred, xgb_pred]).T  # shape: (N, 3)

    # ì„±ëŠ¥ ê¸°ë°˜ ì´ˆê¸° weight ì„¤ì • (LogLossê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ weight)
    losses = np.array([cb_loss, lgb_loss, xgb_loss])
    # LogLossë¥¼ ì—­ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ weightë¡œ ì‚¬ìš© (ì •ê·œí™”)
    inv_losses = 1.0 / (losses + 1e-10)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    init_w = inv_losses / inv_losses.sum()
    
    print(f"\nì´ˆê¸° weight (ì„±ëŠ¥ ê¸°ë°˜) - CatBoost: {init_w[0]:.4f}, LightGBM: {init_w[1]:.4f}, XGBoost: {init_w[2]:.4f}")

    # ì œì•½ì¡°ê±´: w >= 0, sum(w)=1
    constraints = ({
        'type': 'eq',
        'fun': lambda w: np.sum(w) - 1
    })
    # ìµœì†Œ weight ì œì•½ ì¶”ê°€ (ê° ëª¨ë¸ì´ ìµœì†Œ 5% ì´ìƒ ê¸°ì—¬í•˜ë„ë¡)
    min_w = 0.33
    bounds = [(min_w, 1)] * 3

    # ëª©ì  í•¨ìˆ˜: logloss ìµœì†Œí™”
    def loss_fn(w):
        blended = np.dot(preds, w)
        return log_loss(y_val, blended)

    result = minimize(loss_fn, init_w, method='SLSQP',
                      bounds=bounds, constraints=constraints)

    optimal_w = result.x
    print(f"\nâœ… ìµœì  weight - CatBoost: {optimal_w[0]:.4f}, LightGBM: {optimal_w[1]:.4f}, XGBoost: {optimal_w[2]:.4f}")
    
    # ìµœì  weightë¡œ ì•™ìƒë¸” ì„±ëŠ¥ í™•ì¸
    ensemble_pred = np.dot(preds, optimal_w)
    ensemble_loss = log_loss(y_val, ensemble_pred)
    ensemble_auc = roc_auc_score(y_val, ensemble_pred)
    print(f"ì•™ìƒë¸” ì„±ëŠ¥ - LogLoss: {ensemble_loss:.6f}, AUC: {ensemble_auc:.6f}")
    print("="*80 + "\n")
    
    return optimal_w

def ensemble_predict(models, preprocessed_X_list, weights):
    """ê° ëª¨ë¸ë³„ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ensemble ì˜ˆì¸¡"""
    cb_model = models['catboost']
    lgb_model = models['lgbm']
    xgb_model = models['xgb']
    
    X_lgbm = preprocessed_X_list[0]      # lgbmìš© ì „ì²˜ë¦¬ëœ ë°ì´í„°
    X_xgb = preprocessed_X_list[1]       # xgbìš© ì „ì²˜ë¦¬ëœ ë°ì´í„°
    X_catboost = preprocessed_X_list[2]  # catboostìš© ì „ì²˜ë¦¬ëœ ë°ì´í„°

    # ê° ëª¨ë¸ì— ë§ëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ ì˜ˆì¸¡
    # predict_probaê°€ 1ì°¨ì› ë°°ì—´ì„ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    def safe_predict_proba(model, X):
        """predict_proba ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (1ì°¨ì› ë˜ëŠ” 2ì°¨ì› ë°°ì—´ ëª¨ë‘ ì§€ì›)"""
        proba = model.predict_proba(X)
        if proba.ndim == 1:
            # 1ì°¨ì› ë°°ì—´ì¸ ê²½ìš° (CatBoost ë“±ì—ì„œ ë°œìƒ ê°€ëŠ¥)
            return proba
        elif proba.ndim == 2 and proba.shape[1] > 1:
            # 2ì°¨ì› ë°°ì—´ì´ê³  í´ë˜ìŠ¤ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°
            return proba[:, 1]  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ 
        else:
            # 2ì°¨ì›ì´ì§€ë§Œ í´ë˜ìŠ¤ê°€ 1ê°œì¸ ê²½ìš°
            return proba.flatten()
    
    cb_pred = safe_predict_proba(cb_model, X_catboost)
    lgb_pred = safe_predict_proba(lgb_model, X_lgbm)
    xgb_pred = safe_predict_proba(xgb_model, X_xgb)
    
    preds = np.vstack([cb_pred, lgb_pred, xgb_pred]).T
    ensemble_pred_proba = np.dot(preds, weights)
    ensemble_pred = (ensemble_pred_proba >=0.5).astype(float)
    
    return ensemble_pred, ensemble_pred_proba

def optimize_weights_random_search(models, X_val_list, y_val, n_trials=3000):
    """
    ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜ ì¡°í•©ì„ 3000ë²ˆ ì‹œë„í•˜ì—¬ AUCê°€ ê°€ì¥ ë†’ì€ ì¡°í•©ì„ ì°¾ìŠµë‹ˆë‹¤.
    X_val_list: ê° ëª¨ë¸ë³„ë¡œ ì „ì²˜ë¦¬ëœ X_val ë°ì´í„°ì˜ ë¦¬ìŠ¤íŠ¸ [X_cat, X_lgbm, X_xgb]
    """
    print(f"\nâš–ï¸ Optimizing Ensemble Weights (Random Search {n_trials} trials)...")
    model_order = ['lgbm', 'xgb', 'catboost']

    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ë¯¸ë¦¬ ê³„ì‚° (ì†ë„ í–¥ìƒ)
    preds_list = []
    for model_name in model_order:
        model = models[model_name]
        preds_list.append(model.predict_proba(X_val_list[model_order.index(model_name)])[:, 1])
    
    best_auc = 0
    best_weights = [0.33, 0.33, 0.33]
    
    for _ in range(n_trials):
        # ëœë¤ ê°€ì¤‘ì¹˜ ìƒì„± (Dirichlet ë¶„í¬ ì‚¬ìš© ì‹œ í•©ì´ 1ì´ ë¨)
        weights = np.random.dirichlet(np.ones(len(models)), size=1)[0]
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        final_pred = np.zeros_like(preds_list[0])
        for i, pred in enumerate(preds_list):
            final_pred += pred * weights[i]
            
        score = roc_auc_score(y_val, final_pred)
        if score > best_auc:
            best_auc = score
            best_weights = weights
            
    print(f"âœ… Optimal Weights Found: {best_weights}")
    print(f"âœ… Best Validation AUC: {best_auc:.5f}")
    return best_weights

# def ensemble_predict(models, X_list, weights):
#     model_order = ['lgbm', 'xgb', 'catboost']
#     final_pred_proba = np.zeros(X_list[0].shape[0])

#     for i, model_name in enumerate(model_order):
#         model = models[model_name]
#         pred = model.predict_proba(X_list[i])[:, 1]
#         final_pred_proba += pred * weights[i]
        
#     final_pred = (final_pred_proba >= 0.3).astype(float)
#     return final_pred, final_pred_proba

def analyze_feature_importance(models, preprocessor_list, top_n=20):
    """
    ê° ëª¨ë¸ë³„ë¡œ feature importanceë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    models : dict
        í•™ìŠµëœ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ {'lgbm': model, 'xgb': model, 'catboost': model}
    preprocessor_list : list
        ê° ëª¨ë¸ë³„ ì „ì²˜ë¦¬ê¸° ë¦¬ìŠ¤íŠ¸
    top_n : int
        ìƒìœ„ Nê°œ featureë§Œ í‘œì‹œ (ê¸°ë³¸ê°’: 20)
    """
    print("\n" + "="*80)
    print("ğŸ“Š ëª¨ë¸ë³„ Feature Importance ë¶„ì„")
    print("="*80)
    
    model_types = ["lgbm", "xgb", "catboost"]
    all_importances = {}
    
    for idx, model_name in enumerate(model_types):
        model = models[model_name]
        preprocessor = preprocessor_list[idx][4]  # preprocessor ê°ì²´
        feature_names = preprocessor.get_feature_names_out()
        
        # ëª¨ë¸ë³„ë¡œ feature importance ì¶”ì¶œ
        if model_name == "catboost":
            importances = model.get_feature_importance()
        elif model_name == "lgbm":
            importances = model.feature_importances_
        elif model_name == "xgb":
            importances = model.feature_importances_
        else:
            importances = None
        
        if importances is not None:
            # DataFrameìœ¼ë¡œ ë³€í™˜
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)
            
            all_importances[model_name] = importance_df
            
            # ìƒìœ„ Nê°œ ì¶œë ¥
            print(f"\nğŸ”¹ {model_name.upper()} - Top {top_n} Features:")
            print("-" * 80)
            for i, row in importance_df.head(top_n).iterrows():
                print(f"  {row['feature']:50s} : {row['importance']:10.4f}")
    
    # ì‹œê°í™”
    visualize_feature_importance(all_importances, top_n=top_n)
    
    # ê³µí†µ ì¤‘ìš” feature ì°¾ê¸°
    find_common_important_features(all_importances, top_n=top_n)
    
    return all_importances

def visualize_feature_importance(all_importances, top_n=20, figsize=(15, 12)):
    """
    ëª¨ë¸ë³„ feature importanceë¥¼ ì‹œê°í™”
    """
    n_models = len(all_importances)
    fig, axes = plt.subplots(n_models, 1, figsize=figsize)
    
    if n_models == 1:
        axes = [axes]
    
    for idx, (model_name, importance_df) in enumerate(all_importances.items()):
        top_features = importance_df.head(top_n)
        
        # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„
        axes[idx].barh(range(len(top_features)), top_features['importance'].values, color='crimson')
        axes[idx].set_yticks(range(len(top_features)))
        axes[idx].set_yticklabels(top_features['feature'].values)
        axes[idx].set_xlabel('Feature Importance', fontsize=12)
        axes[idx].set_title(f'{model_name.upper()} - Top {top_n} Feature Importance', fontsize=14, fontweight='bold')
        axes[idx].invert_yaxis()
        axes[idx].grid(axis='x', alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for i, v in enumerate(top_features['importance'].values):
            axes[idx].text(v, i, f' {v:.2f}', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    print(f"\nâœ… Feature importance ì‹œê°í™”ê°€ 'feature_importance.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def find_common_important_features(all_importances, top_n=20):
    """
    ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•˜ëŠ” feature ì°¾ê¸°
    """
    print("\n" + "="*80)
    print(f"ğŸ” ê³µí†µ ì¤‘ìš” Feature (ëª¨ë“  ëª¨ë¸ì—ì„œ Top {top_n}ì— í¬í•¨)")
    print("="*80)
    
    # ê° ëª¨ë¸ì˜ top N feature ì§‘í•©
    top_features_sets = {}
    for model_name, importance_df in all_importances.items():
        top_features = set(importance_df.head(top_n)['feature'].values)
        top_features_sets[model_name] = top_features
    
    # êµì§‘í•© ì°¾ê¸°
    common_features = set.intersection(*top_features_sets.values())
    
    if common_features:
        print(f"\nì´ {len(common_features)}ê°œì˜ ê³µí†µ ì¤‘ìš” feature ë°œê²¬:")
        print("-" * 80)
        
        # ì¤‘ìš”ë„ í‰ê·  ê³„ì‚°
        common_importance = {}
        for feature in common_features:
            avg_importance = np.mean([
                all_importances[model_name][
                    all_importances[model_name]['feature'] == feature
                ]['importance'].values[0]
                for model_name in all_importances.keys()
            ])
            common_importance[feature] = avg_importance
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_common = sorted(common_importance.items(), key=lambda x: x[1], reverse=True)
        
        for i, (feature, avg_imp) in enumerate(sorted_common, 1):
            print(f"  {i:2d}. {feature:50s} : í‰ê·  ì¤‘ìš”ë„ {avg_imp:10.4f}")
    else:
        print("\nê³µí†µ ì¤‘ìš” featureê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ë³„ ë¹„êµ í…Œì´ë¸”
    print("\n" + "="*80)
    print("ğŸ“ˆ ëª¨ë¸ë³„ Feature Importance ë¹„êµ (ê³µí†µ feature)")
    print("="*80)
    
    if common_features:
        comparison_data = []
        for feature in sorted_common[:10]:  # ìƒìœ„ 10ê°œë§Œ
            row = {'feature': feature[0]}
            for model_name in all_importances.keys():
                imp = all_importances[model_name][
                    all_importances[model_name]['feature'] == feature[0]
                ]['importance'].values[0]
                row[model_name] = imp
            row['average'] = feature[1]
            comparison_data.append(row)
        
        comparison_df = pd.DataFrame(comparison_data)
        print("\n", comparison_df.to_string(index=False))


def main(df_array, target_col, num_cols, cat_cols):
    
    df_train = df_array[0]
    df_test = df_array[1]
    
    model_types = ["lgbm", "xgb", "catboost"]

    X = df_train.drop(columns=[target_col])
    y = df_train[target_col]

    
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=rs)
    
    preprocessor_list = []
    for idx, model_type in enumerate(model_types):
        preprocessor_list.append(encoding_data(df_train, target_col, model_types[idx], X, y, X_train, X_val, y_train, y_val))

    model_result = []

    for idx, preprocessor in enumerate(preprocessor_list):
        
        X_train_processed = preprocessor[0]
        X_val_processed = preprocessor[1]
        y_train = preprocessor[2]
        y_val = preprocessor[3]
        preprocessor = preprocessor[4]
        
        print("ğŸ” Optimizing models with Optuna...")
        
        # if idx == 0:
        #     processed_model = create_models_with_optuna(X_train_processed, y_train, model_type=model_types[idx], use_fixed_params=False)
        # else:
        processed_model = create_models_with_optuna(X_train_processed, y_train, model_type=model_types[idx], use_fixed_params=True)

        feature_names = preprocessor.get_feature_names_out()
        X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)
        X_val_df = pd.DataFrame(X_val_processed, columns=feature_names)
        
        processed_model.fit(X_train_df, y_train)

        model_result.append(processed_model)

    # model_result.append(cb_model_trained)

    models = {
        'lgbm': model_result[0],
        'xgb': model_result[1],
        'catboost': model_result[2],
    }
    # preprocessor_list.append([X_train, X_val, y_train, y_val, cb_model])

    
    for idx, (name, model) in enumerate(models.items()):
        X_val_data = preprocessor_list[idx][1]  # validation set
        y_val_data = preprocessor_list[idx][3]  # validation labels
        
        try:
            pred = model.predict(X_val_data)
            proba = model.predict_proba(X_val_data)[:, 1]
            
            acc = accuracy_score(y_val_data, pred)
            auc = roc_auc_score(y_val_data, proba)

            print(f"{name} - Accuracy: {acc:.4f}, AUC: {auc:.4f}")
        
        except Exception as e:
            print(f"{name} - Error: {e}")
            import traceback
            traceback.print_exc()
    best_weights = [1/3, 1/3, 1/3]
    try:
        # ê° ëª¨ë¸ì˜ ì „ì²˜ë¦¬ëœ validation set ê°€ì ¸ì˜¤ê¸°
        X_val_list = [preprocessor_list[i][1] for i in range(3)] 
        y_val = preprocessor_list[0][3]  # ëª¨ë“  ëª¨ë¸ì˜ y_valì€ ë™ì¼
        
        # model_result ëŒ€ì‹  models ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©
        best_weights = find_optimal_weights(models, preprocessor_list)
        # best_weights = optimize_weights_random_search(models, X_val_list, y_val, n_trials=5000)
        ensemble_pred, ensemble_pred_proba = ensemble_predict(models, X_val_list, best_weights)
        ensemble_acc = accuracy_score(y_val, ensemble_pred)
        ensemble_auc = roc_auc_score(y_val, ensemble_pred_proba)
        print(f"Ensemble - Accuracy: {ensemble_acc:.4f}, AUC: {ensemble_auc:.4f}")
    except Exception as e:
        print(f"Ensemble Error: {e}")
    
    # Feature Importance ë¶„ì„ ì¶”ê°€
    try:
        feature_importances = analyze_feature_importance(models, preprocessor_list, top_n=20)
    except Exception as e:
        print(f"Feature Importance ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    return preprocessor_list, models, df_array, best_weights

if __name__ == "__main__":

    df_train = df_train
    df_test = df_test
    df_sub = df_sub
    
    # eda(df_train, df_test, df_sub)

    df_num_cols = df_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
    df_cat_cols = df_train.select_dtypes(include=['object', 'category']).columns.tolist()
    
    target_col = 'loan_paid_back'
    df_array = [df_train, df_test]
    df_array = data_preprocessing(df_array, target_col, df_num_cols, df_cat_cols )
    

    df_train.head()
    df_test.head()
    
    preprocessor_list, models, df_array, best_weights = main(df_array, target_col, df_num_cols, df_cat_cols)
    
    print("\n" + "="*80)
    print("ğŸ” ëª¨ë¸ ìƒì„¸ ë¶„ì„ ì‹œì‘")
    print("="*80)
     
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
    X_test_processed_list = []
    
    for i in range(len(preprocessor_list)):
        preprocessor = preprocessor_list[i][4]  # preprocessor ê°ì²´
        X_test_processed = preprocessor.transform(df_test)
        X_test_processed_list.append(X_test_processed)
    
    # X_test_processed_list.append(df_array[1])
    
    # 2. Validation set ê°€ì ¸ì˜¤ê¸° (weights ê³„ì‚°ìš©)
    X_val_list = [preprocessor_list[i][1] for i in range(3)]  # [catboost, lgbm, xgb]
    y_val = preprocessor_list[0][3]  # ëª¨ë“  ëª¨ë¸ì˜ y_valì€ ë™ì¼
    # best_weights = optimize_weights_random_search(models, X_val_list, n_trials=5000)
    # 4. Ensemble ì˜ˆì¸¡
    _, y_pred_ensemble = ensemble_predict(
        models, 
        X_test_processed_list,  # ê° ëª¨ë¸ë³„ ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        best_weights
    )

    # 5. ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.DataFrame({
        'id': df_sub['id'],
        'loan_paid_back': y_pred_ensemble
    })

    submission.to_csv('submission_ensemble.csv', index=False)
    df_confirm = pd.read_csv('submission_ensemble.csv')

    print(df_confirm.head())