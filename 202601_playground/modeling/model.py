import numpy as np
import pandas as pd
import os
from typing import Dict, List, Optional, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from catboost import CatBoostRegressor, CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

# í‰ê°€ ë° ê²€ì¦
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score, roc_auc_score, log_loss
from sklearn.inspection import permutation_importance



def check_gpu_availability():
    """
    GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    
    Returns:
    --------
    bool
        GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False


def get_gpu_device():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ë°˜í™˜
    
    Returns:
    --------
    str or None
        GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ('0', '1', ...) ë˜ëŠ” None
    """
    if check_gpu_availability():
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--list-gpus'], capture_output=True, text=True)
            if result.returncode == 0 and result.stdout.strip():
                # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
                return '0'
        except:
            pass
    return None


class BaseModel:
    """ëª¨ë“  ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_type: str, task_type: str = 'regression', random_state: int = 42):
        """
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ì… ('catboost', 'lightgbm', 'xgboost')
        task_type : str
            ì‘ì—… íƒ€ì… ('regression' or 'classification')
        random_state : int
            ëœë¤ ì‹œë“œ
        """
        self.model_type = model_type
        self.task_type = task_type
        self.random_state = random_state
        self.model = None
        self.feature_importance_ = None
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """ëª¨ë¸ í•™ìŠµ"""
        raise NotImplementedError
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        raise NotImplementedError
        
    def get_feature_importance(self):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë°˜í™˜"""
        return self.feature_importance_


class CatBoostModel(BaseModel):
    """CatBoost ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, 
                 cat_features: Optional[List] = None, use_gpu: bool = False, **kwargs):
        super().__init__('catboost', task_type, random_state)
        self.cat_features = cat_features
        self.params = kwargs
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'random_state': random_state,
            'verbose': False,
            'allow_writing_files': False,
        }
        
        # GPU ì„¤ì •
        if use_gpu:
            gpu_device = get_gpu_device()
            if gpu_device is not None:
                default_params['task_type'] = 'GPU'
                default_params['devices'] = gpu_device
                print(f"âœ… CatBoost: GPU ì‚¬ìš© ì„¤ì • (Device: {gpu_device})")
            else:
                print("âš ï¸ CatBoost: GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
        if task_type == 'regression':
            default_params.update({
                'loss_function': 'RMSE',
                'eval_metric': 'RMSE',
            })
        else:
            default_params.update({
                'loss_function': 'Logloss',
                'eval_metric': 'Logloss',
            })
        
        default_params.update(self.params)
        self.params = default_params
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """CatBoost ëª¨ë¸ í•™ìŠµ"""
        # CatBoostëŠ” ë²”ì£¼í˜• íŠ¹ì„±ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
        if isinstance(X_train, pd.DataFrame):
            if self.cat_features is None:
                # ë²”ì£¼í˜• íŠ¹ì„± ìë™ ê°ì§€
                self.cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
            cat_indices = [X_train.columns.get_loc(col) for col in self.cat_features if col in X_train.columns]
        else:
            cat_indices = self.cat_features if self.cat_features else None
        
        # ëª¨ë¸ ìƒì„±
        if self.task_type == 'regression':
            self.model = CatBoostRegressor(**self.params)
        else:
            self.model = CatBoostClassifier(**self.params)
        
        # í•™ìŠµ
        if X_val is not None and y_val is not None:
            self.model.fit(
                X_train, y_train,
                eval_set=(X_val, y_val),
                cat_features=cat_indices,
                **kwargs
            )
        else:
            self.model.fit(
                X_train, y_train,
                cat_features=cat_indices,
                **kwargs
            )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance_ = self.model.feature_importances_
        
        return self
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")
        return self.model.predict(X)


class LightGBMModel(BaseModel):
    """LightGBM ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, 
                 use_gpu: bool = False, **kwargs):
        super().__init__('lightgbm', task_type, random_state)
        self.params = kwargs
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'random_state': random_state,
            'verbosity': -1,
        }
        
        # GPU ì„¤ì •
        if use_gpu:
            gpu_device = get_gpu_device()
            if gpu_device is not None:
                default_params['device'] = 'gpu'
                default_params['gpu_platform_id'] = 0
                default_params['gpu_device_id'] = int(gpu_device)
                print(f"âœ… LightGBM: GPU ì‚¬ìš© ì„¤ì • (Device: {gpu_device})")
            else:
                print("âš ï¸ LightGBM: GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
        if task_type == 'regression':
            default_params.update({
                'objective': 'regression',
                'metric': 'rmse',
            })
        else:
            default_params.update({
                'objective': 'binary',
                'metric': 'binary_logloss',
            })
        
        default_params.update(self.params)
        self.params = default_params
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """LightGBM ëª¨ë¸ í•™ìŠµ"""
        # ë°ì´í„°ì…‹ ìƒì„±
        train_data = lgb.Dataset(X_train, label=y_train)
        
        if X_val is not None and y_val is not None:
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            self.model = lgb.train(
                self.params,
                train_data,
                valid_sets=[train_data, val_data],
                valid_names=['train', 'val'],
                **kwargs
            )
        else:
            self.model = lgb.train(
                self.params,
                train_data,
                **kwargs
            )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance_ = self.model.feature_importance(importance_type='gain')
        
        return self
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")
        return self.model.predict(X)


class XGBoostModel(BaseModel):
    """XGBoost ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, 
                 use_gpu: bool = False, **kwargs):
        super().__init__('xgboost', task_type, random_state)
        self.params = kwargs
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'random_state': random_state,
            'verbosity': 0,
        }
        
        # GPU ì„¤ì • (XGBoost 2.0+ì—ì„œëŠ” tree_method='hist'ì™€ device íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        self.use_gpu = False  # ì‹¤ì œ GPU ì‚¬ìš© ì—¬ë¶€ í”Œë˜ê·¸
        if use_gpu:
            gpu_device = get_gpu_device()
            if gpu_device is not None:
                # GPU ì„¤ì • ì‹œë„ (ì‹¤ì œ ì§€ì› ì—¬ë¶€ëŠ” fitì—ì„œ í™•ì¸)
                default_params['tree_method'] = 'hist'  # gpu_histëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŒ
                # XGBoost 2.0+ì—ì„œëŠ” deviceì— 'cuda:0' í˜•ì‹ìœ¼ë¡œ GPU ë²ˆí˜¸ í¬í•¨
                default_params['device'] = f'cuda:{gpu_device}'
            else:
                default_params['tree_method'] = 'hist'
        else:
            # CPU ê¸°ë³¸ê°’ ì„¤ì •
            default_params['tree_method'] = 'hist'
        
        if task_type == 'regression':
            default_params.update({
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
            })
        else:
            default_params.update({
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
            })
        
        default_params.update(self.params)
        self.params = default_params
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """XGBoost ëª¨ë¸ í•™ìŠµ"""
        # ë°ì´í„°ì…‹ ìƒì„±
        train_data = xgb.DMatrix(X_train, label=y_train)
        
        # GPU ì‚¬ìš© ì‹œë„, ì‹¤íŒ¨í•˜ë©´ CPUë¡œ ìë™ ì „í™˜
        try:
            if X_val is not None and y_val is not None:
                val_data = xgb.DMatrix(X_val, label=y_val)
                watchlist = [(train_data, 'train'), (val_data, 'val')]
                self.model = xgb.train(
                    self.params,
                    train_data,
                    evals=watchlist,
                    **kwargs
                )
            else:
                self.model = xgb.train(
                    self.params,
                    train_data,
                    **kwargs
                )
        except Exception as e:
            error_msg = str(e).lower()
            # GPU ê´€ë ¨ ì˜¤ë¥˜ì¸ ê²½ìš° CPUë¡œ ì „í™˜
            if 'gpu' in error_msg or 'gpu_hist' in error_msg or 'cuda' in error_msg:
                # CPU íŒŒë¼ë¯¸í„°ë¡œ ë³€ê²½
                cpu_params = self.params.copy()
                cpu_params['tree_method'] = 'hist'
                if 'device' in cpu_params:
                    del cpu_params['device']
                
                if X_val is not None and y_val is not None:
                    val_data = xgb.DMatrix(X_val, label=y_val)
                    watchlist = [(train_data, 'train'), (val_data, 'val')]
                    self.model = xgb.train(
                        cpu_params,
                        train_data,
                        evals=watchlist,
                        **kwargs
                    )
                else:
                    self.model = xgb.train(
                        cpu_params,
                        train_data,
                        **kwargs
                    )
                self.use_gpu = False
            else:
                # GPUì™€ ë¬´ê´€í•œ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                raise
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance_ = np.array(list(self.model.get_score(importance_type='gain').values()))
        
        return self
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")
        test_data = xgb.DMatrix(X)
        return self.model.predict(test_data)


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, use_gpu: bool = False):
        """
        Parameters:
        -----------
        task_type : str
            ì‘ì—… íƒ€ì… ('regression' or 'classification')
        random_state : int
            ëœë¤ ì‹œë“œ
        use_gpu : bool
            GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        """
        self.task_type = task_type
        self.random_state = random_state
        self.use_gpu = use_gpu
        self.models = {}
        self.cv_scores = {}
        self.oof_predictions = {}
        self.best_params = {}  # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì¶œë ¥
        if use_gpu:
            if check_gpu_availability():
                print("ğŸš€ GPU ì‚¬ìš© ê°€ëŠ¥: ëª¨ë¸ í•™ìŠµì— GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
                self.use_gpu = False
        
    def create_model(self, model_type: str, cat_features: Optional[List] = None, **kwargs):
        """
        ëª¨ë¸ ìƒì„±
        
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ì… ('catboost', 'lightgbm', 'xgboost')
        cat_features : List, optional
            ë²”ì£¼í˜• íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ (CatBoostìš©)
        **kwargs
            ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        # kwargsì—ì„œ random_state ì œê±° (ì¤‘ë³µ ì „ë‹¬ ë°©ì§€)
        kwargs_clean = {k: v for k, v in kwargs.items() if k != 'random_state'}
        
        if model_type == 'catboost':
            return CatBoostModel(
                task_type=self.task_type,
                random_state=self.random_state,
                cat_features=cat_features,
                use_gpu=self.use_gpu,
                **kwargs_clean
            )
        elif model_type == 'lightgbm':
            return LightGBMModel(
                task_type=self.task_type,
                random_state=self.random_state,
                use_gpu=self.use_gpu,
                **kwargs_clean
            )
        elif model_type == 'xgboost':
            return XGBoostModel(
                task_type=self.task_type,
                random_state=self.random_state,
                use_gpu=self.use_gpu,
                **kwargs_clean
            )
        else:
            raise ValueError(f"Unknown model_type: {model_type}")
    
    def train_with_cv(self, X, y, model_type: str, n_folds: int = 5, 
                     cat_features: Optional[List] = None, **model_params):

        # K-Fold ì„¤ì •
        if self.task_type == 'classification':
            kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)
        else:
            kf = KFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)
        
        models = []
        oof_preds = np.zeros(len(X))
        fold_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), 1):
            print(f"\nğŸ“Š Fold {fold}/{n_folds} í•™ìŠµ ì¤‘...")
            
            X_train, X_val = X.iloc[train_idx] if isinstance(X, pd.DataFrame) else X[train_idx], \
                            X.iloc[val_idx] if isinstance(X, pd.DataFrame) else X[val_idx]
            y_train, y_val = y.iloc[train_idx] if isinstance(y, pd.Series) else y[train_idx], \
                            y.iloc[val_idx] if isinstance(y, pd.Series) else y[val_idx]
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = self.create_model(model_type, cat_features=cat_features, **model_params)
            
            if model_type == 'lightgbm':
                model.fit(
                    X_train, y_train,
                    X_val=X_val, y_val=y_val,
                    num_boost_round=model_params.get('num_boost_round', 1000),
                    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]
                )
            elif model_type == 'xgboost':
                model.fit(
                    X_train, y_train,
                    X_val=X_val, y_val=y_val,
                    num_boost_round=model_params.get('num_boost_round', 1000),
                    early_stopping_rounds=100,
                    verbose_eval=False
                )
            else:  # catboost
                model.fit(
                    X_train, y_train,
                    X_val=X_val, y_val=y_val,
                    early_stopping_rounds=100
                )
            
            # ê²€ì¦ ì˜ˆì¸¡
            val_pred = model.predict(X_val)
            oof_preds[val_idx] = val_pred
            
            # í‰ê°€ ì ìˆ˜ ê³„ì‚°
            if self.task_type == 'regression':
                score = np.sqrt(mean_squared_error(y_val, val_pred))
                print(f"  Fold {fold} RMSE: {score:.4f}")
            else:
                score = roc_auc_score(y_val, val_pred)
                print(f"  Fold {fold} AUC: {score:.4f}")
            
            fold_scores.append(score)
            models.append(model)
        
        # ì „ì²´ CV ì ìˆ˜
        if self.task_type == 'regression':
            cv_score = np.sqrt(mean_squared_error(y, oof_preds))
            print(f"\nâœ… CV RMSE: {cv_score:.4f} (std: {np.std(fold_scores):.4f})")
        else:
            cv_score = roc_auc_score(y, oof_preds)
            print(f"\nâœ… CV AUC: {cv_score:.4f} (std: {np.std(fold_scores):.4f})")
        
        self.models[model_type] = models
        self.oof_predictions[model_type] = oof_preds
        self.cv_scores[model_type] = {
            'mean': cv_score,
            'std': np.std(fold_scores),
            'fold_scores': fold_scores
        }
        
        return {
            'models': models,
            'oof_predictions': oof_preds,
            'cv_score': cv_score,
            'fold_scores': fold_scores
        }
    
    def predict_test(self, X_test, model_type: str):

        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not found. Train the model first.")
        
        models = self.models[model_type]
        predictions = np.array([model.predict(X_test) for model in models])
        return predictions.mean(axis=0)
    
    def get_feature_importance(self, model_type: str, feature_names: List[str], 
                              average_across_folds: bool = True) -> pd.DataFrame:

        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not found. Train the model first.")
        
        models = self.models[model_type]
        n_folds = len(models)
        
        # ê° Foldì˜ feature importance ì¶”ì¶œ
        fold_importances = []
        for fold_idx, model in enumerate(models):
            try:
                # ê° ëª¨ë¸ í´ë˜ìŠ¤ì˜ get_feature_importance ë©”ì„œë“œ ì‚¬ìš©
                if model_type == 'catboost':
                    importance = model.get_feature_importance()
                elif model_type == 'lightgbm':
                    importance = model.get_feature_importance()
                elif model_type == 'xgboost':
                    # XGBoostëŠ” BaseModelì˜ get_feature_importance() ì‚¬ìš© (íŒŒë¼ë¯¸í„° ì—†ìŒ)
                    importance = model.get_feature_importance()
                else:
                    importance = None
                
                # importance ê¸¸ì´ í™•ì¸
                if importance is not None:
                    if len(importance) != len(feature_names):
                        print(f"âš ï¸ Warning: Feature importance length ({len(importance)}) != feature names length ({len(feature_names)}) for {model_type} Fold {fold_idx + 1}")
                        # ê¸¸ì´ê°€ ë§ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
                        if len(importance) < len(feature_names):
                            importance = np.pad(importance, (0, len(feature_names) - len(importance)), 'constant')
                        elif len(importance) > len(feature_names):
                            importance = importance[:len(feature_names)] # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                    
                    # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
                    if np.sum(importance) > 0:
                        importance = importance / np.sum(importance)
                    fold_importances.append(importance)
            except Exception as e:
                print(f"âš ï¸ Fold {fold_idx + 1}ì˜ feature importance ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if len(fold_importances) == 0:
            return pd.DataFrame()
        
        fold_importances = np.array(fold_importances)
        
        if average_across_folds:
            # Foldë³„ í‰ê· 
            mean_importance = np.mean(fold_importances, axis=0)
            std_importance = np.std(fold_importances, axis=0)
            
            importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': mean_importance,
                'Std': std_importance
            }).sort_values('Importance', ascending=False)
        else:
            # ê° Foldë³„ ì¤‘ìš”ë„
            importance_dict = {'Feature': feature_names}
            for fold_idx in range(len(fold_importances)):
                importance_dict[f'Fold_{fold_idx+1}'] = fold_importances[fold_idx]
            importance_df = pd.DataFrame(importance_dict)
            # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë„ ì¶”ê°€
            importance_df['Mean'] = importance_df[[f'Fold_{i+1}' for i in range(len(fold_importances))]].mean(axis=1)
            importance_df['Std'] = importance_df[[f'Fold_{i+1}' for i in range(len(fold_importances))]].std(axis=1)
            importance_df = importance_df.sort_values('Mean', ascending=False)
        
        return importance_df
    
    def get_permutation_importance(self, model_type: str, X: pd.DataFrame, y: Union[pd.Series, np.ndarray],
                                   feature_names: Optional[List[str]] = None,
                                   n_repeats: int = 10, random_state: Optional[int] = None,
                                   scoring: Optional[str] = None, n_jobs: int = -1) -> pd.DataFrame:
        """
        Permutation Importance ê³„ì‚°
        
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ì… ('catboost', 'lightgbm', 'xgboost')
        X : pd.DataFrame
            ê²€ì¦ ë°ì´í„° (íŠ¹ì„±)
        y : Union[pd.Series, np.ndarray]
            ê²€ì¦ ë°ì´í„° (íƒ€ê²Ÿ)
        feature_names : List[str], optional
            íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ X.columns ì‚¬ìš©)
        n_repeats : int
            Permutation ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)
        random_state : int, optional
            ëœë¤ ì‹œë“œ
        scoring : str, optional
            í‰ê°€ ì§€í‘œ ('neg_mean_squared_error', 'roc_auc' ë“±)
            Noneì´ë©´ task_typeì— ë”°ë¼ ìë™ ì„ íƒ
        n_jobs : int
            ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: -1, ëª¨ë“  CPU ì‚¬ìš©)
            
        Returns:
        --------
        pd.DataFrame
            Permutation Importance ê²°ê³¼ (Feature, Importance, Std)
        """
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not found. Train the model first.")
        
        if feature_names is None:
            feature_names = X.columns.tolist()
        
        models = self.models[model_type]
        n_folds = len(models)
        
        # scoring ìë™ ì„¤ì •
        if scoring is None:
            if self.task_type == 'regression':
                scoring = 'neg_mean_squared_error'
            else:
                scoring = 'roc_auc'
        
        # ê° Foldì˜ permutation importance ê³„ì‚°
        fold_importances = []
        
        print(f"\nğŸ“Š {model_type.upper()} Permutation Importance ê³„ì‚° ì¤‘...")
        print(f"   ë°˜ë³µ íšŸìˆ˜: {n_repeats}, Fold ìˆ˜: {n_folds}")
        
        for fold_idx, model in enumerate(models, 1):
            print(f"   Fold {fold_idx}/{n_folds} ì²˜ë¦¬ ì¤‘...", end='\r')
            
            try:
                # ëª¨ë¸ì˜ predict ë©”ì„œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                # CatBoost, LightGBM, XGBoostëŠ” ëª¨ë‘ sklearn APIë¥¼ ë”°ë¦„
                if hasattr(model, 'predict'):
                    # permutation_importance ê³„ì‚°
                    perm_result = permutation_importance(
                        model, X, y,
                        n_repeats=n_repeats,
                        random_state=random_state if random_state is not None else self.random_state,
                        scoring=scoring,
                        n_jobs=n_jobs
                    )
                    
                    # Permutation importanceëŠ” ì›ë˜ ì ìˆ˜ - permuted ì ìˆ˜
                    # neg_mean_squared_errorì˜ ê²½ìš°: ë” í° ê°’(ëœ ìŒìˆ˜)ì´ ë” ì¤‘ìš”í•¨
                    importances = perm_result.importances_mean
                    if scoring.startswith('neg_'):
                        # ìŒìˆ˜ ì§€í‘œëŠ” ë°˜ì „ (ë” í° ê°’ì´ ë” ì¤‘ìš”)
                        importances = -importances
                    
                    stds = perm_result.importances_std
                    
                    fold_importances.append({
                        'importances': importances,
                        'stds': stds
                    })
                else:
                    print(f"   âš ï¸ Fold {fold_idx}: ëª¨ë¸ì— predict ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"   âš ï¸ Fold {fold_idx} Permutation Importance ê³„ì‚° ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print()  # ì¤„ë°”ê¿ˆ
        
        if len(fold_importances) == 0:
            print("   âš ï¸ Permutation Importanceë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # Foldë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
        all_importances = np.array([fold['importances'] for fold in fold_importances])
        all_stds = np.array([fold['stds'] for fold in fold_importances])
        
        mean_importance = np.mean(all_importances, axis=0)
        std_importance = np.std(all_importances, axis=0)
        
        # íŠ¹ì„± ì´ë¦„ê³¼ ì¤‘ìš”ë„ ê¸¸ì´ í™•ì¸
        if len(feature_names) != len(mean_importance):
            print(f"   âš ï¸ Warning: Feature names length ({len(feature_names)}) != importance length ({len(mean_importance)})")
            if len(feature_names) < len(mean_importance):
                feature_names = feature_names + [f'Feature_{i}' for i in range(len(feature_names), len(mean_importance))]
            else:
                feature_names = feature_names[:len(mean_importance)]
        
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': mean_importance,
            'Std': std_importance
        }).sort_values('Importance', ascending=False)
        
        print(f"   âœ… Permutation Importance ê³„ì‚° ì™„ë£Œ!")
        print(f"   ìƒìœ„ 10ê°œ íŠ¹ì„±:")
        for idx, row in importance_df.head(10).iterrows():
            print(f"     {row['Feature']:40s}: {row['Importance']:8.4f} (std: {row['Std']:.4f})")
        
        return importance_df


class EnsembleModel:
    """ì•™ìƒë¸” ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression'):
        """
        Parameters:
        -----------
        task_type : str
            ì‘ì—… íƒ€ì… ('regression' or 'classification')
        """
        self.task_type = task_type
        self.weights = None
        self.ensemble_score = None
        
    def fit(self, predictions_dict: Dict[str, np.ndarray], y_true: np.ndarray, 
            method: str = 'weighted_average', optimize: bool = True):
        """
        ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
        
        Parameters:
        -----------
        predictions_dict : dict
            {model_name: predictions} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        y_true : np.ndarray
            ì‹¤ì œ íƒ€ê²Ÿ ê°’
        method : str
            ì•™ìƒë¸” ë°©ë²• ('weighted_average', 'simple_average', 'stacking')
        optimize : bool
            ê°€ì¤‘ì¹˜ ìµœì í™” ì—¬ë¶€
        """
        self.method = method
        
        if method == 'simple_average':
            self.weights = {name: 1.0 / len(predictions_dict) for name in predictions_dict.keys()}
        
        elif method == 'weighted_average':
            if optimize:
                self.weights = self._optimize_weights(predictions_dict, y_true)
            else:
                # ë™ì¼ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘
                self.weights = {name: 1.0 / len(predictions_dict) for name in predictions_dict.keys()}
        
        elif method == 'stacking':
            # ê°„ë‹¨í•œ ì„ í˜• ìŠ¤íƒœí‚¹ (ì‹¤ì œë¡œëŠ” ë©”íƒ€ ëª¨ë¸ì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ì²´)
            self.weights = self._optimize_weights(predictions_dict, y_true)
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ê³„ì‚°)
        ensemble_pred = np.zeros_like(list(predictions_dict.values())[0])
        for name, pred in predictions_dict.items():
            if self.weights and name in self.weights:
                ensemble_pred += self.weights[name] * pred
        
        if self.task_type == 'regression':
            self.ensemble_score = np.sqrt(mean_squared_error(y_true, ensemble_pred))
        else:
            self.ensemble_score = roc_auc_score(y_true, ensemble_pred)
    
    def _optimize_weights(self, predictions_dict: Dict[str, np.ndarray], 
                         y_true: np.ndarray) -> Dict[str, float]:
        """
        ê°€ì¤‘ì¹˜ ìµœì í™” (scipy.optimize ì‚¬ìš©)
        """
        from scipy.optimize import minimize
        
        model_names = list(predictions_dict.keys())
        predictions_list = [predictions_dict[name] for name in model_names]
        
        def objective(weights):
            """ìµœì í™” ëª©ì  í•¨ìˆ˜"""
            weighted_pred = np.zeros_like(predictions_list[0])
            for pred, weight in zip(predictions_list, weights):
                weighted_pred += weight * pred
            
            if self.task_type == 'regression':
                return np.sqrt(mean_squared_error(y_true, weighted_pred))
            else:
                return -roc_auc_score(y_true, weighted_pred)  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
        
        # ì œì•½ ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1, ëª¨ë“  ê°€ì¤‘ì¹˜ >= 0
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0.33, 1) for _ in range(len(model_names))]
        
        # ì´ˆê¸°ê°’: ë™ì¼ ê°€ì¤‘ì¹˜
        initial_weights = np.array([1.0 / len(model_names)] * len(model_names))
        
        # ìµœì í™”
        result = minimize(
            objective,
            initial_weights,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        weights_dict = {name: weight for name, weight in zip(model_names, result.x)}
        
        print(f"\nğŸ“Š ìµœì í™”ëœ ê°€ì¤‘ì¹˜:")
        for name, weight in weights_dict.items():
            print(f"  {name}: {weight:.4f}")
        
        return weights_dict
    
    def predict(self, predictions_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ì•™ìƒë¸” ì˜ˆì¸¡
        
        Parameters:
        -----------
        predictions_dict : dict
            {model_name: predictions} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
            
        Returns:
        --------
        np.ndarray
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        if self.weights is None:
            raise ValueError("Ensemble model must be fitted first")
        
        ensemble_pred = np.zeros_like(list(predictions_dict.values())[0])
        
        for name, pred in predictions_dict.items():
            ensemble_pred += self.weights[name] * pred
        
        return ensemble_pred


def evaluate_model(y_true, y_pred, task_type: str = 'regression'):
    """
    ëª¨ë¸ í‰ê°€
    
    Parameters:
    -----------
    y_true : np.ndarray
        ì‹¤ì œ íƒ€ê²Ÿ ê°’
    y_pred : np.ndarray
        ì˜ˆì¸¡ ê°’
    task_type : str
        ì‘ì—… íƒ€ì…
        
    Returns:
    --------
    dict
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    if task_type == 'regression':
        return {
            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
            'MAE': mean_absolute_error(y_true, y_pred),
            'R2': r2_score(y_true, y_pred)
        }
    else:
        return {
            'AUC': roc_auc_score(y_true, y_pred),
            'Accuracy': accuracy_score(y_true, (y_pred > 0.5).astype(int)),
            'LogLoss': log_loss(y_true, y_pred)
        }

