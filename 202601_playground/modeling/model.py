import numpy as np
import pandas as pd
import os
from typing import Dict, List, Optional, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from catboost import CatBoostRegressor, CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

# í‰ê°€ ë° ê²€ì¦
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score, roc_auc_score, log_loss

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
import optuna
from optuna import Trial


def check_gpu_availability():
    """
    GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    
    Returns:
    --------
    bool
        GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False


def get_gpu_device():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ë°˜í™˜
    
    Returns:
    --------
    str or None
        GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ('0', '1', ...) ë˜ëŠ” None
    """
    if check_gpu_availability():
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--list-gpus'], capture_output=True, text=True)
            if result.returncode == 0 and result.stdout.strip():
                # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
                return '0'
        except:
            pass
    return None


class BaseModel:
    """ëª¨ë“  ëª¨ë¸ì˜ ê¸°ë³¸ í´ëž˜ìŠ¤"""
    
    def __init__(self, model_type: str, task_type: str = 'regression', random_state: int = 42):
        """
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ìž… ('catboost', 'lightgbm', 'xgboost')
        task_type : str
            ìž‘ì—… íƒ€ìž… ('regression' or 'classification')
        random_state : int
            ëžœë¤ ì‹œë“œ
        """
        self.model_type = model_type
        self.task_type = task_type
        self.random_state = random_state
        self.model = None
        self.feature_importance_ = None
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """ëª¨ë¸ í•™ìŠµ"""
        raise NotImplementedError
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        raise NotImplementedError
        
    def get_feature_importance(self):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë°˜í™˜"""
        return self.feature_importance_


class CatBoostModel(BaseModel):
    """CatBoost ëª¨ë¸ í´ëž˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, 
                 cat_features: Optional[List] = None, use_gpu: bool = False, **kwargs):
        super().__init__('catboost', task_type, random_state)
        self.cat_features = cat_features
        self.params = kwargs
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'random_state': random_state,
            'verbose': False,
            'allow_writing_files': False,
        }
        
        # GPU ì„¤ì •
        if use_gpu:
            gpu_device = get_gpu_device()
            if gpu_device is not None:
                default_params['task_type'] = 'GPU'
                default_params['devices'] = gpu_device
                print(f"âœ… CatBoost: GPU ì‚¬ìš© ì„¤ì • (Device: {gpu_device})")
            else:
                print("âš ï¸ CatBoost: GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
        if task_type == 'regression':
            default_params.update({
                'loss_function': 'RMSE',
                'eval_metric': 'RMSE',
            })
        else:
            default_params.update({
                'loss_function': 'Logloss',
                'eval_metric': 'Logloss',
            })
        
        default_params.update(self.params)
        self.params = default_params
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """CatBoost ëª¨ë¸ í•™ìŠµ"""
        # CatBoostëŠ” ë²”ì£¼í˜• íŠ¹ì„±ì„ ìžë™ìœ¼ë¡œ ì²˜ë¦¬
        if isinstance(X_train, pd.DataFrame):
            if self.cat_features is None:
                # ë²”ì£¼í˜• íŠ¹ì„± ìžë™ ê°ì§€
                self.cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
            cat_indices = [X_train.columns.get_loc(col) for col in self.cat_features if col in X_train.columns]
        else:
            cat_indices = self.cat_features if self.cat_features else None
        
        # ëª¨ë¸ ìƒì„±
        if self.task_type == 'regression':
            self.model = CatBoostRegressor(**self.params)
        else:
            self.model = CatBoostClassifier(**self.params)
        
        # í•™ìŠµ
        if X_val is not None and y_val is not None:
            self.model.fit(
                X_train, y_train,
                eval_set=(X_val, y_val),
                cat_features=cat_indices,
                **kwargs
            )
        else:
            self.model.fit(
                X_train, y_train,
                cat_features=cat_indices,
                **kwargs
            )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ìž¥
        self.feature_importance_ = self.model.feature_importances_
        
        return self
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")
        return self.model.predict(X)


class LightGBMModel(BaseModel):
    """LightGBM ëª¨ë¸ í´ëž˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, 
                 use_gpu: bool = False, **kwargs):
        super().__init__('lightgbm', task_type, random_state)
        self.params = kwargs
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'random_state': random_state,
            'verbosity': -1,
        }
        
        # GPU ì„¤ì •
        if use_gpu:
            gpu_device = get_gpu_device()
            if gpu_device is not None:
                default_params['device'] = 'gpu'
                default_params['gpu_platform_id'] = 0
                default_params['gpu_device_id'] = int(gpu_device)
                print(f"âœ… LightGBM: GPU ì‚¬ìš© ì„¤ì • (Device: {gpu_device})")
            else:
                print("âš ï¸ LightGBM: GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
        if task_type == 'regression':
            default_params.update({
                'objective': 'regression',
                'metric': 'rmse',
            })
        else:
            default_params.update({
                'objective': 'binary',
                'metric': 'binary_logloss',
            })
        
        default_params.update(self.params)
        self.params = default_params
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """LightGBM ëª¨ë¸ í•™ìŠµ"""
        # ë°ì´í„°ì…‹ ìƒì„±
        train_data = lgb.Dataset(X_train, label=y_train)
        
        if X_val is not None and y_val is not None:
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            self.model = lgb.train(
                self.params,
                train_data,
                valid_sets=[train_data, val_data],
                valid_names=['train', 'val'],
                **kwargs
            )
        else:
            self.model = lgb.train(
                self.params,
                train_data,
                **kwargs
            )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ìž¥
        self.feature_importance_ = self.model.feature_importance(importance_type='gain')
        
        return self
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")
        return self.model.predict(X)


class XGBoostModel(BaseModel):
    """XGBoost ëª¨ë¸ í´ëž˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, 
                 use_gpu: bool = False, **kwargs):
        super().__init__('xgboost', task_type, random_state)
        self.params = kwargs
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'random_state': random_state,
            'verbosity': 0,
        }
        
        # GPU ì„¤ì •
        self.use_gpu = False  # ì‹¤ì œ GPU ì‚¬ìš© ì—¬ë¶€ í”Œëž˜ê·¸
        if use_gpu:
            gpu_device = get_gpu_device()
            if gpu_device is not None:
                # GPU ì„¤ì • ì‹œë„ (ì‹¤ì œ ì§€ì› ì—¬ë¶€ëŠ” fitì—ì„œ í™•ì¸)
                default_params['tree_method'] = 'gpu_hist'
                # XGBoost 3.1+ì—ì„œëŠ” deviceì— 'cuda:0' í˜•ì‹ìœ¼ë¡œ GPU ë²ˆí˜¸ í¬í•¨
                default_params['device'] = f'cuda:{gpu_device}'
                print(f"ðŸ”„ XGBoost: GPU ì‚¬ìš© ì‹œë„ (Device: cuda:{gpu_device})")
                print("   (GPU ì§€ì›ì´ ì—†ìœ¼ë©´ ìžë™ìœ¼ë¡œ CPUë¡œ ì „í™˜ë©ë‹ˆë‹¤)")
            else:
                print("âš ï¸ XGBoost: GPU ë””ë°”ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
                default_params['tree_method'] = 'hist'
        else:
            # CPU ê¸°ë³¸ê°’ ì„¤ì •
            default_params['tree_method'] = 'hist'
        
        if task_type == 'regression':
            default_params.update({
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
            })
        else:
            default_params.update({
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
            })
        
        default_params.update(self.params)
        self.params = default_params
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """XGBoost ëª¨ë¸ í•™ìŠµ"""
        # ë°ì´í„°ì…‹ ìƒì„±
        train_data = xgb.DMatrix(X_train, label=y_train)
        
        # GPU ì‚¬ìš© ì‹œë„, ì‹¤íŒ¨í•˜ë©´ CPUë¡œ ìžë™ ì „í™˜
        try:
            if X_val is not None and y_val is not None:
                val_data = xgb.DMatrix(X_val, label=y_val)
                watchlist = [(train_data, 'train'), (val_data, 'val')]
                self.model = xgb.train(
                    self.params,
                    train_data,
                    evals=watchlist,
                    **kwargs
                )
            else:
                self.model = xgb.train(
                    self.params,
                    train_data,
                    **kwargs
                )
        except Exception as e:
            error_msg = str(e).lower()
            # GPU ê´€ë ¨ ì˜¤ë¥˜ì¸ ê²½ìš° CPUë¡œ ì „í™˜
            if 'gpu' in error_msg or 'gpu_hist' in error_msg or 'cuda' in error_msg:
                print(f"âš ï¸ XGBoost GPU í•™ìŠµ ì‹¤íŒ¨: {str(e)[:100]}")
                print("ðŸ”„ CPUë¡œ ìžë™ ì „í™˜í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤...")
                # CPU íŒŒë¼ë¯¸í„°ë¡œ ë³€ê²½
                cpu_params = self.params.copy()
                cpu_params['tree_method'] = 'hist'
                if 'device' in cpu_params:
                    del cpu_params['device']
                
                if X_val is not None and y_val is not None:
                    val_data = xgb.DMatrix(X_val, label=y_val)
                    watchlist = [(train_data, 'train'), (val_data, 'val')]
                    self.model = xgb.train(
                        cpu_params,
                        train_data,
                        evals=watchlist,
                        **kwargs
                    )
                else:
                    self.model = xgb.train(
                        cpu_params,
                        train_data,
                        **kwargs
                    )
                self.use_gpu = False
            else:
                # GPUì™€ ë¬´ê´€í•œ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                raise
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ìž¥
        self.feature_importance_ = np.array(list(self.model.get_score(importance_type='gain').values()))
        
        return self
        
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model must be fitted before prediction")
        test_data = xgb.DMatrix(X)
        return self.model.predict(test_data)


class HyperparameterOptimizer:
    """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í´ëž˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42):
        """
        Parameters:
        -----------
        task_type : str
            ìž‘ì—… íƒ€ìž… ('regression' or 'classification')
        random_state : int
            ëžœë¤ ì‹œë“œ
        """
        self.task_type = task_type
        self.random_state = random_state
        self.best_params = {}
        
    def optimize_catboost(self, X_train, y_train, n_trials: int = 50, 
                         cat_features: Optional[List] = None, **kwargs):
        """
        CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        
        Parameters:
        -----------
        X_train : pd.DataFrame or np.ndarray
            í•™ìŠµ ë°ì´í„°
        y_train : pd.Series or np.ndarray
            íƒ€ê²Ÿ ë°ì´í„°
        n_trials : int
            Optuna ì‹œë„ íšŸìˆ˜
        cat_features : List, optional
            ë²”ì£¼í˜• íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
        **kwargs
            ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
        --------
        dict
            ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
        
        def objective(trial):
            if self.task_type == 'regression':
                params = {
                    'iterations': trial.suggest_int('iterations', 100, 2000),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'depth': trial.suggest_int('depth', 4, 10),
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 100, log=True),
                    'random_strength': trial.suggest_float('random_strength', 0, 1),
                    'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),
                    'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),
                    'random_state': self.random_state,
                    'verbose': False,
                    'allow_writing_files': False,
                }
                model = CatBoostRegressor(**params)
                scoring = 'neg_mean_squared_error'
            else:
                params = {
                    'iterations': trial.suggest_int('iterations', 100, 2000),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'depth': trial.suggest_int('depth', 4, 10),
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 100, log=True),
                    'random_strength': trial.suggest_float('random_strength', 0, 1),
                    'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),
                    'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),
                    'random_state': self.random_state,
                    'verbose': False,
                    'allow_writing_files': False,
                }
                model = CatBoostClassifier(**params)
                scoring = 'roc_auc'
            
            # K-Fold êµì°¨ ê²€ì¦
            if self.task_type == 'classification':
                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
            else:
                cv = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
            
            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)
            
            if self.task_type == 'regression':
                return -scores.mean()  # RMSEëŠ” ìµœì†Œí™”
            else:
                return scores.mean()  # AUCëŠ” ìµœëŒ€í™”
        
        study = optuna.create_study(
            direction='minimize' if self.task_type == 'regression' else 'maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        self.best_params['catboost'] = study.best_params
        return study.best_params
    
    def optimize_lightgbm(self, X_train, y_train, n_trials: int = 50, **kwargs):
        """
        LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        """
        from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
        
        def objective(trial):
            if self.task_type == 'regression':
                params = {
                    'objective': 'regression',
                    'metric': 'rmse',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 31, 255),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),
                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': -1,
                }
                model = lgb.LGBMRegressor(**params)
                scoring = 'neg_mean_squared_error'
            else:
                params = {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 31, 255),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),
                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': -1,
                }
                model = lgb.LGBMClassifier(**params)
                scoring = 'roc_auc'
            
            if self.task_type == 'classification':
                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
            else:
                cv = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
            
            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)
            
            if self.task_type == 'regression':
                return -scores.mean()
            else:
                return scores.mean()
        
        study = optuna.create_study(
            direction='minimize' if self.task_type == 'regression' else 'maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        self.best_params['lightgbm'] = study.best_params
        return study.best_params
    
    def optimize_xgboost(self, X_train, y_train, n_trials: int = 50, **kwargs):
        """
        XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        """
        from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
        
        def objective(trial):
            if self.task_type == 'regression':
                params = {
                    'objective': 'reg:squarederror',
                    'eval_metric': 'rmse',
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                    'gamma': trial.suggest_float('gamma', 0, 1),
                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': 0,
                }
                model = xgb.XGBRegressor(**params)
                scoring = 'neg_mean_squared_error'
            else:
                params = {
                    'objective': 'binary:logistic',
                    'eval_metric': 'logloss',
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                    'gamma': trial.suggest_float('gamma', 0, 1),
                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': 0,
                }
                model = xgb.XGBClassifier(**params)
                scoring = 'roc_auc'
            
            if self.task_type == 'classification':
                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
            else:
                cv = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
            
            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)
            
            if self.task_type == 'regression':
                return -scores.mean()
            else:
                return scores.mean()
        
        study = optuna.create_study(
            direction='minimize' if self.task_type == 'regression' else 'maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        self.best_params['xgboost'] = study.best_params
        return study.best_params


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµ ë° ê´€ë¦¬ í´ëž˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, use_gpu: bool = False):
        """
        Parameters:
        -----------
        task_type : str
            ìž‘ì—… íƒ€ìž… ('regression' or 'classification')
        random_state : int
            ëžœë¤ ì‹œë“œ
        use_gpu : bool
            GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        """
        self.task_type = task_type
        self.random_state = random_state
        self.use_gpu = use_gpu
        self.models = {}
        self.cv_scores = {}
        self.oof_predictions = {}
        self.best_params = {}  # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ìž¥
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì¶œë ¥
        if use_gpu:
            if check_gpu_availability():
                print("ðŸš€ GPU ì‚¬ìš© ê°€ëŠ¥: ëª¨ë¸ í•™ìŠµì— GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
                self.use_gpu = False
        
    def create_model(self, model_type: str, cat_features: Optional[List] = None, **kwargs):
        """
        ëª¨ë¸ ìƒì„±
        
        Parameters:
        -----------
        model_type : str
            ëª¨ë¸ íƒ€ìž… ('catboost', 'lightgbm', 'xgboost')
        cat_features : List, optional
            ë²”ì£¼í˜• íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ (CatBoostìš©)
        **kwargs
            ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        if model_type == 'catboost':
            return CatBoostModel(
                task_type=self.task_type,
                random_state=self.random_state,
                cat_features=cat_features,
                use_gpu=self.use_gpu,
                **kwargs
            )
        elif model_type == 'lightgbm':
            return LightGBMModel(
                task_type=self.task_type,
                random_state=self.random_state,
                use_gpu=self.use_gpu,
                **kwargs
            )
        elif model_type == 'xgboost':
            return XGBoostModel(
                task_type=self.task_type,
                random_state=self.random_state,
                use_gpu=self.use_gpu,
                **kwargs
            )
        else:
            raise ValueError(f"Unknown model_type: {model_type}")
    
    def train_with_cv(self, X, y, model_type: str, n_folds: int = 5, 
                     cat_features: Optional[List] = None, **model_params):
        """
        K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
        
        Parameters:
        -----------
        X : pd.DataFrame or np.ndarray
            í•™ìŠµ ë°ì´í„°
        y : pd.Series or np.ndarray
            íƒ€ê²Ÿ ë°ì´í„°
        model_type : str
            ëª¨ë¸ íƒ€ìž… ('catboost', 'lightgbm', 'xgboost')
        n_folds : int
            K-Fold ê°œìˆ˜
        cat_features : List, optional
            ë²”ì£¼í˜• íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
        **model_params
            ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°
            
        Returns:
        --------
        dict
            í•™ìŠµëœ ëª¨ë¸ë“¤, OOF ì˜ˆì¸¡, CV ì ìˆ˜
        """
        # K-Fold ì„¤ì •
        if self.task_type == 'classification':
            kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)
        else:
            kf = KFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)
        
        models = []
        oof_preds = np.zeros(len(X))
        fold_scores = []
        
        print(f"\n{'='*60}")
        print(f"ðŸš€ {model_type.upper()} ëª¨ë¸ í•™ìŠµ ì‹œìž‘ (K-Fold={n_folds})")
        print(f"{'='*60}")
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), 1):
            print(f"\nðŸ“Š Fold {fold}/{n_folds} í•™ìŠµ ì¤‘...")
            
            X_train, X_val = X.iloc[train_idx] if isinstance(X, pd.DataFrame) else X[train_idx], \
                            X.iloc[val_idx] if isinstance(X, pd.DataFrame) else X[val_idx]
            y_train, y_val = y.iloc[train_idx] if isinstance(y, pd.Series) else y[train_idx], \
                            y.iloc[val_idx] if isinstance(y, pd.Series) else y[val_idx]
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = self.create_model(model_type, cat_features=cat_features, **model_params)
            
            if model_type == 'lightgbm':
                model.fit(
                    X_train, y_train,
                    X_val=X_val, y_val=y_val,
                    num_boost_round=model_params.get('num_boost_round', 1000),
                    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]
                )
            elif model_type == 'xgboost':
                model.fit(
                    X_train, y_train,
                    X_val=X_val, y_val=y_val,
                    num_boost_round=model_params.get('num_boost_round', 1000),
                    early_stopping_rounds=100,
                    verbose_eval=False
                )
            else:  # catboost
                model.fit(
                    X_train, y_train,
                    X_val=X_val, y_val=y_val,
                    early_stopping_rounds=100
                )
            
            # ê²€ì¦ ì˜ˆì¸¡
            val_pred = model.predict(X_val)
            oof_preds[val_idx] = val_pred
            
            # í‰ê°€ ì ìˆ˜ ê³„ì‚°
            if self.task_type == 'regression':
                score = np.sqrt(mean_squared_error(y_val, val_pred))
                print(f"  Fold {fold} RMSE: {score:.4f}")
            else:
                score = roc_auc_score(y_val, val_pred)
                print(f"  Fold {fold} AUC: {score:.4f}")
            
            fold_scores.append(score)
            models.append(model)
        
        # ì „ì²´ CV ì ìˆ˜
        if self.task_type == 'regression':
            cv_score = np.sqrt(mean_squared_error(y, oof_preds))
            print(f"\nâœ… CV RMSE: {cv_score:.4f} (std: {np.std(fold_scores):.4f})")
        else:
            cv_score = roc_auc_score(y, oof_preds)
            print(f"\nâœ… CV AUC: {cv_score:.4f} (std: {np.std(fold_scores):.4f})")
        
        self.models[model_type] = models
        self.oof_predictions[model_type] = oof_preds
        self.cv_scores[model_type] = {
            'mean': cv_score,
            'std': np.std(fold_scores),
            'fold_scores': fold_scores
        }
        
        return {
            'models': models,
            'oof_predictions': oof_preds,
            'cv_score': cv_score,
            'fold_scores': fold_scores
        }
    
    def predict_test(self, X_test, model_type: str):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ì•™ìƒë¸”: ëª¨ë“  fold ëª¨ë¸ì˜ í‰ê· )
        
        Parameters:
        -----------
        X_test : pd.DataFrame or np.ndarray
            í…ŒìŠ¤íŠ¸ ë°ì´í„°
        model_type : str
            ëª¨ë¸ íƒ€ìž…
            
        Returns:
        --------
        np.ndarray
            ì˜ˆì¸¡ ê²°ê³¼
        """
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not found. Train the model first.")
        
        models = self.models[model_type]
        predictions = np.array([model.predict(X_test) for model in models])
        return predictions.mean(axis=0)


class EnsembleModel:
    """ì•™ìƒë¸” ëª¨ë¸ í´ëž˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression'):
        """
        Parameters:
        -----------
        task_type : str
            ìž‘ì—… íƒ€ìž… ('regression' or 'classification')
        """
        self.task_type = task_type
        self.weights = None
        
    def fit(self, predictions_dict: Dict[str, np.ndarray], y_true: np.ndarray, 
            method: str = 'weighted_average', optimize: bool = True):
        """
        ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
        
        Parameters:
        -----------
        predictions_dict : dict
            {model_name: predictions} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        y_true : np.ndarray
            ì‹¤ì œ íƒ€ê²Ÿ ê°’
        method : str
            ì•™ìƒë¸” ë°©ë²• ('weighted_average', 'simple_average', 'stacking')
        optimize : bool
            ê°€ì¤‘ì¹˜ ìµœì í™” ì—¬ë¶€
        """
        self.method = method
        
        if method == 'simple_average':
            self.weights = {name: 1.0 / len(predictions_dict) for name in predictions_dict.keys()}
        
        elif method == 'weighted_average':
            if optimize:
                self.weights = self._optimize_weights(predictions_dict, y_true)
            else:
                # ë™ì¼ ê°€ì¤‘ì¹˜ë¡œ ì‹œìž‘
                self.weights = {name: 1.0 / len(predictions_dict) for name in predictions_dict.keys()}
        
        elif method == 'stacking':
            # ê°„ë‹¨í•œ ì„ í˜• ìŠ¤íƒœí‚¹ (ì‹¤ì œë¡œëŠ” ë©”íƒ€ ëª¨ë¸ì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ì²´)
            self.weights = self._optimize_weights(predictions_dict, y_true)
        
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _optimize_weights(self, predictions_dict: Dict[str, np.ndarray], 
                         y_true: np.ndarray) -> Dict[str, float]:
        """
        ê°€ì¤‘ì¹˜ ìµœì í™” (scipy.optimize ì‚¬ìš©)
        """
        from scipy.optimize import minimize
        
        model_names = list(predictions_dict.keys())
        predictions_list = [predictions_dict[name] for name in model_names]
        
        def objective(weights):
            """ìµœì í™” ëª©ì  í•¨ìˆ˜"""
            weighted_pred = np.zeros_like(predictions_list[0])
            for pred, weight in zip(predictions_list, weights):
                weighted_pred += weight * pred
            
            if self.task_type == 'regression':
                return np.sqrt(mean_squared_error(y_true, weighted_pred))
            else:
                return -roc_auc_score(y_true, weighted_pred)  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
        
        # ì œì•½ ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1, ëª¨ë“  ê°€ì¤‘ì¹˜ >= 0
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0, 1) for _ in range(len(model_names))]
        
        # ì´ˆê¸°ê°’: ë™ì¼ ê°€ì¤‘ì¹˜
        initial_weights = np.array([1.0 / len(model_names)] * len(model_names))
        
        # ìµœì í™”
        result = minimize(
            objective,
            initial_weights,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        weights_dict = {name: weight for name, weight in zip(model_names, result.x)}
        
        print(f"\nðŸ“Š ìµœì í™”ëœ ê°€ì¤‘ì¹˜:")
        for name, weight in weights_dict.items():
            print(f"  {name}: {weight:.4f}")
        
        return weights_dict
    
    def predict(self, predictions_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ì•™ìƒë¸” ì˜ˆì¸¡
        
        Parameters:
        -----------
        predictions_dict : dict
            {model_name: predictions} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
            
        Returns:
        --------
        np.ndarray
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        if self.weights is None:
            raise ValueError("Ensemble model must be fitted first")
        
        ensemble_pred = np.zeros_like(list(predictions_dict.values())[0])
        
        for name, pred in predictions_dict.items():
            ensemble_pred += self.weights[name] * pred
        
        return ensemble_pred


def save_hyperparameters(best_params: Dict[str, dict], filepath: str, 
                         task_type: str = 'regression', additional_info: Optional[dict] = None):
    """
    ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ìž¥
    
    Parameters:
    -----------
    best_params : dict
        {model_type: params} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    filepath : str
        ì €ìž¥í•  íŒŒì¼ ê²½ë¡œ
    task_type : str
        ìž‘ì—… íƒ€ìž…
    additional_info : dict, optional
        ì¶”ê°€ ì •ë³´ (CV ì ìˆ˜, ë‚ ì§œ ë“±)
    """
    import json
    from datetime import datetime
    
    save_data = {
        'task_type': task_type,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'hyperparameters': best_params
    }
    
    if additional_info:
        save_data['additional_info'] = additional_info
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ìž¥ ì™„ë£Œ: {filepath}")


def load_hyperparameters(filepath: str) -> Dict[str, dict]:
    """
    ì €ìž¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    
    Parameters:
    -----------
    filepath : str
        íŒŒì¼ ê²½ë¡œ
        
    Returns:
    --------
    dict
        {model_type: params} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    import json
    
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nðŸ“‚ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {filepath}")
    print(f"  Task Type: {data.get('task_type', 'unknown')}")
    print(f"  Timestamp: {data.get('timestamp', 'unknown')}")
    
    return data.get('hyperparameters', {})


def evaluate_model(y_true, y_pred, task_type: str = 'regression'):
    """
    ëª¨ë¸ í‰ê°€
    
    Parameters:
    -----------
    y_true : np.ndarray
        ì‹¤ì œ íƒ€ê²Ÿ ê°’
    y_pred : np.ndarray
        ì˜ˆì¸¡ ê°’
    task_type : str
        ìž‘ì—… íƒ€ìž…
        
    Returns:
    --------
    dict
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    if task_type == 'regression':
        return {
            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
            'MAE': mean_absolute_error(y_true, y_pred),
            'R2': r2_score(y_true, y_pred)
        }
    else:
        return {
            'AUC': roc_auc_score(y_true, y_pred),
            'Accuracy': accuracy_score(y_true, (y_pred > 0.5).astype(int)),
            'LogLoss': log_loss(y_true, y_pred)
        }

