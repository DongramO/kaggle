"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° ê´€ë¦¬ ëª¨ë“ˆ
"""
import os
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Optional
from datetime import datetime

# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from catboost import CatBoostRegressor, CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

# í‰ê°€ ë° ê²€ì¦
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
from sklearn.metrics import mean_squared_error, roc_auc_score

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
try:
    import optuna
    from optuna import Trial
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optunaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# GPU ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def get_gpu_device():
    """GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ë°˜í™˜"""
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--list-gpus'], capture_output=True, text=True)
        if result.returncode == 0:
            # ì²« ë²ˆì§¸ GPU ì‚¬ìš© (0ë²ˆ)
            return 0
    except:
        pass
    return None


class HyperparameterOptimizer:
    """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, task_type: str = 'regression', random_state: int = 42, use_gpu: bool = False):
        """
        Parameters:
        -----------
        task_type : str
            ì‘ì—… íƒ€ì… ('regression' or 'classification')
        random_state : int
            ëœë¤ ì‹œë“œ
        use_gpu : bool
            GPU ì‚¬ìš© ì—¬ë¶€
        """
        if not OPTUNA_AVAILABLE:
            raise ImportError("Optunaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install optuna'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        self.task_type = task_type
        self.random_state = random_state
        self.use_gpu = use_gpu
        self.best_params = {}
        
    def optimize_catboost(self, X_train, y_train, n_trials: int = 50, 
                         cat_features: Optional[List] = None, **kwargs):
        """
        CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        
        Parameters:
        -----------
        X_train : pd.DataFrame or np.ndarray
            í•™ìŠµ ë°ì´í„°
        y_train : pd.Series or np.ndarray
            íƒ€ê²Ÿ ë°ì´í„°
        n_trials : int
            Optuna ì‹œë„ íšŸìˆ˜
        cat_features : List, optional
            ë²”ì£¼í˜• íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
        **kwargs
            ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
        --------
        dict
            ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        def objective(trial):
            # bootstrap_typeì„ ë¨¼ì € ì„ íƒ
            bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS'])
            
            if self.task_type == 'regression':
                params = {
                    # CatBoost ìµœì í™”: ë” ë§ì€ iterationsì™€ ë‚®ì€ learning_rate ë²”ìœ„
                    'iterations': trial.suggest_int('iterations', 500, 2000),  # ë²”ìœ„ ì¦ê°€: 500-2000
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),  # 0.3 -> 0.15ë¡œ ì œí•œ (CatBoost ê¶Œì¥ ë²”ìœ„)
                    'depth': trial.suggest_int('depth', 4, 10),
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 100, log=True),
                    'random_strength': trial.suggest_float('random_strength', 0, 1),
                    'bootstrap_type': bootstrap_type,
                    'random_state': self.random_state,
                    'verbose': False,
                    'allow_writing_files': False,
                }
                # bagging_temperatureëŠ” bootstrap_type='Bayesian'ì¼ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥
                if bootstrap_type == 'Bayesian':
                    params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 1)
                # GPU ì„¤ì • ì¶”ê°€
                if self.use_gpu:
                    gpu_device = get_gpu_device()
                    if gpu_device is not None:
                        params['task_type'] = 'GPU'
                        params['devices'] = str(gpu_device)
                model = CatBoostRegressor(**params)
                scoring = 'neg_mean_squared_error'
            else:
                params = {
                    # CatBoost ìµœì í™”: ë” ë§ì€ iterationsì™€ ë‚®ì€ learning_rate ë²”ìœ„
                    'iterations': trial.suggest_int('iterations', 500, 2000),  # ë²”ìœ„ ì¦ê°€: 500-2000
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),  # 0.3 -> 0.15ë¡œ ì œí•œ (CatBoost ê¶Œì¥ ë²”ìœ„)
                    'depth': trial.suggest_int('depth', 4, 10),
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 100, log=True),
                    'random_strength': trial.suggest_float('random_strength', 0, 1),
                    'bootstrap_type': bootstrap_type,
                    'random_state': self.random_state,
                    'verbose': False,
                    'allow_writing_files': False,
                }
                # bagging_temperatureëŠ” bootstrap_type='Bayesian'ì¼ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥
                if bootstrap_type == 'Bayesian':
                    params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 1)
                # GPU ì„¤ì • ì¶”ê°€
                if self.use_gpu:
                    gpu_device = get_gpu_device()
                    if gpu_device is not None:
                        params['task_type'] = 'GPU'
                        params['devices'] = str(gpu_device)
                model = CatBoostClassifier(**params)
                scoring = 'roc_auc'
            
            # K-Fold êµì°¨ ê²€ì¦ (Optuna ìµœì í™” ì‹œ ì†ë„ ê°œì„ ì„ ìœ„í•´ 3-fold ì‚¬ìš©)
            if self.task_type == 'classification':
                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)  # 5 -> 3ìœ¼ë¡œ ì¶•ì†Œ
            else:
                cv = KFold(n_splits=3, shuffle=True, random_state=self.random_state)  # 5 -> 3ìœ¼ë¡œ ì¶•ì†Œ
            
            # CatBoost ë²”ì£¼í˜• íŠ¹ì„± ì²˜ë¦¬
            # DataFrameì¸ ê²½ìš° CatBoostê°€ ìë™ìœ¼ë¡œ ë²”ì£¼í˜•ì„ ê°ì§€í•˜ì§€ë§Œ,
            # ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ìˆ˜ë™ìœ¼ë¡œ CV ìˆ˜í–‰
            if cat_features is not None and isinstance(X_train, pd.DataFrame):
                cat_indices = [X_train.columns.get_loc(col) for col in cat_features if col in X_train.columns]
                # ìˆ˜ë™ìœ¼ë¡œ CV ìˆ˜í–‰í•˜ì—¬ cat_features ì „ë‹¬
                cv_scores = []
                for train_idx, val_idx in cv.split(X_train, y_train):
                    X_train_fold = X_train.iloc[train_idx] if isinstance(X_train, pd.DataFrame) else X_train[train_idx]
                    X_val_fold = X_train.iloc[val_idx] if isinstance(X_train, pd.DataFrame) else X_train[val_idx]
                    y_train_fold = y_train.iloc[train_idx] if isinstance(y_train, pd.Series) else y_train[train_idx]
                    y_val_fold = y_train.iloc[val_idx] if isinstance(y_train, pd.Series) else y_train[val_idx]
                    
                    # ê° foldë§ˆë‹¤ ìƒˆ ëª¨ë¸ ìƒì„± (paramsëŠ” ì´ë¯¸ GPU ì„¤ì • í¬í•¨)
                    model_fold = CatBoostRegressor(**params) if self.task_type == 'regression' else CatBoostClassifier(**params)
                    model_fold.fit(X_train_fold, y_train_fold, cat_features=cat_indices, verbose=False)
                    y_pred = model_fold.predict(X_val_fold)
                    
                    if self.task_type == 'regression':
                        score = -mean_squared_error(y_val_fold, y_pred)  # neg_mean_squared_error
                    else:
                        score = roc_auc_score(y_val_fold, y_pred)
                    cv_scores.append(score)
                scores = np.array(cv_scores)
            else:
                # ë²”ì£¼í˜• íŠ¹ì„±ì´ ì—†ê±°ë‚˜ ìë™ ê°ì§€ ê°€ëŠ¥í•œ ê²½ìš°
                # n_jobs=1ë¡œ ì„¤ì •í•˜ì—¬ Optunaì™€ì˜ ì¶©ëŒ ë°©ì§€
                scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=1)
            
            if self.task_type == 'regression':
                return -scores.mean()  # RMSEëŠ” ìµœì†Œí™”
            else:
                return scores.mean()  # AUCëŠ” ìµœëŒ€í™”
        
        study = optuna.create_study(
            direction='minimize' if self.task_type == 'regression' else 'maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ì„ ìœ„í•œ ì½œë°±
        def callback(study, trial):
            # 1% ê°„ê²© ë˜ëŠ” ì²« ë²ˆì§¸ trialì—ì„œ ì¶œë ¥
            interval = max(1, n_trials // 100)
            if (trial.number + 1) % interval == 0 or trial.number == 0:
                best_value = study.best_value if study.best_value is not None else float('inf')
                current_value = trial.value if trial.value is not None else 'N/A'
                if isinstance(current_value, (int, float)):
                    print(f"  Trial {trial.number + 1}/{n_trials} ì™„ë£Œ | "
                          f"Best Score: {best_value:.6f} | "
                          f"Current Score: {current_value:.6f}")
                else:
                    print(f"  Trial {trial.number + 1}/{n_trials} ì™„ë£Œ | "
                          f"Best Score: {best_value:.6f} | "
                          f"Current Score: {current_value}")
        
        print(f"  ğŸ”„ Optuna ìµœì í™” ì‹œì‘ (ì´ {n_trials} trials)...")
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True, callbacks=[callback])
        
        best_value = study.best_value if study.best_value is not None else float('inf')
        print(f"  âœ… ìµœì í™” ì™„ë£Œ! Best Score: {best_value:.6f}")
        self.best_params['catboost'] = study.best_params
        return study.best_params
    
    def optimize_lightgbm(self, X_train, y_train, n_trials: int = 50, **kwargs):
        """
        LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        """
        def objective(trial):
            if self.task_type == 'regression':
                params = {
                    'objective': 'regression',
                    'metric': 'rmse',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 31, 255),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),
                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': -1,
                }
                # GPU ì„¤ì • ì¶”ê°€
                if self.use_gpu:
                    gpu_device = get_gpu_device()
                    if gpu_device is not None:
                        params['device'] = 'gpu'
                        params['gpu_platform_id'] = 0
                        params['gpu_device_id'] = gpu_device
                model = lgb.LGBMRegressor(**params)
                scoring = 'neg_mean_squared_error'
            else:
                params = {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 31, 255),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),
                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': -1,
                }
                # GPU ì„¤ì • ì¶”ê°€
                if self.use_gpu:
                    gpu_device = get_gpu_device()
                    if gpu_device is not None:
                        params['device'] = 'gpu'
                        params['gpu_platform_id'] = 0
                        params['gpu_device_id'] = gpu_device
                model = lgb.LGBMClassifier(**params)
                scoring = 'roc_auc'
            
            # K-Fold êµì°¨ ê²€ì¦ (Optuna ìµœì í™” ì‹œ ì†ë„ ê°œì„ ì„ ìœ„í•´ 3-fold ì‚¬ìš©)
            if self.task_type == 'classification':
                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)  # 5 -> 3ìœ¼ë¡œ ì¶•ì†Œ
            else:
                cv = KFold(n_splits=3, shuffle=True, random_state=self.random_state)  # 5 -> 3ìœ¼ë¡œ ì¶•ì†Œ
            
            # n_jobs=1ë¡œ ì„¤ì •í•˜ì—¬ Optunaì™€ì˜ ì¶©ëŒ ë°©ì§€
            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=1)
            
            if self.task_type == 'regression':
                return -scores.mean()
            else:
                return scores.mean()
        
        study = optuna.create_study(
            direction='minimize' if self.task_type == 'regression' else 'maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ì„ ìœ„í•œ ì½œë°±
        def callback(study, trial):
            # 1% ê°„ê²© ë˜ëŠ” ì²« ë²ˆì§¸ trialì—ì„œ ì¶œë ¥
            interval = max(1, n_trials // 100)
            if (trial.number + 1) % interval == 0 or trial.number == 0:
                best_value = study.best_value if study.best_value is not None else float('inf')
                current_value = trial.value if trial.value is not None else 'N/A'
                if isinstance(current_value, (int, float)):
                    print(f"  Trial {trial.number + 1}/{n_trials} ì™„ë£Œ | "
                          f"Best Score: {best_value:.6f} | "
                          f"Current Score: {current_value:.6f}")
                else:
                    print(f"  Trial {trial.number + 1}/{n_trials} ì™„ë£Œ | "
                          f"Best Score: {best_value:.6f} | "
                          f"Current Score: {current_value}")
        
        print(f"  ğŸ”„ Optuna ìµœì í™” ì‹œì‘ (ì´ {n_trials} trials)...")
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True, callbacks=[callback])
        
        best_value = study.best_value if study.best_value is not None else float('inf')
        print(f"  âœ… ìµœì í™” ì™„ë£Œ! Best Score: {best_value:.6f}")
        self.best_params['lightgbm'] = study.best_params
        return study.best_params
    
    def optimize_xgboost(self, X_train, y_train, n_trials: int = 50, **kwargs):
        """
        XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        """
        def objective(trial):
            if self.task_type == 'regression':
                params = {
                    'objective': 'reg:squarederror',
                    'eval_metric': 'rmse',
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                    'gamma': trial.suggest_float('gamma', 0, 1),
                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': 0,
                }
                # GPU ì„¤ì • ì¶”ê°€ (XGBoost 2.0+ì—ì„œëŠ” tree_method='hist'ì™€ device íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                if self.use_gpu:
                    gpu_device = get_gpu_device()
                    if gpu_device is not None:
                        params['tree_method'] = 'hist'  # gpu_histëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŒ
                        params['device'] = f'cuda:{gpu_device}'
                    else:
                        params['tree_method'] = 'hist'
                else:
                    params['tree_method'] = 'hist'
                model = xgb.XGBRegressor(**params)
                scoring = 'neg_mean_squared_error'
            else:
                params = {
                    'objective': 'binary:logistic',
                    'eval_metric': 'logloss',
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                    'gamma': trial.suggest_float('gamma', 0, 1),
                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
                    'random_state': self.random_state,
                    'verbosity': 0,
                }
                # GPU ì„¤ì • ì¶”ê°€ (XGBoost 2.0+ì—ì„œëŠ” tree_method='hist'ì™€ device íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                if self.use_gpu:
                    gpu_device = get_gpu_device()
                    if gpu_device is not None:
                        params['tree_method'] = 'hist'  # gpu_histëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŒ
                        params['device'] = f'cuda:{gpu_device}'
                    else:
                        params['tree_method'] = 'hist'
                else:
                    params['tree_method'] = 'hist'
                model = xgb.XGBClassifier(**params)
                scoring = 'roc_auc'
            
            # K-Fold êµì°¨ ê²€ì¦ (Optuna ìµœì í™” ì‹œ ì†ë„ ê°œì„ ì„ ìœ„í•´ 3-fold ì‚¬ìš©)
            if self.task_type == 'classification':
                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)  # 5 -> 3ìœ¼ë¡œ ì¶•ì†Œ
            else:
                cv = KFold(n_splits=3, shuffle=True, random_state=self.random_state)  # 5 -> 3ìœ¼ë¡œ ì¶•ì†Œ
            
            # n_jobs=1ë¡œ ì„¤ì •í•˜ì—¬ Optunaì™€ì˜ ì¶©ëŒ ë°©ì§€
            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=1)
            
            if self.task_type == 'regression':
                return -scores.mean()
            else:
                return scores.mean()
        
        study = optuna.create_study(
            direction='minimize' if self.task_type == 'regression' else 'maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ì„ ìœ„í•œ ì½œë°±
        def callback(study, trial):
            # 1% ê°„ê²© ë˜ëŠ” ì²« ë²ˆì§¸ trialì—ì„œ ì¶œë ¥
            interval = max(1, n_trials // 100)
            if (trial.number + 1) % interval == 0 or trial.number == 0:
                best_value = study.best_value if study.best_value is not None else float('inf')
                current_value = trial.value if trial.value is not None else 'N/A'
                if isinstance(current_value, (int, float)):
                    print(f"  Trial {trial.number + 1}/{n_trials} ì™„ë£Œ | "
                          f"Best Score: {best_value:.6f} | "
                          f"Current Score: {current_value:.6f}")
                else:
                    print(f"  Trial {trial.number + 1}/{n_trials} ì™„ë£Œ | "
                          f"Best Score: {best_value:.6f} | "
                          f"Current Score: {current_value}")
        
        print(f"  ğŸ”„ Optuna ìµœì í™” ì‹œì‘ (ì´ {n_trials} trials)...")
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True, callbacks=[callback])
        
        best_value = study.best_value if study.best_value is not None else float('inf')
        print(f"  âœ… ìµœì í™” ì™„ë£Œ! Best Score: {best_value:.6f}")
        self.best_params['xgboost'] = study.best_params
        return study.best_params


def optimize_hyperparameters(X_train, y_train, categorical_cols, task_type='regression',
                              n_trials=50, random_state=42, use_saved_params=False, 
                              params_filepath=None, encoded_cols_tag='_encoded', use_gpu=False,
                              sample_size=None):
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ì €ì¥ëœ íŒŒë¼ë¯¸í„° ë¡œë“œ
    
    Parameters:
    -----------
    X_train : pd.DataFrame
        í•™ìŠµ ë°ì´í„°
    y_train : pd.Series
        íƒ€ê²Ÿ ë°ì´í„°
    categorical_cols : List[str]
        ë²”ì£¼í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    task_type : str
        ì‘ì—… íƒ€ì… ('regression' or 'classification')
    n_trials : int
        Optuna ì‹œë„ íšŸìˆ˜
    random_state : int
        ëœë¤ ì‹œë“œ
    use_saved_params : bool
        ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
    params_filepath : str
        íŒŒë¼ë¯¸í„° íŒŒì¼ ê²½ë¡œ
    encoded_cols_tag : str
        ì¸ì½”ë”©ëœ ì»¬ëŸ¼ íƒœê·¸
    use_gpu : bool
        GPU ì‚¬ìš© ì—¬ë¶€
    sample_size : int, optional
        ìƒ˜í”Œ í¬ê¸° (ì†ë„ ê°œì„ ìš©)
        
    Returns:
    --------
    dict
        ëª¨ë¸ë³„ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    """
    # use_saved_paramsê°€ Trueì´ê³  íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
    # ë‹¨, use_optunaê°€ Trueì¸ ê²½ìš°ëŠ” Optuna ìµœì í™”ë¥¼ ì‹¤í–‰ (use_saved_params ë¬´ì‹œ)
    # use_optunaëŠ” optimize_hyperparameters í•¨ìˆ˜ì—ì„œ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ, 
    # í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ use_saved_params=Falseë¡œ ì „ë‹¬í•´ì•¼ í•¨
    if use_saved_params and params_filepath and os.path.exists(params_filepath):
        print(f"\nğŸ“‚ ì €ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸°: {params_filepath}")
        best_params = load_hyperparameters(params_filepath)
        return best_params
    
    if not OPTUNA_AVAILABLE:
        raise ImportError("Optunaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install optuna'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    print(f"\n{'='*60}")
    print("ğŸ” Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    if use_gpu:
        print("ğŸš€ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ë°ì´í„° ìƒ˜í”Œë§ (ì†ë„ ê°œì„ )
    if sample_size is not None and len(X_train) > sample_size:
        print(f"âš¡ ì†ë„ ê°œì„ ì„ ìœ„í•´ ë°ì´í„° ìƒ˜í”Œë§: {len(X_train)} -> {sample_size}ê°œ")
        if task_type == 'classification':
            from sklearn.model_selection import train_test_split
            X_train_sample, _, y_train_sample, _ = train_test_split(
                X_train, y_train, train_size=sample_size, 
                stratify=y_train, random_state=random_state
            )
        else:
            # Regressionì˜ ê²½ìš° ë‹¨ìˆœ ìƒ˜í”Œë§
            sample_idx = np.random.RandomState(seed=random_state).choice(
                len(X_train), size=sample_size, replace=False
            )
            X_train_sample = X_train.iloc[sample_idx] if isinstance(X_train, pd.DataFrame) else X_train[sample_idx]
            y_train_sample = y_train.iloc[sample_idx] if isinstance(y_train, pd.Series) else y_train[sample_idx]
        print(f"   ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    else:
        X_train_sample = X_train
        y_train_sample = y_train
    
    print(f"{'='*60}")
    
    optimizer = HyperparameterOptimizer(task_type=task_type, random_state=random_state, use_gpu=use_gpu)
    best_params = {}
    
    # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì¤€ë¹„ (ìƒ˜í”Œë§ëœ ë°ì´í„° ì‚¬ìš©)
    encoded_cols = [col for col in X_train_sample.columns if col.endswith(encoded_cols_tag)]
    
    # ê° ëª¨ë¸ë³„ë¡œ ìµœì í™”
    for model_type in ['xgboost', 'catboost', 'lightgbm']:
        print(f"\n{'='*60}")
        print(f"ğŸ” {model_type.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (n_trials={n_trials})")
        print(f"{'='*60}")
        
        # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì„ íƒ
        if model_type == 'catboost':
            # CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ + ì¡°í•© ë²”ì£¼í˜• ì»¬ëŸ¼ ì‚¬ìš© (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸)
            feature_cols = [col for col in X_train_sample.columns if col not in encoded_cols]
            X_train_model = X_train_sample[feature_cols].copy()
            
            # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì—ì„œ ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ ë™ì ìœ¼ë¡œ ì°¾ê¸° (ì¡°í•© íŠ¹ì„± í¬í•¨)
            all_categorical_cols = X_train_model.select_dtypes(include=['object', 'category']).columns.tolist()
            print(f"  CatBoost: ë²”ì£¼í˜• ì»¬ëŸ¼ {len(all_categorical_cols)}ê°œ ì‚¬ìš© (ì›ë³¸ + ì¡°í•© íŠ¹ì„± í¬í•¨)")
            if len(categorical_cols) > 0:
                print(f"    ì›ë³¸ ë²”ì£¼í˜•: {len(categorical_cols)}ê°œ")
                print(f"    ì¡°í•© íŠ¹ì„± í¬í•¨: {len(all_categorical_cols) - len([c for c in categorical_cols if c in all_categorical_cols])}ê°œ")
            
            params = optimizer.optimize_catboost(
                X_train_model, y_train_sample,
                n_trials=n_trials,
                cat_features=all_categorical_cols if all_categorical_cols else None
            )
        elif model_type == 'lightgbm':
            # LightGBM: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)
            all_categorical_cols = X_train_sample.select_dtypes(include=['object', 'category']).columns.tolist()
            feature_cols = [col for col in X_train_sample.columns if col not in all_categorical_cols]
            X_train_model = X_train_sample[feature_cols].copy()
            print(f"  LightGBM: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ {len(encoded_cols)}ê°œ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‚¬ìš© (ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)")
            
            params = optimizer.optimize_lightgbm(
                X_train_model, y_train_sample,
                n_trials=n_trials
            )
        elif model_type == 'xgboost':
            # XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)
            all_categorical_cols = X_train_sample.select_dtypes(include=['object', 'category']).columns.tolist()
            feature_cols = [col for col in X_train_sample.columns if col not in all_categorical_cols]
            X_train_model = X_train_sample[feature_cols].copy()
            print(f"  XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ {len(encoded_cols)}ê°œ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‚¬ìš© (ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)")
            
            params = optimizer.optimize_xgboost(
                X_train_model, y_train_sample,
                n_trials=n_trials
            )
        
        best_params[model_type] = params
        
        print(f"\nâœ… {model_type.upper()} ìµœì í™” ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì  íŒŒë¼ë¯¸í„°:")
        for key, value in sorted(params.items()):
            print(f"  {key:25s}: {value}")
        print()  # ë¹ˆ ì¤„ ì¶”ê°€
    
    # ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì €ì¥
    if params_filepath:
        print(f"\nğŸ’¾ Optuna ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘...")
        save_hyperparameters(
            best_params,
            params_filepath,
            task_type=task_type,
            additional_info={
                'n_trials': n_trials,
                'random_state': random_state,
                'optimization_method': 'Optuna'
            }
        )
    
    return best_params


def save_hyperparameters(best_params: Dict[str, dict], filepath: str, 
                         task_type: str = 'regression', additional_info: Optional[dict] = None):
    """
    ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    
    Parameters:
    -----------
    best_params : dict
        {model_type: params} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    filepath : str
        ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    task_type : str
        ì‘ì—… íƒ€ì…
    additional_info : dict, optional
        ì¶”ê°€ ì •ë³´ (CV ì ìˆ˜, ë‚ ì§œ ë“±)
    """
    save_data = {
        'task_type': task_type,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'hyperparameters': best_params
    }
    
    if additional_info:
        save_data['additional_info'] = additional_info
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì™„ë£Œ: {filepath}")


def load_hyperparameters(filepath: str) -> Dict[str, dict]:
    """
    ì €ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    
    Parameters:
    -----------
    filepath : str
        íŒŒì¼ ê²½ë¡œ
        
    Returns:
    --------
    dict
        {model_type: params} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ“‚ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {filepath}")
    print(f"  Task Type: {data.get('task_type', 'unknown')}")
    print(f"  Timestamp: {data.get('timestamp', 'unknown')}")
    
    return data.get('hyperparameters', {})
