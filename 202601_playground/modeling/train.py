"""
ëª¨ë¸ í•™ìŠµ ë° ì•™ìƒë¸” ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import os
import pandas as pd
import numpy as np

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)
def load_data():
    """ë°ì´í„° ë¡œë“œ í•¨ìˆ˜"""
    import os
    # modeling/train.py -> 202601_playground
    base_dir = os.path.dirname(os.path.dirname(__file__))
    data_dir = os.path.join(base_dir, 'data')
    df_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))
    df_test = pd.read_csv(os.path.join(data_dir, 'test.csv'))
    df_sub = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))
    
    return df_train, df_test, df_sub
from preprocess.encoder import fit_encoder, transform_with_encoder
from preprocess.feature_engineering import apply_feature_engineering_pipeline
from modeling.model import (
    ModelTrainer, EnsembleModel, evaluate_model,
    HyperparameterOptimizer, save_hyperparameters, load_hyperparameters
)


def prepare_data(df_train, df_test, target_col='exam_score', 
                 use_feature_engineering=True, encoding_config=None):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¤€ë¹„
    
    Parameters:
    -----------
    df_train : pd.DataFrame
        í•™ìŠµ ë°ì´í„°
    df_test : pd.DataFrame
        í…ŒìŠ¤íŠ¸ ë°ì´í„°
    target_col : str
        íƒ€ê²Ÿ ì»¬ëŸ¼ ì´ë¦„
    use_feature_engineering : bool
        Feature Engineering ì ìš© ì—¬ë¶€
    encoding_config : dict, optional
        ì¸ì½”ë”© ì„¤ì •
        
    Returns:
    --------
    tuple
        (X_train, y_train, X_test, categorical_cols, numeric_cols, encoder)
    """
    # ë°ì´í„° ë³µì‚¬
    train = df_train.copy()
    test = df_test.copy()
    
    # ID ì»¬ëŸ¼ ì œê±°
    if 'id' in train.columns:
        train = train.drop(columns=['id'])
    if 'id' in test.columns:
        test = test.drop(columns=['id'])
    
    # íƒ€ê²Ÿ ë¶„ë¦¬
    y_train = train[target_col]
    X_train = train.drop(columns=[target_col])
    X_test = test.copy()
    
    # ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜
    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    
    print(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"  í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}")
    print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}")
    print(f"  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
    print(f"  ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_cols)}ê°œ")
    
    # Feature Engineering ì ìš©
    if use_feature_engineering:
        print("\nğŸ”§ Feature Engineering ì ìš© ì¤‘...")
        # ê°„ë‹¨í•œ ì„¤ì • ì˜ˆì œ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
        fe_config = {
            'clip_outliers': True,
            'create_frequency': True,  # ë¹ˆë„ ì¸ì½”ë”©
        }
        
        X_train = apply_feature_engineering_pipeline(
            X_train,
            numeric_cols=numeric_cols,
            categorical_cols=categorical_cols,
            target_col=target_col,
            config=fe_config
        )
        
        X_test = apply_feature_engineering_pipeline(
            X_test,
            numeric_cols=numeric_cols,
            categorical_cols=categorical_cols,
            target_col=None,  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” íƒ€ê²Ÿ ì—†ìŒ
            config=fe_config
        )
        
        # ì—…ë°ì´íŠ¸ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
        categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # ë²”ì£¼í˜• ì¸ì½”ë”© (LightGBM, XGBoostìš©)
    # CatBoostëŠ” ë²”ì£¼í˜•ì„ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ ì›ë³¸ ì»¬ëŸ¼ì„ ìœ ì§€í•˜ê³ ,
    # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ì€ ë³„ë„ë¡œ íƒœê·¸ë¥¼ ë‹¬ì•„ì„œ êµ¬ë¶„
    encoder = None
    original_categorical_cols = categorical_cols.copy()  # CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì €ì¥
    encoded_cols_tag = '_encoded'  # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ íƒœê·¸
    
    if len(categorical_cols) > 0:
        print("\nğŸ”¤ ë²”ì£¼í˜• ì¸ì½”ë”© ì ìš© ì¤‘...")
        print(f"  ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_cols)}ê°œ")
        print(f"  CatBoostëŠ” ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì‚¬ìš©, LightGBM/XGBoostëŠ” ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‚¬ìš©")
        
        if encoding_config is None:
            # í•™ìŠµ ë°ì´í„°ë¡œ ì¸ì½”ë” í•™ìŠµ
            encoder = fit_encoder(
                X_train,
                categorical_cols=categorical_cols,
                encoding_type='ordinal',
                handle_unknown='use_encoded_value',
                unknown_value=-1
            )
            
            # ì¸ì½”ë”ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”© (ì›ë³¸ ì»¬ëŸ¼ì€ ìœ ì§€)
            # OrdinalEncoderëŠ” ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì„ ë®ì–´ì“°ë¯€ë¡œ, ìƒˆë¡œìš´ ì´ë¦„ìœ¼ë¡œ ì¶”ê°€
            train_encoded_array = encoder.transform(X_train[categorical_cols])
            test_encoded_array = encoder.transform(X_test[categorical_cols])
            
            # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ì„ ìƒˆë¡œìš´ ì´ë¦„ìœ¼ë¡œ ì¶”ê°€
            for idx, col in enumerate(categorical_cols):
                encoded_col_name = f"{col}{encoded_cols_tag}"
                X_train[encoded_col_name] = train_encoded_array[:, idx]
                X_test[encoded_col_name] = test_encoded_array[:, idx]
            
            print(f"  ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€: {len(categorical_cols)}ê°œ (íƒœê·¸: '{encoded_cols_tag}')")
            print(f"  ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ìœ ì§€: {len(categorical_cols)}ê°œ (CatBoostìš©)")
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ê°„ë‹¨í•˜ê²Œ í‰ê· /ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´)
    print("\nğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
    for col in numeric_cols:
        if col in X_train.columns and X_train[col].isnull().sum() > 0:
            mean_val = X_train[col].mean()
            X_train[col].fillna(mean_val, inplace=True)
            if col in X_test.columns:
                X_test[col].fillna(mean_val, inplace=True)
    
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    
    # ë°˜í™˜ê°’:
    # - categorical_cols: CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ (ì¸ì½”ë”©ë˜ì§€ ì•ŠìŒ)
    # - encoded_cols_tag: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ íƒœê·¸ (LightGBM/XGBoostì—ì„œ ì‚¬ìš©)
    return X_train, y_train, X_test, original_categorical_cols, numeric_cols, encoder, encoded_cols_tag


def optimize_hyperparameters(X_train, y_train, categorical_cols, task_type='regression',
                              n_trials=50, random_state=42, use_saved_params=False, 
                              params_filepath=None, encoded_cols_tag='_encoded'):
    """
    Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    
    Parameters:
    -----------
    X_train : pd.DataFrame
        í•™ìŠµ ë°ì´í„°
    y_train : pd.Series
        íƒ€ê²Ÿ ë°ì´í„°
    categorical_cols : list
        CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    task_type : str
        ì‘ì—… íƒ€ì… ('regression' or 'classification')
    n_trials : int
        Optuna ì‹œë„ íšŸìˆ˜
    random_state : int
        ëœë¤ ì‹œë“œ
    use_saved_params : bool
        ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
    params_filepath : str, optional
        íŒŒë¼ë¯¸í„° íŒŒì¼ ê²½ë¡œ
    encoded_cols_tag : str
        ì¸ì½”ë”©ëœ ì»¬ëŸ¼ íƒœê·¸
        
    Returns:
    --------
    dict
        ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    """
    if use_saved_params and params_filepath and os.path.exists(params_filepath):
        print(f"\nğŸ“‚ ì €ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸°: {params_filepath}")
        best_params = load_hyperparameters(params_filepath)
        return best_params
    
    print(f"\n{'='*60}")
    print("ğŸ” Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    print(f"{'='*60}")
    
    optimizer = HyperparameterOptimizer(task_type=task_type, random_state=random_state)
    best_params = {}
    
    # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì¤€ë¹„
    encoded_cols = [col for col in X_train.columns if col.endswith(encoded_cols_tag)]
    
    # ê° ëª¨ë¸ë³„ë¡œ ìµœì í™”
    for model_type in ['catboost', 'lightgbm', 'xgboost']:
        print(f"\n{'='*60}")
        print(f"ğŸ” {model_type.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (n_trials={n_trials})")
        print(f"{'='*60}")
        
        # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì„ íƒ
        if model_type == 'catboost':
            # CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì‚¬ìš©
            feature_cols = [col for col in X_train.columns if col not in encoded_cols]
            X_train_model = X_train[feature_cols].copy()
            print(f"  CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ {len(categorical_cols)}ê°œ ì‚¬ìš©")
            
            params = optimizer.optimize_catboost(
                X_train_model, y_train,
                n_trials=n_trials,
                cat_features=categorical_cols
            )
        elif model_type == 'lightgbm':
            # LightGBM: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‚¬ìš©
            feature_cols = [col for col in X_train.columns if col not in categorical_cols]
            X_train_model = X_train[feature_cols].copy()
            print(f"  LightGBM: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ {len(encoded_cols)}ê°œ ì‚¬ìš©")
            
            params = optimizer.optimize_lightgbm(
                X_train_model, y_train,
                n_trials=n_trials
            )
        elif model_type == 'xgboost':
            # XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‚¬ìš©
            feature_cols = [col for col in X_train.columns if col not in categorical_cols]
            X_train_model = X_train[feature_cols].copy()
            print(f"  XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ {len(encoded_cols)}ê°œ ì‚¬ìš©")
            
            params = optimizer.optimize_xgboost(
                X_train_model, y_train,
                n_trials=n_trials
            )
        
        best_params[model_type] = params
        
        print(f"\nâœ… {model_type.upper()} ìµœì í™” ì™„ë£Œ!")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°:")
        for key, value in sorted(params.items()):
            print(f"  {key:25s}: {value}")
    
    # ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì €ì¥
    if params_filepath:
        save_hyperparameters(
            best_params,
            params_filepath,
            task_type=task_type
        )
    
    return best_params


def train_models(X_train, y_train, X_test, categorical_cols, task_type='regression', 
                 n_folds=5, random_state=42, use_optuna=False, n_trials=50,
                 use_saved_params=False, params_filepath=None, encoded_cols_tag='_encoded',
                 use_gpu=False):
    """
    ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
    
    Parameters:
    -----------
    X_train : pd.DataFrame
        í•™ìŠµ ë°ì´í„°
    y_train : pd.Series
        íƒ€ê²Ÿ ë°ì´í„°
    X_test : pd.DataFrame
        í…ŒìŠ¤íŠ¸ ë°ì´í„°
    categorical_cols : list
        CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    task_type : str
        ì‘ì—… íƒ€ì… ('regression' or 'classification')
    n_folds : int
        K-Fold ê°œìˆ˜
    random_state : int
        ëœë¤ ì‹œë“œ
    use_optuna : bool
        Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì—¬ë¶€
    n_trials : int
        Optuna ì‹œë„ íšŸìˆ˜ (use_optuna=Trueì¼ ë•Œ)
    use_saved_params : bool
        ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
    params_filepath : str, optional
        íŒŒë¼ë¯¸í„° íŒŒì¼ ê²½ë¡œ
    encoded_cols_tag : str
        ì¸ì½”ë”©ëœ ì»¬ëŸ¼ íƒœê·¸ (LightGBM/XGBoostì—ì„œ ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‹ë³„ìš©)
    use_gpu : bool
        GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
    Returns:
    --------
    dict
        ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ë° ì˜ˆì¸¡
    """
    trainer = ModelTrainer(task_type=task_type, random_state=random_state, use_gpu=use_gpu)
    
    # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì¤€ë¹„
    # CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì‚¬ìš© (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸)
    # LightGBM/XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‚¬ìš© (ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)
    encoded_cols = [col for col in X_train.columns if col.endswith(encoded_cols_tag)]
    
    def get_features_for_model(model_type):
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì‚¬ìš©í•  ì»¬ëŸ¼ ë°˜í™˜"""
        if model_type == 'catboost':
            # CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸)
            exclude_cols = encoded_cols
            return [col for col in X_train.columns if col not in exclude_cols]
        else:
            # LightGBM/XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)
            exclude_cols = categorical_cols
            return [col for col in X_train.columns if col not in exclude_cols]
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    if use_optuna or use_saved_params:
        # Optuna ìµœì í™” ë˜ëŠ” ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        best_params = optimize_hyperparameters(
            X_train, y_train, categorical_cols,
            task_type=task_type,
            n_trials=n_trials,
            random_state=random_state,
            use_saved_params=use_saved_params,
            params_filepath=params_filepath,
            encoded_cols_tag=encoded_cols_tag
        )
        
        # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ë³€í™˜
        model_configs = {}
        
        # CatBoost íŒŒë¼ë¯¸í„° ë³€í™˜
        if 'catboost' in best_params:
            cb_params = best_params['catboost'].copy()
            cb_params['iterations'] = cb_params.get('iterations', 1000)
            # CatBoostì— í•„ìš”í•œ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
            cb_params.setdefault('random_state', random_state)
            cb_params.setdefault('verbose', False)
            cb_params.setdefault('allow_writing_files', False)
            model_configs['catboost'] = cb_params
        
        # LightGBM íŒŒë¼ë¯¸í„° ë³€í™˜
        if 'lightgbm' in best_params:
            lgb_params = best_params['lightgbm'].copy()
            # LightGBMì€ num_boost_roundë¥¼ fitì—ì„œ ì „ë‹¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
            # í•„ìš”í•œ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
            lgb_params.setdefault('random_state', random_state)
            lgb_params.setdefault('verbosity', -1)
            model_configs['lightgbm'] = lgb_params
        
        # XGBoost íŒŒë¼ë¯¸í„° ë³€í™˜
        if 'xgboost' in best_params:
            xgb_params = best_params['xgboost'].copy()
            # XGBoostëŠ” num_boost_roundë¥¼ fitì—ì„œ ì „ë‹¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
            # í•„ìš”í•œ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
            xgb_params.setdefault('random_state', random_state)
            xgb_params.setdefault('verbosity', 0)
            model_configs['xgboost'] = xgb_params
        
        trainer.best_params = best_params
    else:
        # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        model_configs = {
            'catboost': {
                'iterations': 1000,
                'learning_rate': 0.05,
                'depth': 6,
                'l2_leaf_reg': 3,
                'bootstrap_type': 'Bayesian',
                'random_strength': 1,
                'bagging_temperature': 1,
                'od_type': 'Iter',
                'od_wait': 50,
            },
            'lightgbm': {
                'num_boost_round': 1000,
                'learning_rate': 0.05,
                'num_leaves': 31,
                'max_depth': 6,
                'min_data_in_leaf': 20,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'lambda_l1': 0.1,
                'lambda_l2': 0.1,
            },
            'xgboost': {
                'num_boost_round': 1000,
                'learning_rate': 0.05,
                'max_depth': 6,
                'min_child_weight': 1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'gamma': 0,
                'reg_alpha': 0.1,
                'reg_lambda': 1,
            }
        }
    
    # ê° ëª¨ë¸ í•™ìŠµ
    test_predictions = {}
    
    for model_type in ['catboost', 'lightgbm', 'xgboost']:
        print(f"\n{'='*60}")
        print(f"ğŸš€ {model_type.upper()} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì„ íƒ
        feature_cols = get_features_for_model(model_type)
        X_train_model = X_train[feature_cols].copy()
        X_test_model = X_test[feature_cols].copy()
        
        # CatBoostëŠ” ë²”ì£¼í˜• íŠ¹ì„±ì„ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ cat_features ì „ë‹¬
        # LightGBM/XGBoostëŠ” ì¸ì½”ë”©ëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ cat_featuresëŠ” None
        cat_features = categorical_cols if model_type == 'catboost' else None
        
        if model_type == 'catboost':
            print(f"  ì‚¬ìš© ì»¬ëŸ¼: ì›ë³¸ ë²”ì£¼í˜• {len(categorical_cols)}ê°œ + ìˆ˜ì¹˜í˜• (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸)")
        else:
            print(f"  ì‚¬ìš© ì»¬ëŸ¼: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ {len(encoded_cols)}ê°œ + ìˆ˜ì¹˜í˜• (ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)")
        
        # K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ í•™ìŠµ
        result = trainer.train_with_cv(
            X_train_model, y_train,
            model_type=model_type,
            n_folds=n_folds,
            cat_features=cat_features,
            **model_configs[model_type]
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
        test_pred = trainer.predict_test(X_test_model, model_type)
        test_predictions[model_type] = test_pred
        
        print(f"âœ… {model_type.upper()} í•™ìŠµ ì™„ë£Œ!")
    
    return {
        'trainer': trainer,
        'test_predictions': test_predictions,
        'oof_predictions': trainer.oof_predictions,
        'best_params': trainer.best_params
    }


def create_ensemble(oof_predictions, y_train, test_predictions, task_type='regression'):
    """
    ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° ì˜ˆì¸¡
    
    Parameters:
    -----------
    oof_predictions : dict
        OOF ì˜ˆì¸¡ ë”•ì…”ë„ˆë¦¬
    y_train : pd.Series
        ì‹¤ì œ íƒ€ê²Ÿ ê°’
    test_predictions : dict
        í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë”•ì…”ë„ˆë¦¬
    task_type : str
        ì‘ì—… íƒ€ì…
        
    Returns:
    --------
    np.ndarray
        ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
    """
    print(f"\n{'='*60}")
    print("ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
    print(f"{'='*60}")
    
    # OOF ì˜ˆì¸¡ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ìµœì í™”
    ensemble = EnsembleModel(task_type=task_type)
    ensemble.fit(
        oof_predictions,
        y_train.values if isinstance(y_train, pd.Series) else y_train,
        method='weighted_average',
        optimize=True
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred = ensemble.predict(test_predictions)
    
    print("âœ… ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ!")
    
    return ensemble_pred, ensemble


def main(use_optuna=False, n_trials=50, use_saved_params=False, 
         params_filepath='../data/modeling/best_hyperparameters.json', use_gpu=False):
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    Parameters:
    -----------
    use_optuna : bool
        Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì—¬ë¶€
    n_trials : int
        Optuna ì‹œë„ íšŸìˆ˜
    use_saved_params : bool
        ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
    params_filepath : str
        íŒŒë¼ë¯¸í„° íŒŒì¼ ê²½ë¡œ
    use_gpu : bool
        GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    """
    print("="*60)
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ë° ì•™ìƒë¸” ì‹œì‘")
    print("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df_train, df_test, df_sub = load_data()
    
    # ë°ì´í„° ì¤€ë¹„
    X_train, y_train, X_test, categorical_cols, numeric_cols, encoder, encoded_cols_tag = prepare_data(
        df_train, df_test,
        target_col='exam_score',
        use_feature_engineering=True
    )
    
    # ëª¨ë¸ í•™ìŠµ
    results = train_models(
        X_train, y_train, X_test, categorical_cols,  # CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼
        task_type='regression',
        n_folds=5,
        random_state=42,
        use_optuna=use_optuna,
        n_trials=n_trials,
        use_saved_params=use_saved_params,
        params_filepath=params_filepath,
        encoded_cols_tag=encoded_cols_tag,
        use_gpu=use_gpu
    )
    
    # ì•™ìƒë¸” ìƒì„±
    ensemble_pred, ensemble = create_ensemble(
        results['oof_predictions'],
        y_train,
        results['test_predictions'],
        task_type='regression'
    )
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    print("\nğŸ“ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    submission = pd.DataFrame({
        'id': df_sub['id'],
        'exam_score': ensemble_pred
    })
    
    submission_path = '../submission_ensemble.csv'
    submission.to_csv(submission_path, index=False)
    print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    
    trainer = results['trainer']
    for model_type, scores in trainer.cv_scores.items():
        print(f"\n{model_type.upper()}:")
        print(f"  CV Score: {scores['mean']:.4f} (std: {scores['std']:.4f})")
    
    # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì €ì¥
    if 'best_params' in results and results['best_params']:
        print(f"\nğŸ’¾ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘...")
        save_hyperparameters(
            results['best_params'],
            params_filepath,
            task_type='regression',
            additional_info={
                'cv_scores': {k: {'mean': v['mean'], 'std': v['std']} 
                            for k, v in trainer.cv_scores.items()}
            }
        )
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    
    return results, ensemble_pred, submission


if __name__ == "__main__":
    # Optuna ìµœì í™” ì‚¬ìš© ì—¬ë¶€ ì„¤ì •
    USE_OPTUNA = True  # Trueë¡œ ì„¤ì •í•˜ë©´ Optuna ìµœì í™” ì‹¤í–‰
    N_TRIALS = 50  # Optuna ì‹œë„ íšŸìˆ˜
    USE_SAVED_PARAMS = True  # ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
    PARAMS_FILEPATH = os.path.join(
        os.path.dirname(__file__), 
        'best_hyperparameters.json'
    )
    
    results, ensemble_pred, submission = main(
        use_optuna=USE_OPTUNA,
        n_trials=N_TRIALS,
        use_saved_params=USE_SAVED_PARAMS,
        params_filepath=PARAMS_FILEPATH
    )

