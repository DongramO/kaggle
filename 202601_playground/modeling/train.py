"""
ëª¨ë¸ í•™ìŠµ ë° ì•™ìƒë¸” ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional
from sklearn.metrics import mean_squared_error, roc_auc_score
from preprocess.encoder import fit_encoder, transform_with_encoder, one_hot_encode, ordinal_encode
from preprocess.feature_engineering import clip_outliers, create_interaction_features, create_ratio_features, create_categorical_interactions
from eda import analyze_feature_importance, analyze_permutation_importance, analyze_high_error_samples
from eda.dataload import load_data
from modeling.model import ModelTrainer, EnsembleModel, evaluate_model
from modeling.hyperparameter import (
    HyperparameterOptimizer, 
    save_hyperparameters, 
    load_hyperparameters,
    optimize_hyperparameters
)
# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# eda ëª¨ë“ˆ import (Feature Importance ë¶„ì„)
def prepare_data(df_train, df_test, target_col='exam_score', 
                 use_feature_engineering=True, encoding_config=None):
    # ë°ì´í„° ë³µì‚¬
    train = df_train.copy()
    test = df_test.copy()
    
    # ID ì»¬ëŸ¼ ì œê±°
    if 'id' in train.columns:
        train = train.drop(columns=['id'])
    if 'id' in test.columns:
        test = test.drop(columns=['id'])
    
    # íƒ€ê²Ÿ ë¶„ë¦¬
    y_train = train[target_col]
    X_train = train.drop(columns=[target_col])
    X_test = test.copy()
    
    # ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜ (Feature Engineering ì „ ì›ë³¸ ì»¬ëŸ¼ ì •ë³´ ì €ì¥)
    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    original_categorical_cols_before_fe = categorical_cols.copy()  # Feature Engineering ì „ ë²”ì£¼í˜• ì»¬ëŸ¼ ì €ì¥
    
    print(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"  í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}")
    print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}")
    print(f"  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
    print(f"  ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_cols)}ê°œ")
    
    # Feature Engineering ì ìš© (2ë‹¨ê³„ë¡œ ë¶„ë¦¬)
    if use_feature_engineering:
        print("\nğŸ”§ Feature Engineering ì ìš© ì¤‘...")
        # ê°„ë‹¨í•œ ì„¤ì • ì˜ˆì œ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
        config = {
            'clip_outliers': {
                'flag': True,
                'clip_rules': {
                    'study_hours': 0.99,
                    'class_attendance': 0.99,
                    'sleep_hours': 0.99,
                    'age': 0.99,
                }
            },
            
            'create_interactions_before_encoding': {
                'flag': True,
                'feature_pairs': [
                    ('class_attendance', 'study_hours'),
                    ('study_hours', 'sleep_hours'),
                   
                ],
                'operations': [
                    'multiply',
                    'add',
                ]
            },
            
            'create_interactions_after_encoding': {
                'flag': True,
                'feature_pairs': [
                    ('class_attendance', 'sleep_quality_encoded'),
                    ('study_hours', 'sleep_quality_encoded'),
                    ('study_hours', 'facility_rating_encoded'),
                    ('study_hours', 'exam_difficulty_encoded'),
                ],
                'operations': [
                    'multiply',
                    'multiply',
                    'multiply',
                    'multiply',
                ]
            },
        
            'create_frequency': {
                'flag': False
            },
            
            'create_ratios': {
                'flag': True,
                'numerator_cols': [
                    'study_hours',
                    'class_attendance',
                ],
                'denominator_cols': [
                    'study_hours_add_sleep_hours',
                    'study_hours',
                ],
                'ratio_feature_names': "_ratio"
            },
            'create_categorical_interactions': {
                'flag': True,
                'categorical_pairs': [
                    ('facility_rating', 'exam_difficulty'),
                    ('sleep_quality', 'exam_difficulty'),
                ],
                'separator': '_'
            }
        }
        
        # ========== Feature Engineering 1ë‹¨ê³„: ì¸ì½”ë”© ì „ ==========
        print("  ğŸ“Œ 1ë‹¨ê³„: ì¸ì½”ë”© ì „ Feature Engineering")
        
        # ì´ìƒì¹˜ í´ë¦¬í•‘
        clip_cfg = config.get('clip_outliers', {})
        if clip_cfg.get('flag', False):
            clip_rules = clip_cfg.get('clip_rules', None)
            X_train = clip_outliers(X_train, numeric_cols, clip_rules)
            X_test = clip_outliers(X_test, numeric_cols, clip_rules)
        
        # ë²”ì£¼í˜• ì¡°í•© íŠ¹ì„± ìƒì„± (ì¸ì½”ë”© ì „ì—ë§Œ ê°€ëŠ¥)
        # cat_interaction_cfg = config.get('create_categorical_interactions', {})
        # if cat_interaction_cfg.get('flag', False):
        #     categorical_pairs = cat_interaction_cfg.get('categorical_pairs', [])
        #     separator = cat_interaction_cfg.get('separator', '_')
        #     X_train = create_categorical_interactions(X_train, categorical_pairs, separator)
        #     X_test = create_categorical_interactions(X_test, categorical_pairs, separator)
        
        # ì¸ì½”ë”© ì „ ìˆ˜ì¹˜í˜• ì¡°í•© (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒë“¤)
        interaction_before_cfg = config.get('create_interactions_before_encoding', {})
        if interaction_before_cfg.get('flag', False):
            feature_pairs = interaction_before_cfg.get('feature_pairs', [])
            operations = interaction_before_cfg.get('operations', [])
            X_train = create_interaction_features(X_train, feature_pairs, operations)
            X_test = create_interaction_features(X_test, feature_pairs, operations)

        # ì—…ë°ì´íŠ¸ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
        categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # ë²”ì£¼í˜• ì¸ì½”ë”© (LightGBM, XGBoostìš©)
    encoder = None

    original_categorical_cols = original_categorical_cols_before_fe.copy()  # CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì €ì¥ (FE ì „)
    encoded_cols_tag = '_encoded'  # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ íƒœê·¸
    

    if len(original_categorical_cols_before_fe) > 0:
        categorical_cols_for_encoding = [col for col in original_categorical_cols_before_fe if col in X_train.columns]
    else:

        categorical_cols_for_encoding = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if len(categorical_cols_for_encoding) > 0:
        print("\nğŸ”¤ ë²”ì£¼í˜• ì¸ì½”ë”© ì ìš© ì¤‘...")
        print(f"  ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_cols_for_encoding)}ê°œ")
        print(f"  CatBoostëŠ” ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì‚¬ìš©, LightGBM/XGBoostëŠ” ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‚¬ìš©")
        
        if encoding_config is not None:
            # encoding_configê°€ ìˆìœ¼ë©´ onehot_colsì™€ ordinal_colsë¡œ ë¶„ë¦¬í•˜ì—¬ ì¸ì½”ë”©
            onehot_cols = encoding_config.get('onehot_cols', [])
            ordinal_cols = encoding_config.get('ordinal_cols', [])
            onehot_params = encoding_config.get('onehot_params', {})
            ordinal_params = encoding_config.get('ordinal_params', {})
            
            # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
            onehot_cols = [col for col in onehot_cols if col in X_train.columns]
            ordinal_cols = [col for col in ordinal_cols if col in X_train.columns]
            
            print(f"  OneHot ì¸ì½”ë”©: {onehot_cols} (ê°œìˆ˜: {len(onehot_cols)})")
            print(f"  Ordinal ì¸ì½”ë”©: {ordinal_cols} (ê°œìˆ˜: {len(ordinal_cols)})")
            
            onehot_feature_count = 0
            # OneHot ì¸ì½”ë”©: ì›ë³¸ ìœ ì§€í•˜ê³  ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€
            if len(onehot_cols) > 0:
                onehot_params_clean = {k: v for k, v in onehot_params.items() if k not in ['drop_original']}
                onehot_params_clean.setdefault('sparse', False)
                
                # OneHot ì¸ì½”ë” í•™ìŠµ
                onehot_encoder = fit_encoder(
                    X_train,
                    categorical_cols=onehot_cols,
                    encoding_type='onehot',
                    **{k: v for k, v in onehot_params_clean.items() if k != 'sparse'}
                )
                
                # í•™ìŠµ ë°ì´í„° ë³€í™˜
                train_onehot_array = onehot_encoder.transform(X_train[onehot_cols])
                if hasattr(train_onehot_array, 'toarray'):
                    train_onehot_array = train_onehot_array.toarray()
                onehot_feature_names = onehot_encoder.get_feature_names_out(onehot_cols)
                onehot_feature_count = len(onehot_feature_names)
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜
                test_onehot_array = onehot_encoder.transform(X_test[onehot_cols])
                if hasattr(test_onehot_array, 'toarray'):
                    test_onehot_array = test_onehot_array.toarray()
                
                # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€
                for idx, feature_name in enumerate(onehot_feature_names):
                    encoded_col_name = f"{feature_name}{encoded_cols_tag}"
                    X_train[encoded_col_name] = train_onehot_array[:, idx]
                    X_test[encoded_col_name] = test_onehot_array[:, idx]
            
            # Ordinal ì¸ì½”ë”©: ì›ë³¸ ìœ ì§€í•˜ê³  ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€
            if len(ordinal_cols) > 0:
                ordinal_params_clean = {k: v for k, v in ordinal_params.items() if k != 'drop_original'}
                
                # Ordinal ì¸ì½”ë” í•™ìŠµ
                ordinal_encoder = fit_encoder(
                    X_train,
                    categorical_cols=ordinal_cols,
                    category_orders={
                        'sleep_quality': ['poor', 'average', 'good'],
                        'facility_rating': ['low', 'medium', 'high'],
                        'exam_difficulty': ['easy', 'moderate', 'hard']
                    },
                    encoding_type='ordinal',
                    **ordinal_params_clean
                )
                
                # í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜
                train_ordinal_array = ordinal_encoder.transform(X_train[ordinal_cols])
                test_ordinal_array = ordinal_encoder.transform(X_test[ordinal_cols])
                
                # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€
                for idx, col in enumerate(ordinal_cols):
                    encoded_col_name = f"{col}{encoded_cols_tag}"
                    X_train[encoded_col_name] = train_ordinal_array[:, idx]
                    X_test[encoded_col_name] = test_ordinal_array[:, idx]
            
            total_encoded = onehot_feature_count + len(ordinal_cols)
            
            print(f"  ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€: OneHot {onehot_feature_count}ê°œ + Ordinal {len(ordinal_cols)}ê°œ = ì´ {total_encoded}ê°œ")
        
        
        print(f"  ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ìœ ì§€: {len(categorical_cols_for_encoding)}ê°œ (CatBoostìš©)")
    
    # ========== Feature Engineering 2ë‹¨ê³„: ì¸ì½”ë”© í›„ ==========
    if use_feature_engineering:
        print("\n  ğŸ“Œ 2ë‹¨ê³„: ì¸ì½”ë”© í›„ Feature Engineering")
        
        # ì¸ì½”ë”© í›„ ìˆ˜ì¹˜í˜• ì¡°í•© (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤)
        interaction_after_cfg = config.get('create_interactions_after_encoding', {})
        if interaction_after_cfg.get('flag', False):
            feature_pairs = interaction_after_cfg.get('feature_pairs', [])
            operations = interaction_after_cfg.get('operations', [])
       

            X_train = create_interaction_features(X_train, feature_pairs, operations)
            X_test = create_interaction_features(X_test, feature_pairs, operations)
        
        # ë¹„ìœ¨ íŠ¹ì„± ìƒì„± (ì¸ì½”ë”© í›„ì—ë„ ê°€ëŠ¥)
        ratio_cfg = config.get('create_ratios', {})

        if ratio_cfg.get('flag', False):
            numerator_cols = ratio_cfg.get('numerator_cols', [])
            denominator_cols = ratio_cfg.get('denominator_cols', [])
            feature_names = ratio_cfg.get('ratio_feature_names', None)
       
            X_train = create_ratio_features(X_train, numerator_cols, denominator_cols, feature_names)
            X_test = create_ratio_features(X_test, numerator_cols, denominator_cols, feature_names)
          
        
        # ìµœì¢… ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
        categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in numeric_cols:
        if col in X_train.columns and X_train[col].isnull().sum() > 0:
            mean_val = X_train[col].mean()
            X_train[col].fillna(mean_val, inplace=True)
            if col in X_test.columns:
                X_test[col].fillna(mean_val, inplace=True)
    
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")

    return X_train, y_train, X_test, original_categorical_cols, numeric_cols, encoder, encoded_cols_tag


def train_models(X_train, y_train, X_test, categorical_cols, task_type='regression', 
                 n_folds=5, random_state=42, use_optuna=False, n_trials=50,
                 use_saved_params=None, params_filepath=None, encoded_cols_tag='_encoded',
                 use_gpu=False, optuna_sample_size=None):

    trainer = ModelTrainer(task_type=task_type, random_state=random_state, use_gpu=use_gpu)
    
    # ì €ì¥ëœ íŒŒë¼ë¯¸í„° ìë™ ê°ì§€ (use_saved_paramsê°€ Noneì¸ ê²½ìš°)
    # ë‹¨, use_optuna=Trueì¸ ê²½ìš°ëŠ” Optuna ìµœì í™”ë¥¼ ìš°ì„ í•˜ë¯€ë¡œ use_saved_params=Falseë¡œ ì„¤ì •
    if use_optuna:
        # Optuna ìµœì í™”ë¥¼ ì‹¤í–‰í•˜ëŠ” ê²½ìš°, ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        use_saved_params = False
        print(f"\nğŸ” Optuna ìµœì í™” ëª¨ë“œ: ì €ì¥ëœ íŒŒë¼ë¯¸í„° ë¬´ì‹œí•˜ê³  ìµœì í™” ì‹¤í–‰")
    elif use_saved_params is None and params_filepath and os.path.exists(params_filepath):
        use_saved_params = True
        print(f"\nâœ… ì €ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ìë™ ê°ì§€: {params_filepath}")
    elif use_saved_params is None:
        use_saved_params = False
    
    # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì¤€ë¹„
    # CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì‚¬ìš© (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸)
    # LightGBM/XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì‚¬ìš© (ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)
    print('X_train.columns:', X_train.columns)
    encoded_cols = [col for col in X_train.columns if col.endswith(encoded_cols_tag)]
    
    def get_features_for_model(model_type):
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì‚¬ìš©í•  ì»¬ëŸ¼ ë°˜í™˜"""
        if model_type == 'catboost':
            # CatBoost: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ + ì¡°í•© ë²”ì£¼í˜• ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸)
            exclude_cols = encoded_cols
            return [col for col in X_train.columns if col not in exclude_cols]
        else:
            # LightGBM/XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            # ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸ (ì›ë³¸ + ì¡°í•© íŠ¹ì„± ëª¨ë‘)
            # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì—ì„œ ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ì„ ë™ì ìœ¼ë¡œ ì°¾ì•„ì„œ ì œì™¸
            all_categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
            exclude_cols = all_categorical_cols
            return [col for col in X_train.columns if col not in exclude_cols]
       
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    if use_optuna or use_saved_params:
        # Optuna ìµœì í™” ë˜ëŠ” ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        best_params = optimize_hyperparameters(
            X_train, y_train, categorical_cols,
            task_type=task_type,
            n_trials=n_trials,
            random_state=random_state,
            use_saved_params=use_saved_params,
            params_filepath=params_filepath,
            encoded_cols_tag=encoded_cols_tag,
            use_gpu=use_gpu,
            sample_size=optuna_sample_size
        )
        
        # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ë³€í™˜
        model_configs = {}
        
        # CatBoost íŒŒë¼ë¯¸í„° ë³€í™˜
        if 'catboost' in best_params:
            cb_params = best_params['catboost'].copy()
            cb_params['iterations'] = cb_params.get('iterations', 1000)
            cb_params.setdefault('random_state', random_state)
            cb_params.setdefault('verbose', False)
            cb_params.setdefault('allow_writing_files', False)
            model_configs['catboost'] = cb_params
        
        # LightGBM íŒŒë¼ë¯¸í„° ë³€í™˜
        if 'lightgbm' in best_params:
            lgb_params = best_params['lightgbm'].copy()
            lgb_params.setdefault('random_state', random_state)
            lgb_params.setdefault('verbosity', -1)
            model_configs['lightgbm'] = lgb_params
        
        # XGBoost íŒŒë¼ë¯¸í„° ë³€í™˜
        if 'xgboost' in best_params:
            xgb_params = best_params['xgboost'].copy()
            xgb_params.setdefault('random_state', random_state)
            xgb_params.setdefault('verbosity', 0)
            model_configs['xgboost'] = xgb_params
        
        trainer.best_params = best_params
    
    # ê° ëª¨ë¸ í•™ìŠµ
    test_predictions = {}
    model_features = {}  # ê° ëª¨ë¸ì´ ì‚¬ìš©í•œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì €ì¥
    
    for model_type in ['catboost', 'lightgbm', 'xgboost']:
        print(f"\n{'='*60}")
        print(f"ğŸš€ {model_type.upper()} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        # ëª¨ë¸ë³„ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ ì„ íƒ
        feature_cols = get_features_for_model(model_type)
        model_features[model_type] = feature_cols  # ì‚¬ìš©í•œ ì»¬ëŸ¼ ì €ì¥
        X_train_model = X_train[feature_cols].copy()
        X_test_model = X_test[feature_cols].copy()
        
        # CatBoostëŠ” ë²”ì£¼í˜• íŠ¹ì„±ì„ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ cat_features ì „ë‹¬
        # LightGBM/XGBoostëŠ” ì¸ì½”ë”©ëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ cat_featuresëŠ” None
        if model_type == 'catboost':
            # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ëª¨ë“  ë²”ì£¼í˜• ì»¬ëŸ¼ í•„í„°ë§ (ì¡°í•© íŠ¹ì„± í¬í•¨)
            # ì´ë ‡ê²Œ í•˜ë©´ ë²”ì£¼í˜• ì¡°í•© íŠ¹ì„±ë„ ìë™ìœ¼ë¡œ í¬í•¨ë¨
            actual_categorical_cols = X_train_model.select_dtypes(include=['object', 'category']).columns.tolist()
            actual_numeric_cols = X_train_model.select_dtypes(include=['int64', 'float64']).columns.tolist()
            cat_features = actual_categorical_cols
            print(f"  ì‚¬ìš© ì»¬ëŸ¼: ë²”ì£¼í˜• {len(actual_categorical_cols)}ê°œ (ì›ë³¸ + ì¡°í•© íŠ¹ì„± í¬í•¨) + ìˆ˜ì¹˜í˜• {actual_numeric_cols}")
            print(f"    ì´ í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ")
            print(f"    ë²”ì£¼í˜• ì»¬ëŸ¼: {actual_categorical_cols}")
        else:
            cat_features = None
            print(f"  ì‚¬ìš© ì»¬ëŸ¼: ë²”ì£¼í˜• {len(actual_categorical_cols)}ê°œ (ì›ë³¸ + ì¡°í•© íŠ¹ì„± í¬í•¨) + ìˆ˜ì¹˜í˜• {actual_numeric_cols}")
            print(f"    ì´ í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ")
            print(f"  ì‚¬ìš© ì»¬ëŸ¼: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ {len(encoded_cols)}ê°œ + ìˆ˜ì¹˜í˜• (ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ì œì™¸)")
        
        print('model_configs[model_type]:', model_configs[model_type])
        # K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ í•™ìŠµ
        result = trainer.train_with_cv(
            X_train_model, y_train,
            model_type=model_type,
            n_folds=n_folds,
            cat_features=cat_features,
            **model_configs[model_type]
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
        test_pred = trainer.predict_test(X_test_model, model_type)
        test_predictions[model_type] = test_pred
        
        print(f"âœ… {model_type.upper()} í•™ìŠµ ì™„ë£Œ!")
    
    return {
        'trainer': trainer,
        'test_predictions': test_predictions,
        'oof_predictions': trainer.oof_predictions,
        'best_params': trainer.best_params,
        'model_features': model_features
    }


def create_ensemble(oof_predictions, y_train, test_predictions, task_type='regression'):
 
    print(f"\n{'='*60}")
    print("ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
    print(f"{'='*60}")
    
    # OOF ì˜ˆì¸¡ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ìµœì í™”
    ensemble = EnsembleModel(task_type=task_type)
    ensemble.fit(
        oof_predictions,
        y_train.values if isinstance(y_train, pd.Series) else y_train,
        method='weighted_average',
        optimize=True
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred = ensemble.predict(test_predictions)
    
    print("âœ… ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ!")
    
    return ensemble_pred, ensemble


def main(use_optuna=False, n_trials=50, use_saved_params=None, 
         params_filepath=None, use_gpu=False,
         optuna_sample_size=None, encoding_config=None,
         use_permutation_importance=False):

    print("="*60)
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ë° ì•™ìƒë¸” ì‹œì‘")
    print("="*60)
    
    # params_filepath ê¸°ë³¸ê°’ ì„¤ì •
    if params_filepath is None:
        base_dir = os.path.dirname(os.path.dirname(__file__))
        params_filepath = os.path.join(base_dir, 'best_hyperparameters.json')
    
    # ì €ì¥ëœ íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ìˆê³  use_saved_paramsê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
    if use_saved_params is False and params_filepath and os.path.exists(params_filepath):
        print(f"\nğŸ“‚ ì €ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ë°œê²¬: {params_filepath}")
    elif use_saved_params is True and params_filepath and os.path.exists(params_filepath):
        print(f"\nâœ… ì €ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©: {params_filepath}")
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df_train, df_test, df_sub = load_data()
    
    # ë°ì´í„° ì¤€ë¹„
    X_train, y_train, X_test, categorical_cols, numeric_cols, encoder, encoded_cols_tag = prepare_data(
        df_train, df_test,
        target_col='exam_score',
        use_feature_engineering=True,
        encoding_config=encoding_config
    )
    
    # ëª¨ë¸ í•™ìŠµ
    results = train_models(
        X_train, y_train, X_test, categorical_cols,  # CatBoostìš© ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼
        task_type='regression',
        n_folds=5,
        random_state=42,
        use_optuna=use_optuna,
        n_trials=n_trials,
        use_saved_params=use_saved_params,
        params_filepath=params_filepath,
        encoded_cols_tag=encoded_cols_tag,
        use_gpu=use_gpu,
        optuna_sample_size=optuna_sample_size
    )
    
    # Feature Importance ë¶„ì„
    if analyze_feature_importance is not None:
        print(f"\n{'='*60}")
        print("ğŸ“Š Feature Importance ë¶„ì„ ì‹œì‘")
        print(f"{'='*60}")
        
        base_dir = os.path.dirname(os.path.dirname(__file__))
        feature_importance_dir = os.path.join(base_dir, 'feature_importance_results')
        
       
        feature_importances = analyze_feature_importance(
            trainer=results['trainer'],
            X_train=X_train,
            categorical_cols=categorical_cols,
            encoded_cols_tag=encoded_cols_tag,
            top_n=30,
            save_dir=feature_importance_dir
        )
        results['feature_importances'] = feature_importances
     
    else:
        results['feature_importances'] = None
    
    # Permutation Importance ë¶„ì„
    if use_permutation_importance and analyze_permutation_importance is not None:
        print(f"\n{'='*60}")
        print("ğŸ“Š Permutation Importance ë¶„ì„ ì‹œì‘")
        print(f"{'='*60}")
        
        base_dir = os.path.dirname(os.path.dirname(__file__))
        feature_importance_dir = os.path.join(base_dir, 'feature_importance_results')
        
        permutation_importances = analyze_permutation_importance(
            trainer=results['trainer'],
            X_train=X_train,
            y_train=y_train,
            categorical_cols=categorical_cols,
            encoded_cols_tag=encoded_cols_tag,
            top_n=30,
            n_repeats=10,
            save_dir=feature_importance_dir
        )
        results['permutation_importances'] = permutation_importances
    else:
        if use_permutation_importance:
            print(f"\nâš ï¸ Permutation Importance ë¶„ì„ì„ ìš”ì²­í–ˆì§€ë§Œ analyze_permutation_importanceë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        results['permutation_importances'] = None
    
    # ì•™ìƒë¸” ìƒì„±
    ensemble_pred, ensemble = create_ensemble(
        results['oof_predictions'],
        y_train,
        results['test_predictions'],
        task_type='regression'
    )
    
    # ì•™ìƒë¸” OOF ì˜ˆì¸¡ ê³„ì‚° (ì˜¤ì°¨ ë¶„ì„ìš©)
    ensemble_oof_pred = np.zeros_like(list(results['oof_predictions'].values())[0])
    for name, pred in results['oof_predictions'].items():
        if hasattr(ensemble, 'weights') and ensemble.weights and name in ensemble.weights:
            ensemble_oof_pred += ensemble.weights[name] * pred
    
    # ì˜¤ì°¨ê°€ í° ìƒ˜í”Œ ë¶„ì„
    if analyze_high_error_samples is not None:
        print(f"\n{'='*60}")
        print("ğŸ“Š ì˜¤ì°¨ê°€ í° ìƒ˜í”Œ ë¶„ì„ ì‹œì‘")
        print(f"{'='*60}")
        
        base_dir = os.path.dirname(os.path.dirname(__file__))
        error_analysis_dir = os.path.join(base_dir, 'error_analysis_results')
        
        # ì›ë³¸ í•™ìŠµ ë°ì´í„° ë¡œë“œ (ì˜¤ì°¨ ë¶„ì„ìš©)
        df_train_original, _, _ = load_data()
        
        # ì˜¤ì°¨ ë¶„ì„ ìˆ˜í–‰
        error_samples = analyze_high_error_samples(
            trainer=results['trainer'],
            X_train=df_train_original.drop(columns=['exam_score'] if 'exam_score' in df_train_original.columns else []),
            y_train=y_train,
            ensemble_pred=ensemble_oof_pred,
            top_n=100,
            error_threshold=None,  # ìë™ ì„¤ì •
            save_dir=error_analysis_dir
        )
        results['error_samples'] = error_samples
    else:
        results['error_samples'] = None
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    print("\nğŸ“ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    submission = pd.DataFrame({
        'id': df_sub['id'],
        'exam_score': ensemble_pred
    })
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì €ì¥
    base_dir = os.path.dirname(os.path.dirname(__file__))
    submission_path = os.path.join(base_dir, 'submission.csv')
    
    submission.to_csv(submission_path, index=False)
    print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}")
    print(f"   íŒŒì¼ í¬ê¸°: {os.path.getsize(submission_path) / 1024 / 1024:.2f} MB")
    print(f"   ì˜ˆì¸¡ ê°œìˆ˜: {len(submission)}")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    
    trainer = results['trainer']
    summary_lines = []
    summary_lines.append("="*60)
    summary_lines.append("ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½")
    summary_lines.append("="*60)
    summary_lines.append(f"\nì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_lines.append(f"GPU ì‚¬ìš©: {use_gpu}")
    summary_lines.append(f"\nëª¨ë¸ë³„ CV Score:")
    
    for model_type, scores in trainer.cv_scores.items():
        model_summary = f"\n{model_type.upper()}:"
        model_summary += f"\n  CV Score: {scores['mean']:.4f} (std: {scores['std']:.4f})"
        if 'fold_scores' in scores:
            model_summary += f"\n  Fold Scores: {[f'{s:.4f}' for s in scores['fold_scores']]}"
        # ì‚¬ìš©í•œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        if 'model_features' in results and model_type in results['model_features']:
            features = results['model_features'][model_type]
            model_summary += f"\n  ì‚¬ìš© ì»¬ëŸ¼ ìˆ˜: {len(features)}ê°œ"
            model_summary += f"\n  ì‚¬ìš© ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸: {features}"
        print(model_summary)
        summary_lines.append(model_summary)
    
    # ì•™ìƒë¸” ì •ë³´ ì¶”ê°€
    summary_lines.append(f"\nì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
    if hasattr(ensemble, 'weights') and ensemble.weights:
        for name, weight in ensemble.weights.items():
            weight_info = f"  {name}: {weight:.4f}"
            print(weight_info)
            summary_lines.append(weight_info)
    else:
        summary_lines.append("  (ê°€ì¤‘ì¹˜ ì •ë³´ ì—†ìŒ)")
    
    # ì•™ìƒë¸” ìµœì¢… ì ìˆ˜ ì¶”ê°€
    if hasattr(ensemble, 'ensemble_score') and ensemble.ensemble_score is not None:
        if ensemble.task_type == 'regression':
            ensemble_info = f"\nì•™ìƒë¸” ìµœì¢… RMSE: {ensemble.ensemble_score:.4f}"
        else:
            ensemble_info = f"\nì•™ìƒë¸” ìµœì¢… AUC: {ensemble.ensemble_score:.4f}"
        print(ensemble_info)
        summary_lines.append(ensemble_info)
    else:
        # ensemble_scoreê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ê³„ì‚°
        ensemble_oof_pred = np.zeros_like(list(results['oof_predictions'].values())[0])
        for name, pred in results['oof_predictions'].items():
            if hasattr(ensemble, 'weights') and ensemble.weights and name in ensemble.weights:
                ensemble_oof_pred += ensemble.weights[name] * pred
        
        if ensemble.task_type == 'regression':
            ensemble_score = np.sqrt(mean_squared_error(y_train.values if isinstance(y_train, pd.Series) else y_train, ensemble_oof_pred))
            ensemble_info = f"\nì•™ìƒë¸” ìµœì¢… RMSE: {ensemble_score:.4f}"
        else:
            ensemble_score = roc_auc_score(y_train.values if isinstance(y_train, pd.Series) else y_train, ensemble_oof_pred)
            ensemble_info = f"\nì•™ìƒë¸” ìµœì¢… AUC: {ensemble_score:.4f}"
        print(ensemble_info)
        summary_lines.append(ensemble_info)
    
    # ì¸ì½”ë”© ì„¤ì • ì •ë³´ ì¶”ê°€
    if encoding_config:
        summary_lines.append(f"\nì¸ì½”ë”© ì„¤ì •:")
        summary_lines.append(f"  One-Hot ì»¬ëŸ¼: {encoding_config.get('onehot_cols', [])}")
        summary_lines.append(f"  Ordinal ì»¬ëŸ¼: {encoding_config.get('ordinal_cols', [])}")
    
    # ì œì¶œ íŒŒì¼ ì •ë³´
    summary_lines.append(f"\nì œì¶œ íŒŒì¼ ì •ë³´:")
    summary_lines.append(f"  ê²½ë¡œ: {submission_path}")
    summary_lines.append(f"  íŒŒì¼ í¬ê¸°: {os.path.getsize(submission_path) / 1024 / 1024:.2f} MB")
    summary_lines.append(f"  ì˜ˆì¸¡ ê°œìˆ˜: {len(submission)}")
    summary_lines.append(f"  ì˜ˆì¸¡ê°’ ë²”ìœ„: [{submission['exam_score'].min():.2f}, {submission['exam_score'].max():.2f}]")
    summary_lines.append(f"  ì˜ˆì¸¡ê°’ í‰ê· : {submission['exam_score'].mean():.2f}")
    summary_lines.append(f"  ì˜ˆì¸¡ê°’ í‘œì¤€í¸ì°¨: {submission['exam_score'].std():.2f}")
    
    summary_lines.append(f"\n{'='*60}")
    summary_lines.append("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    summary_lines.append("="*60)
    
    # ê²°ê³¼ ìš”ì•½ íŒŒì¼ ì €ì¥ (ëˆ„ì  í˜•íƒœ)
    summary_text = "\n".join(summary_lines)
    summary_path = os.path.join(base_dir, 'training_summary.txt')
    
    # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì½ì–´ì„œ ë‚´ìš© ìœ ì§€
    existing_content = ""
    execution_count = 0
    if os.path.exists(summary_path):
        try:
            with open(summary_path, 'r', encoding='utf-8') as f:
                existing_content = f.read()
            # ê¸°ì¡´ ì‹¤í–‰ ê¸°ë¡ ê°œìˆ˜ ê³„ì‚° (êµ¬ë¶„ì„  ê¸°ì¤€)
            execution_count = existing_content.count("="*60 + "\nğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ìš”ì•½ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    # ìƒˆë¡œìš´ ë‚´ìš©ì„ ê¸°ì¡´ ë‚´ìš© ë’¤ì— ì¶”ê°€
    with open(summary_path, 'a', encoding='utf-8') as f:
        if existing_content:
            # ê¸°ì¡´ ë‚´ìš©ì´ ìˆìœ¼ë©´ êµ¬ë¶„ì„  ì¶”ê°€
            f.write("\n\n" + "\n" + "="*80 + "\n")
            f.write("="*80 + "\n")
            f.write(f"ìƒˆë¡œìš´ ì‹¤í–‰ ê¸°ë¡ #{execution_count + 1}\n")
            f.write(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n")
            f.write("="*80 + "\n\n")
        f.write(summary_text)
        f.write("\n")
    
    print(f"\nğŸ“ í•™ìŠµ ê²°ê³¼ ìš”ì•½ì´ {summary_path}ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if existing_content:
        print(f"   (ì´ {execution_count + 1}ê°œì˜ ì‹¤í–‰ ê¸°ë¡ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤)")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ (Optuna ìµœì í™”ëœ ê²½ìš° ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„°)
    if 'best_params' in results and results['best_params']:
        best_params = results['best_params']
        # ë¹ˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì €ì¥
        if best_params and len(best_params) > 0:
            # Optunaë¡œ ìµœì í™”ëœ ê²½ìš°ì™€ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ë¶„
            is_optimized = use_optuna and not use_saved_params
            
            print(f"\nğŸ’¾ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘...")
            if is_optimized:
                print("   (Optuna ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)")
            else:
                print("   (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)")
            
            save_hyperparameters(
                best_params,
                params_filepath,
                task_type='regression',
                additional_info={
                    'cv_scores': {k: {'mean': v['mean'], 'std': v['std']} 
                                for k, v in trainer.cv_scores.items()},
                    'is_optimized': is_optimized,
                    'note': 'Optuna ìµœì í™”ëœ íŒŒë¼ë¯¸í„°' if is_optimized else 'ê¸°ë³¸ íŒŒë¼ë¯¸í„°'
                }
            )
        else:
            print(f"\nâš ï¸ ì €ì¥í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    
    return results, ensemble_pred, submission


if __name__ == "__main__":
    # Optuna ìµœì í™” ì‚¬ìš© ì—¬ë¶€ ì„¤ì •
    USE_OPTUNA = True  # Trueë¡œ ì„¤ì •í•˜ë©´ Optuna ìµœì í™” ì‹¤í–‰
    N_TRIALS = 50  # Optuna ì‹œë„ íšŸìˆ˜
    USE_SAVED_PARAMS = True  # ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
    PARAMS_FILEPATH = os.path.join(
        os.path.dirname(__file__), 
        'best_hyperparameters.json'
    )
    
    results, ensemble_pred, submission = main(
        use_optuna=USE_OPTUNA,
        n_trials=N_TRIALS,
        use_saved_params=USE_SAVED_PARAMS,
        params_filepath=PARAMS_FILEPATH
    )

