"""
Feature Importance ë¶„í¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- íŠ¹ì„± ì¤‘ìš”ë„ê°€ í•œìª½ì— ëª°ë ¤ìˆëŠ” í˜„ìƒ ë¶„ì„
- ê°œì„  ë°©ì•ˆ ì œì‹œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
plt.rcParams['axes.unicode_minus'] = False

def load_feature_importance():
    """Feature importance íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    base_dir = Path('feature_importance_results')
    
    # ê° ëª¨ë¸ë³„ ì¤‘ìš”ë„ ë¡œë“œ
    catboost_df = pd.read_csv(base_dir / 'catboost_feature_importance.csv')
    lightgbm_df = pd.read_csv(base_dir / 'lightgbm_feature_importance.csv')
    xgboost_df = pd.read_csv(base_dir / 'xgboost_feature_importance.csv')
    common_df = pd.read_csv(base_dir / 'common_important_features.csv')
    
    return {
        'catboost': catboost_df,
        'lightgbm': lightgbm_df,
        'xgboost': xgboost_df,
        'common': common_df
    }

def analyze_concentration(df, model_name):
    """íŠ¹ì„± ì¤‘ìš”ë„ ì§‘ì¤‘ë„ ë¶„ì„"""
    importance = df['Importance'].values if 'Importance' in df.columns else df['Average_Importance'].values
    
    # í†µê³„ëŸ‰ ê³„ì‚°
    total_importance = importance.sum()
    top1_importance = importance[0]
    top3_importance = importance[:3].sum()
    top5_importance = importance[:5].sum()
    top10_importance = importance[:10].sum()
    
    # ì§‘ì¤‘ë„ ë¹„ìœ¨
    top1_ratio = top1_importance / total_importance * 100
    top3_ratio = top3_importance / total_importance * 100
    top5_ratio = top5_importance / total_importance * 100
    top10_ratio = top10_importance / total_importance * 100
    
    # Gini ê³„ìˆ˜ (ë¶ˆí‰ë“±ë„ ì¸¡ì •)
    sorted_importance = np.sort(importance)[::-1]
    n = len(sorted_importance)
    cumsum = np.cumsum(sorted_importance)
    gini = (2 * np.sum((np.arange(1, n + 1)) * sorted_importance)) / (n * cumsum[-1]) - (n + 1) / n
    
    # ì—”íŠ¸ë¡œí”¼ (ë‹¤ì–‘ì„± ì¸¡ì •, ë†’ì„ìˆ˜ë¡ ë¶„ì‚°ë¨)
    normalized_importance = importance / total_importance
    normalized_importance = normalized_importance[normalized_importance > 0]  # 0 ì œê±°
    entropy = -np.sum(normalized_importance * np.log2(normalized_importance))
    max_entropy = np.log2(len(importance))  # ìµœëŒ€ ì—”íŠ¸ë¡œí”¼
    normalized_entropy = entropy / max_entropy  # ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼ (0~1)
    
    return {
        'model': model_name,
        'total_features': len(df),
        'total_importance': total_importance,
        'top1_importance': top1_importance,
        'top1_ratio': top1_ratio,
        'top3_ratio': top3_ratio,
        'top5_ratio': top5_ratio,
        'top10_ratio': top10_ratio,
        'gini_coefficient': gini,
        'entropy': entropy,
        'normalized_entropy': normalized_entropy,
        'top_features': df.head(10)['Feature'].tolist()
    }

def categorize_features(feature_name):
    """íŠ¹ì„±ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    if '_x_' in feature_name:
        return 'ìƒí˜¸ì‘ìš© (Interaction)'
    elif '_div_' in feature_name or '_ratio' in feature_name or 'ratio' in feature_name:
        return 'ë¹„ìœ¨ (Ratio)'
    elif '_freq' in feature_name:
        return 'ë¹ˆë„ ì¸ì½”ë”© (Frequency)'
    elif feature_name in ['study_hours', 'class_attendance', 'sleep_hours', 'age']:
        return 'ì›ë³¸ ìˆ˜ì¹˜í˜• (Original Numeric)'
    elif any(col in feature_name for col in ['sleep_quality', 'study_method', 'facility_rating', 
                                              'course', 'gender', 'internet_access', 'exam_difficulty']):
        if '_encoded' in feature_name:
            return 'Ordinal ì¸ì½”ë”©'
        else:
            return 'One-Hot ì¸ì½”ë”©'
    else:
        return 'ê¸°íƒ€ (Other)'

def analyze_feature_categories(df):
    """íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ë¶„ì„"""
    df = df.copy()
    df['Category'] = df['Feature'].apply(categorize_features)
    
    importance_col = 'Importance' if 'Importance' in df.columns else 'Average_Importance'
    
    category_stats = df.groupby('Category').agg({
        importance_col: ['sum', 'mean', 'count']
    }).round(4)
    
    category_stats.columns = ['Total_Importance', 'Mean_Importance', 'Count']
    category_stats = category_stats.sort_values('Total_Importance', ascending=False)
    
    return category_stats, df

def generate_recommendations(concentration_stats, category_stats):
    """ê°œì„  ë°©ì•ˆ ì œì‹œ"""
    recommendations = []
    
    # ì§‘ì¤‘ë„ ë¶„ì„
    avg_top1_ratio = np.mean([s['top1_ratio'] for s in concentration_stats.values()])
    avg_top3_ratio = np.mean([s['top3_ratio'] for s in concentration_stats.values()])
    avg_entropy = np.mean([s['normalized_entropy'] for s in concentration_stats.values()])
    
    recommendations.append("=" * 80)
    recommendations.append("ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ì§‘ì¤‘ë„ ë¶„ì„ ê²°ê³¼")
    recommendations.append("=" * 80)
    recommendations.append(f"\n1. ìƒìœ„ 1ê°œ íŠ¹ì„± ì§‘ì¤‘ë„: í‰ê·  {avg_top1_ratio:.1f}%")
    recommendations.append(f"2. ìƒìœ„ 3ê°œ íŠ¹ì„± ì§‘ì¤‘ë„: í‰ê·  {avg_top3_ratio:.1f}%")
    recommendations.append(f"3. ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼: {avg_entropy:.3f} (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶„ì‚°ë¨)")
    
    # í•´ì„
    recommendations.append("\n" + "-" * 80)
    recommendations.append("ğŸ” ë¶„ì„ í•´ì„")
    recommendations.append("-" * 80)
    
    if avg_top1_ratio > 40:
        recommendations.append("âš ï¸ ê²½ê³ : ìƒìœ„ 1ê°œ íŠ¹ì„±ì´ ì „ì²´ì˜ 40% ì´ìƒì„ ì°¨ì§€í•©ë‹ˆë‹¤.")
        recommendations.append("   â†’ ëª¨ë¸ì´ ë‹¨ì¼ íŠ¹ì„±ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif avg_top1_ratio > 30:
        recommendations.append("ğŸ’¡ ì£¼ì˜: ìƒìœ„ 1ê°œ íŠ¹ì„±ì´ 30-40%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.")
        recommendations.append("   â†’ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ ë‹¤ë¥¸ íŠ¹ì„±ì˜ ì¤‘ìš”ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        recommendations.append("âœ… ì–‘í˜¸: íŠ¹ì„± ì¤‘ìš”ë„ê°€ ë¹„êµì  ê· ë“±í•˜ê²Œ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    if avg_entropy < 0.7:
        recommendations.append(f"\nâš ï¸ ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ìŒ ({avg_entropy:.3f}): íŠ¹ì„± ì¤‘ìš”ë„ê°€ í•œìª½ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        recommendations.append("   â†’ ë” ë§ì€ ìœ ì˜ë¯¸í•œ íŠ¹ì„±ì„ ìƒì„±í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.")
    else:
        recommendations.append(f"\nâœ… ì—”íŠ¸ë¡œí”¼ê°€ ì–‘í˜¸í•¨ ({avg_entropy:.3f}): íŠ¹ì„± ì¤‘ìš”ë„ê°€ ë¹„êµì  ê· ë“±í•˜ê²Œ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    recommendations.append("\n" + "-" * 80)
    recommendations.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ë¶„ì„")
    recommendations.append("-" * 80)
    recommendations.append(category_stats.to_string())
    
    # ê°œì„  ë°©ì•ˆ
    recommendations.append("\n" + "=" * 80)
    recommendations.append("ğŸ’¡ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê°œì„  ë°©ì•ˆ")
    recommendations.append("=" * 80)
    
    # ì›ë³¸ ìˆ˜ì¹˜í˜•ì´ ë„ˆë¬´ ê°•í•œ ê²½ìš°
    if 'ì›ë³¸ ìˆ˜ì¹˜í˜• (Original Numeric)' in category_stats.index:
        numeric_ratio = category_stats.loc['ì›ë³¸ ìˆ˜ì¹˜í˜• (Original Numeric)', 'Total_Importance']
        if numeric_ratio > 0.5:
            recommendations.append("\n1. ì›ë³¸ ìˆ˜ì¹˜í˜• íŠ¹ì„± ë³€í™˜ ì‹œë„:")
            recommendations.append("   - ë¡œê·¸ ë³€í™˜: np.log1p(study_hours)")
            recommendations.append("   - ì œê³±ê·¼ ë³€í™˜: np.sqrt(study_hours)")
            recommendations.append("   - êµ¬ê°„í™”(Binning): study_hoursë¥¼ 5-10ê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• ")
            recommendations.append("   - ì´ìƒì¹˜ ì²˜ë¦¬: IQR ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° ë˜ëŠ” í´ë¦¬í•‘")
    
    # ìƒí˜¸ì‘ìš© íŠ¹ì„± í™•ëŒ€
    if 'ìƒí˜¸ì‘ìš© (Interaction)' in category_stats.index:
        interaction_ratio = category_stats.loc['ìƒí˜¸ì‘ìš© (Interaction)', 'Total_Importance']
        if interaction_ratio < 0.2:
            recommendations.append("\n2. ìƒí˜¸ì‘ìš© íŠ¹ì„± í™•ëŒ€:")
            recommendations.append("   - í˜„ì¬: study_hours Ã— sleep_hoursë§Œ ì‚¬ìš©")
            recommendations.append("   - ì¶”ê°€ ê°€ëŠ¥:")
            recommendations.append("     * study_hours Ã— class_attendance")
            recommendations.append("     * age Ã— study_hours")
            recommendations.append("     * (study_hours / sleep_hours) Ã— class_attendance")
            recommendations.append("   - ë‹¤í•­ì‹ íŠ¹ì„±: PolynomialFeatures(degree=2) ì‚¬ìš©")
    
    # ë¹„ìœ¨ íŠ¹ì„± í™•ëŒ€
    if 'ë¹„ìœ¨ (Ratio)' in category_stats.index:
        ratio_ratio = category_stats.loc['ë¹„ìœ¨ (Ratio)', 'Total_Importance']
        if ratio_ratio < 0.1:
            recommendations.append("\n3. ë¹„ìœ¨ íŠ¹ì„± í™•ëŒ€:")
            recommendations.append("   - í•™ìŠµ íš¨ìœ¨: study_hours / sleep_hours")
            recommendations.append("   - ì¶œì„ íš¨ìœ¨: class_attendance / study_hours")
            recommendations.append("   - ì‹œê°„ í• ë‹¹: study_hours / (study_hours + sleep_hours)")
            recommendations.append("   - ë‚˜ì´ ëŒ€ë¹„ í•™ìŠµëŸ‰: study_hours / age")
    
    # ë²”ì£¼í˜• íŠ¹ì„± í™œìš© ê°œì„ 
    onehot_importance = category_stats[category_stats.index.str.contains('One-Hot', na=False)]['Total_Importance'].sum()
    if onehot_importance < 0.15:
        recommendations.append("\n4. ë²”ì£¼í˜• íŠ¹ì„± í™œìš© ê°œì„ :")
        recommendations.append("   - Target Encoding: íƒ€ê²Ÿ ë³€ìˆ˜ ê¸°ë°˜ ì¸ì½”ë”©")
        recommendations.append("   - ë²”ì£¼í˜• ì¡°í•©: study_method Ã— facility_rating")
        recommendations.append("   - ìˆœì„œí˜• íŠ¹ì„±: ë²”ì£¼í˜• íŠ¹ì„±ì— ìˆœì„œ ë¶€ì—¬")
    
    # í†µê³„ì  íŠ¹ì„±
    recommendations.append("\n5. í†µê³„ì  íŠ¹ì„± ì¶”ê°€:")
    recommendations.append("   - ê·¸ë£¹ë³„ í†µê³„: study_methodë³„ study_hoursì˜ í‰ê· /í‘œì¤€í¸ì°¨")
    recommendations.append("   - ëˆ„ì  íŠ¹ì„±: ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ëˆ„ì  í‰ê· ")
    recommendations.append("   - ìˆœìœ„ íŠ¹ì„±: ê° íŠ¹ì„±ì˜ ìˆœìœ„ ë˜ëŠ” ë°±ë¶„ìœ„ìˆ˜")
    
    # ì‹œê³„ì—´/ìˆœì„œ íŠ¹ì„± (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
    recommendations.append("\n6. ìˆœì„œ/ì‹œê°„ íŠ¹ì„±:")
    recommendations.append("   - í•™ìŠµëŸ‰ ë³€í™”ìœ¨: (í˜„ì¬ study_hours - ì´ì „ study_hours)")
    recommendations.append("   - ëˆ„ì  í•™ìŠµëŸ‰: study_hoursì˜ ëˆ„ì í•©")
    
    recommendations.append("\n" + "=" * 80)
    recommendations.append("âš ï¸ ì£¼ì˜ì‚¬í•­")
    recommendations.append("=" * 80)
    recommendations.append("1. íŠ¹ì„± ì¤‘ìš”ë„ê°€ í•œìª½ì— ëª°ë ¤ìˆë‹¤ê³  ë°˜ë“œì‹œ ë¬¸ì œëŠ” ì•„ë‹™ë‹ˆë‹¤.")
    recommendations.append("   - study_hoursê°€ ì‹¤ì œë¡œ ê°€ì¥ ì˜ˆì¸¡ë ¥ì´ ë†’ì€ íŠ¹ì„±ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    recommendations.append("   - ì‹œí—˜ ì ìˆ˜ ì˜ˆì¸¡ì—ì„œ ê³µë¶€ ì‹œê°„ì´ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    recommendations.append("\n2. ê³¼ë„í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì€ ì˜¤íˆë ¤ ê³¼ì í•©ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    recommendations.append("   - Cross-validationì„ í†µí•´ ê²€ì¦í•˜ì„¸ìš”.")
    recommendations.append("   - Feature importanceê°€ ë‚®ë‹¤ê³  ë¬´ì¡°ê±´ ì œê±°í•˜ì§€ ë§ˆì„¸ìš”.")
    recommendations.append("\n3. ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„í¬ëŠ” ë¶€ì°¨ì ì…ë‹ˆë‹¤.")
    recommendations.append("   - ìµœì¢… ëª©í‘œëŠ” ì˜ˆì¸¡ ì •í™•ë„ì…ë‹ˆë‹¤.")
    recommendations.append("   - íŠ¹ì„± ì¤‘ìš”ë„ëŠ” 'ì„¤ëª…ë ¥'ì„ ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.")
    
    return "\n".join(recommendations)

def visualize_concentration(importance_dict, save_path='feature_importance_results/concentration_analysis.png'):
    """ì§‘ì¤‘ë„ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('íŠ¹ì„± ì¤‘ìš”ë„ ì§‘ì¤‘ë„ ë¶„ì„', fontsize=16, fontweight='bold', y=0.995)
    
    # 1. ê° ëª¨ë¸ë³„ ìƒìœ„ íŠ¹ì„± ì¤‘ìš”ë„ (Bar Chart)
    ax1 = axes[0, 0]
    models = ['catboost', 'lightgbm', 'xgboost']
    top1_values = []
    top3_values = []
    top5_values = []
    
    for model in models:
        df = importance_dict[model]
        importance = df['Importance'].values if 'Importance' in df.columns else df['Average_Importance'].values
        total = importance.sum()
        top1_values.append(importance[0] / total * 100)
        top3_values.append(importance[:3].sum() / total * 100)
        top5_values.append(importance[:5].sum() / total * 100)
    
    x = np.arange(len(models))
    width = 0.25
    ax1.bar(x - width, top1_values, width, label='Top 1', color='#FF6B6B', alpha=0.8)
    ax1.bar(x, top3_values, width, label='Top 3', color='#4ECDC4', alpha=0.8)
    ax1.bar(x + width, top5_values, width, label='Top 5', color='#45B7D1', alpha=0.8)
    
    ax1.set_xlabel('ëª¨ë¸', fontsize=12)
    ax1.set_ylabel('ì¤‘ìš”ë„ ë¹„ìœ¨ (%)', fontsize=12)
    ax1.set_title('ìƒìœ„ íŠ¹ì„±ì˜ ì¤‘ìš”ë„ ì§‘ì¤‘ë„', fontsize=13, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels([m.upper() for m in models])
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3, linestyle='--')
    
    # 2. ê³µí†µ ì¤‘ìš”ë„ ë¶„í¬ (Pareto Chart)
    ax2 = axes[0, 1]
    common_df = importance_dict['common'].head(15)
    importance_col = 'Average_Importance'
    y_pos = np.arange(len(common_df))
    
    bars = ax2.barh(y_pos, common_df[importance_col].values, color='steelblue', alpha=0.7)
    ax2.set_yticks(y_pos)
    ax2.set_yticklabels(common_df['Feature'].values, fontsize=9)
    ax2.set_xlabel('í‰ê·  ì¤‘ìš”ë„', fontsize=12)
    ax2.set_title('ê³µí†µ ìƒìœ„ 15ê°œ íŠ¹ì„±', fontsize=13, fontweight='bold')
    ax2.grid(axis='x', alpha=0.3, linestyle='--')
    
    # ëˆ„ì  ë¹„ìœ¨ ë¼ì¸ ì¶”ê°€
    ax2_twin = ax2.twinx()
    cumulative = np.cumsum(common_df[importance_col].values) / common_df[importance_col].sum() * 100
    ax2_twin.plot(cumulative, y_pos, color='red', marker='o', linewidth=2, markersize=4)
    ax2_twin.set_ylabel('ëˆ„ì  ë¹„ìœ¨ (%)', fontsize=12, color='red')
    ax2_twin.set_ylim(ax2.get_ylim())
    ax2_twin.grid(alpha=0.3, linestyle='--', axis='y')
    
    # 3. ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ (Pie Chart)
    ax3 = axes[1, 0]
    common_df_with_cat = importance_dict['common'].copy()
    common_df_with_cat['Category'] = common_df_with_cat['Feature'].apply(categorize_features)
    category_importance = common_df_with_cat.groupby('Category')[importance_col].sum().sort_values(ascending=False)
    
    colors = plt.cm.Set3(np.linspace(0, 1, len(category_importance)))
    wedges, texts, autotexts = ax3.pie(category_importance.values, 
                                        labels=category_importance.index,
                                        autopct='%1.1f%%',
                                        startangle=90,
                                        colors=colors)
    ax3.set_title('ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ë¶„í¬', fontsize=13, fontweight='bold')
    for autotext in autotexts:
        autotext.set_color('black')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(9)
    
    # 4. ì—”íŠ¸ë¡œí”¼ ë¹„êµ
    ax4 = axes[1, 1]
    models = ['catboost', 'lightgbm', 'xgboost']
    entropies = []
    
    for model in models:
        df = importance_dict[model]
        importance = df['Importance'].values if 'Importance' in df.columns else df['Average_Importance'].values
        total = importance.sum()
        normalized = importance / total
        normalized = normalized[normalized > 0]
        entropy = -np.sum(normalized * np.log2(normalized))
        max_entropy = np.log2(len(importance))
        normalized_entropy = entropy / max_entropy
        entropies.append(normalized_entropy)
    
    bars = ax4.bar(models, entropies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.7)
    ax4.set_ylabel('ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼', fontsize=12)
    ax4.set_title('ëª¨ë¸ë³„ íŠ¹ì„± ì¤‘ìš”ë„ ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼)', fontsize=13, fontweight='bold')
    ax4.set_ylim([0, 1])
    ax4.axhline(y=0.7, color='red', linestyle='--', linewidth=2, label='ê¶Œì¥ ìµœì†Œê°’ (0.7)')
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3, linestyle='--')
    
    # ê°’ í‘œì‹œ
    for bar, entropy in zip(bars, entropies):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{entropy:.3f}',
                ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout(rect=[0, 0, 1, 0.98])
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close()

def main():
    print("=" * 80)
    print("ğŸ“Š Feature Importance ì§‘ì¤‘ë„ ë¶„ì„ ì‹œì‘")
    print("=" * 80)
    
    # ë°ì´í„° ë¡œë“œ
    importance_dict = load_feature_importance()
    
    # ì§‘ì¤‘ë„ ë¶„ì„
    concentration_stats = {}
    for model_name in ['catboost', 'lightgbm', 'xgboost']:
        stats = analyze_concentration(importance_dict[model_name], model_name)
        concentration_stats[model_name] = stats
        
        print(f"\n{model_name.upper()} ì§‘ì¤‘ë„ ë¶„ì„:")
        print(f"  ìƒìœ„ 1ê°œ íŠ¹ì„± ë¹„ìœ¨: {stats['top1_ratio']:.1f}%")
        print(f"  ìƒìœ„ 3ê°œ íŠ¹ì„± ë¹„ìœ¨: {stats['top3_ratio']:.1f}%")
        print(f"  ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼: {stats['normalized_entropy']:.3f}")
        print(f"  ìµœìƒìœ„ íŠ¹ì„±: {stats['top_features'][0]}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    category_stats, common_df_with_cat = analyze_feature_categories(importance_dict['common'])
    
    # ê°œì„  ë°©ì•ˆ ìƒì„±
    recommendations = generate_recommendations(concentration_stats, category_stats)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = Path('feature_importance_results')
    output_dir.mkdir(exist_ok=True)
    
    with open(output_dir / 'concentration_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write(recommendations)
    
    print("\n" + recommendations)
    
    # ì‹œê°í™”
    visualize_concentration(importance_dict, output_dir / 'concentration_analysis.png')
    
    # ìƒì„¸ í†µê³„ ì €ì¥
    stats_df = pd.DataFrame(concentration_stats).T
    stats_df.to_csv(output_dir / 'concentration_statistics.csv', index=True, encoding='utf-8-sig')
    
    category_stats.to_csv(output_dir / 'category_statistics.csv', index=True, encoding='utf-8-sig')
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"   - ë¦¬í¬íŠ¸: {output_dir / 'concentration_analysis_report.txt'}")
    print(f"   - ì‹œê°í™”: {output_dir / 'concentration_analysis.png'}")
    print(f"   - í†µê³„: {output_dir / 'concentration_statistics.csv'}")

if __name__ == '__main__':
    main()
