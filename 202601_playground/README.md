# Kaggle Competition - ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸

ì´ í”„ë¡œì íŠ¸ëŠ” CatBoost, LightGBM, XGBoost ëª¨ë¸ì„ ì‚¬ìš©í•œ ì•™ìƒë¸” í•™ìŠµ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì™€ GPU ì§€ì›ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì„¸ìŠ¤ ê°œìš”

ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```
1. ë°ì´í„° ë¡œë“œ
   â†“
2. ë°ì´í„° ì „ì²˜ë¦¬ ë° Feature Engineering
   â†“
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Optuna, ì„ íƒì‚¬í•­)
   â†“
4. ëª¨ë¸ í•™ìŠµ (K-Fold Cross-Validation)
   - CatBoost
   - LightGBM
   - XGBoost
   â†“
5. ì•™ìƒë¸” ëª¨ë¸ ìƒì„± (ê°€ì¤‘ì¹˜ ìµœì í™”)
   â†“
6. ì œì¶œ íŒŒì¼ ìƒì„±
```

## ğŸ”„ ìƒì„¸ í”„ë¡œì„¸ìŠ¤

### 1. ë°ì´í„° ë¡œë“œ (`load_data()`)
- **ì…ë ¥ íŒŒì¼**: 
  - `train.csv`: í•™ìŠµ ë°ì´í„° (í”¼ì²˜ + íƒ€ê²Ÿ)
  - `test.csv`: í…ŒìŠ¤íŠ¸ ë°ì´í„° (í”¼ì²˜ë§Œ)
  - `sample_submission.csv`: ì œì¶œ í…œí”Œë¦¿ (id ì»¬ëŸ¼)
  
- **í™˜ê²½ ìë™ ê°ì§€**:
  - Kaggle í™˜ê²½: `/kaggle/input` ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ë¡œ íŒë‹¨
  - ë¡œì»¬ í™˜ê²½: í”„ë¡œì íŠ¸ ë‚´ `data/` ë””ë ‰í† ë¦¬ ì‚¬ìš©
  - ê²½ë¡œë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ë‘ í™˜ê²½ ëª¨ë‘ ì§€ì›

- **ì¶œë ¥**: 
  - `df_train`: í•™ìŠµìš© DataFrame
  - `df_test`: í…ŒìŠ¤íŠ¸ìš© DataFrame  
  - `df_sub`: ì œì¶œìš© í…œí”Œë¦¿ DataFrame

### 2. ë°ì´í„° ì „ì²˜ë¦¬ (`prepare_data()`)

#### 2.1 ê¸°ë³¸ ì „ì²˜ë¦¬
- **ID ì»¬ëŸ¼ ì œê±°**: `id` ì»¬ëŸ¼ì„ í•™ìŠµì—ì„œ ì œì™¸ (ì‹ë³„ìë§Œ í¬í•¨)
- **íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬**: 
  - `y_train = train['exam_score']` (íšŒê·€ íƒ€ê²Ÿ)
  - `X_train = train.drop(columns=['exam_score'])` (í”¼ì²˜ë§Œ)
  
#### 2.2 ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜
- **ìˆ˜ì¹˜í˜• ì»¬ëŸ¼** (`numeric_cols`): `int64`, `float64` íƒ€ì…
  - ì˜ˆ: `age`, `study_hours`, `class_attendance`, `sleep_hours`
- **ë²”ì£¼í˜• ì»¬ëŸ¼** (`categorical_cols`): `object`, `category` íƒ€ì…
  - ì˜ˆ: `gender`, `course`, `internet_access`, `sleep_quality`, `study_method`, `facility_rating`

#### 2.3 Feature Engineering (`apply_feature_engineering_pipeline()`)

**ì´ìƒì¹˜ í´ë¦¬í•‘** (`clip_outliers()`):
- **ëª©ì **: ê·¹ë‹¨ê°’ì´ ëª¨ë¸ ì„±ëŠ¥ì— ì•…ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ ë°©ì§€
- **ë°©ë²•**: Quantile ê¸°ë°˜ í´ë¦¬í•‘
- **í´ë¦¬í•‘ ê·œì¹™** (ê¸°ë³¸ê°’):
  ```python
  clip_rules = {
      'study_hours': 0.995,      # 99.5% ë¶„ìœ„ìˆ˜ë¡œ í´ë¦¬í•‘
      'class_attendance': 0.99,   # 99% ë¶„ìœ„ìˆ˜ë¡œ í´ë¦¬í•‘
      'sleep_hours': 0.995,       # 99.5% ë¶„ìœ„ìˆ˜ë¡œ í´ë¦¬í•‘
      'age': 0.99                 # 99% ë¶„ìœ„ìˆ˜ë¡œ í´ë¦¬í•‘
  }
  ```
- **ì›ë¦¬**: ê° ì»¬ëŸ¼ì˜ ìƒìœ„ 0.5~1% ê·¹ë‹¨ê°’ì„ í•´ë‹¹ ë¶„ìœ„ìˆ˜ ê°’ìœ¼ë¡œ ì œí•œ

**ë¹ˆë„ ì¸ì½”ë”©** (`create_frequency_features()`):
- **ëª©ì **: ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ë¹ˆë„ ì •ë³´ë¥¼ ìˆ˜ì¹˜í˜• íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜
- **ìƒì„± ë°©ì‹**: `{categorical_col}_freq` ì»¬ëŸ¼ ìƒì„±
- **ì˜ˆì‹œ**: `gender_freq`, `course_freq`, `study_method_freq`
- **íŠ¹ì§•**: ê° ë²”ì£¼ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµ ê°€ëŠ¥

**ìƒí˜¸ì‘ìš© íŠ¹ì„±** (`create_interaction_features()`):
- **ëª©ì **: ë‘ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ì‘ìš© íš¨ê³¼ í¬ì°©
- **ìƒì„± ê·œì¹™** (í˜„ì¬ ì„¤ì •):
  ```python
  feature_pairs = [
      ('class_attendance', 'sleep_hours'),  # ìˆ˜ì—… ì¶œì„ë¥  Ã— ìˆ˜ë©´ ì‹œê°„
      ('study_hours', 'sleep_hours')        # ê³µë¶€ ì‹œê°„ Ã— ìˆ˜ë©´ ì‹œê°„
  ]
  operations = ['multiply']  # ê³±ì…ˆë§Œ ìƒì„±
  ```
- **ìƒì„± íŠ¹ì„±**:
  - `class_attendance_x_sleep_hours`: ì¶œì„ë¥ ê³¼ ìˆ˜ë©´ ì‹œê°„ì˜ ê³±
  - `study_hours_x_sleep_hours`: ê³µë¶€ ì‹œê°„ê³¼ ìˆ˜ë©´ ì‹œê°„ì˜ ê³±
- **ì¶”ê°€ ê°€ëŠ¥í•œ ì—°ì‚°**: `multiply`, `divide`, `add`, `subtract` (í˜„ì¬ëŠ” ê³±ì…ˆë§Œ ì‚¬ìš©)

**ë¹„ìœ¨ íŠ¹ì„±** (`create_ratio_features()`, í˜„ì¬ ë¯¸ì‚¬ìš©):
- **ëª©ì **: ë‘ ë³€ìˆ˜ ê°„ì˜ ë¹„ìœ¨ ê´€ê³„ í‘œí˜„
- **ì˜ˆì‹œ**: `age / study_hours` (ë‚˜ì´ ëŒ€ë¹„ ê³µë¶€ ì‹œê°„ ë¹„ìœ¨)

#### 2.4 ë²”ì£¼í˜• ì¸ì½”ë”© ì „ëµ

**ëª¨ë¸ë³„ ì¸ì½”ë”© ì°¨ë³„í™”**:

1. **CatBoostìš© (ì›ë³¸ ë²”ì£¼í˜• ìœ ì§€)**
   - CatBoostëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ **ì›ë³¸ ì»¬ëŸ¼ ìœ ì§€**
   - `cat_features` íŒŒë¼ë¯¸í„°ë¡œ ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì „ë‹¬
   - ì˜ˆ: `gender`, `course`, `internet_access` ë“± ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©

2. **LightGBM/XGBoostìš© (ì¸ì½”ë”© í•„ìš”)**
   - Tree ê¸°ë°˜ ëª¨ë¸ì€ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì§ì ‘ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ ì¸ì½”ë”© í•„ìˆ˜
   - **ì¸ì½”ë”© ë°©ì‹**:
     - **One-Hot Encoding**: ì„¤ì • ê°€ëŠ¥í•œ ì»¬ëŸ¼ì— ëŒ€í•´ ì‚¬ìš© (ê¸°ë³¸ê°’: `gender`, `course` ë“±)
       - ê° ë²”ì£¼ë¥¼ ë³„ë„ì˜ ì´ì§„ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
       - ì˜ˆ: `gender_Male`, `gender_Female` (ì›ë³¸ ì»¬ëŸ¼ ì œê±° ê°€ëŠ¥)
     - **Ordinal Encoding**: ì„¤ì • ê°€ëŠ¥í•œ ì»¬ëŸ¼ì— ëŒ€í•´ ì‚¬ìš© (ê¸°ë³¸ê°’: `exam_difficulty`)
       - ê° ë²”ì£¼ë¥¼ ìˆœì„œ ìˆëŠ” ì •ìˆ˜ë¡œ ë§¤í•‘
       - ì˜ˆ: `exam_difficulty`: `Easy=0`, `Medium=1`, `Hard=2`
       - `_encoded` íƒœê·¸ê°€ ë¶™ì€ ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥ (ì›ë³¸ ìœ ì§€)

3. **ì¸ì½”ë”©ëœ ì»¬ëŸ¼ êµ¬ë¶„**:
   - ì›ë³¸ ë²”ì£¼í˜•: CatBoost ì „ìš©
   - `*_encoded`: LightGBM/XGBoost ì „ìš© (Ordinal)
   - `*_0`, `*_1`, ...: One-Hot ì¸ì½”ë”© ê²°ê³¼

**ì¸ì½”ë”© ì„¤ì • ì˜ˆì‹œ**:
```python
ENCODING_CONFIG = {
    'onehot_cols': ['gender', 'course', 'internet_access', 'sleep_quality', 
                   'study_method', 'facility_rating', 'exam_difficulty'],
    'ordinal_cols': ['exam_difficulty'],
    'onehot_params': {'handle_unknown': 'ignore'},
    'ordinal_params': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1},
    'drop_original': False  # ì›ë³¸ ì»¬ëŸ¼ ìœ ì§€ (CatBoostìš©)
}
```

#### 2.5 ê²°ì¸¡ì¹˜ ì²˜ë¦¬
- **ìˆ˜ì¹˜í˜• ì»¬ëŸ¼**: í•´ë‹¹ ì»¬ëŸ¼ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´ (`fillna(mean_val)`)
- **ë²”ì£¼í˜• ì»¬ëŸ¼**: 
  - One-Hot: `handle_unknown='ignore'`ë¡œ ì²˜ë¦¬
  - Ordinal: `unknown_value=-1`ë¡œ ì²˜ë¦¬
- **ì²˜ë¦¬ ìˆœì„œ**: Feature Engineering â†’ ì¸ì½”ë”© â†’ ê²°ì¸¡ì¹˜ ì²˜ë¦¬

#### 2.6 ìµœì¢… ë°ì´í„° êµ¬ì¡° ë° íŠ¹ì„± ì¡°í•©

**ì›ë³¸ íŠ¹ì„± ì˜ˆì‹œ** (ê°€ì •):
- ìˆ˜ì¹˜í˜•: `age`, `study_hours`, `class_attendance`, `sleep_hours`
- ë²”ì£¼í˜•: `gender`, `course`, `internet_access`, `sleep_quality`, `study_method`, `facility_rating`, `exam_difficulty`

**Feature Engineering í›„ ìƒì„±ë˜ëŠ” íŠ¹ì„±**:
```
ì›ë³¸ ìˆ˜ì¹˜í˜•: age, study_hours, class_attendance, sleep_hours
â†“
[ì´ìƒì¹˜ í´ë¦¬í•‘] â†’ ë™ì¼í•œ ì»¬ëŸ¼ëª…, ê°’ë§Œ ì œí•œë¨
â†“
[ë¹ˆë„ ì¸ì½”ë”©] â†’ gender_freq, course_freq, internet_access_freq, 
                sleep_quality_freq, study_method_freq, 
                facility_rating_freq, exam_difficulty_freq
â†“
[ìƒí˜¸ì‘ìš© íŠ¹ì„±] â†’ class_attendance_x_sleep_hours,
                  study_hours_x_sleep_hours
```

**ì¸ì½”ë”© í›„ ìµœì¢… íŠ¹ì„± êµ¬ì„±**:

**CatBoost ì…ë ¥ íŠ¹ì„±** (ì˜ˆì‹œ):
```
ìˆ˜ì¹˜í˜• ì›ë³¸:
  - age, study_hours, class_attendance, sleep_hours

Feature Engineering ê²°ê³¼:
  - gender_freq, course_freq, internet_access_freq, ...
  - class_attendance_x_sleep_hours, study_hours_x_sleep_hours

ë²”ì£¼í˜• ì›ë³¸:
  - gender, course, internet_access, sleep_quality, 
    study_method, facility_rating, exam_difficulty

ì´ íŠ¹ì„± ìˆ˜: ìˆ˜ì¹˜í˜•(4) + FE ê²°ê³¼(9) + ë²”ì£¼í˜•(7) = ì•½ 20ê°œ
```

**LightGBM/XGBoost ì…ë ¥ íŠ¹ì„±** (ì˜ˆì‹œ):
```
ìˆ˜ì¹˜í˜• ì›ë³¸:
  - age, study_hours, class_attendance, sleep_hours

Feature Engineering ê²°ê³¼:
  - gender_freq, course_freq, internet_access_freq, ...
  - class_attendance_x_sleep_hours, study_hours_x_sleep_hours

Ordinal ì¸ì½”ë”©ëœ ë²”ì£¼í˜•:
  - exam_difficulty_encoded (Easy=0, Medium=1, Hard=2)

One-Hot ì¸ì½”ë”©ëœ ë²”ì£¼í˜•:
  - gender_Male, gender_Female
  - course_A, course_B, course_C, ...
  - internet_access_Yes, internet_access_No
  - sleep_quality_Good, sleep_quality_Fair, sleep_quality_Poor
  - study_method_Method1, study_method_Method2, ...
  - facility_rating_1, facility_rating_2, ..., facility_rating_5

ì´ íŠ¹ì„± ìˆ˜: ìˆ˜ì¹˜í˜•(4) + FE ê²°ê³¼(9) + Ordinal(1) + One-Hot(ì•½ 20-30ê°œ) = ì•½ 35-45ê°œ
```

**íŠ¹ì„± ì„ íƒ ë¡œì§ ìƒì„¸**:
- `get_features_for_model()` í•¨ìˆ˜ê°€ ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
- CatBoost: ì›ë³¸ ë²”ì£¼í˜• í¬í•¨, ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œì™¸
- LightGBM/XGBoost: ì¸ì½”ë”©ëœ ì»¬ëŸ¼ í¬í•¨, ì›ë³¸ ë²”ì£¼í˜• ì œì™¸

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (`optimize_hyperparameters()`)

#### 3.1 Optuna ìµœì í™” (`use_optuna=True`)

**ìµœì í™” ë°©ë²•**:
- **Sampler**: TPE (Tree-structured Parzen Estimator)
  - ì´ì „ trialì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ trialì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒ
  - Random Searchë³´ë‹¤ íš¨ìœ¨ì ì´ë©° Bayesian Optimization ë°©ì‹
- **Cross-Validation**: 3-Fold (ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ 5-foldì—ì„œ ì¶•ì†Œ)
- **í‰ê°€ ì§€í‘œ**: 
  - Regression: `neg_mean_squared_error` (ìµœì†Œí™”)
  - Classification: `roc_auc` (ìµœëŒ€í™”)

**ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë²”ìœ„**:

**CatBoost**:
```python
{
    'iterations': [100, 500],              # ë¶€ìŠ¤íŒ… ë°˜ë³µ íšŸìˆ˜ (ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ 2000â†’500ìœ¼ë¡œ ì¶•ì†Œ)
    'learning_rate': [0.01, 0.3] (log),    # í•™ìŠµë¥  (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    'depth': [4, 10],                      # íŠ¸ë¦¬ ê¹Šì´
    'l2_leaf_reg': [1, 100] (log),         # L2 ì •ê·œí™” ê³„ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    'random_strength': [0, 1],             # ëœë¤ì„± ê°•ë„
    'bagging_temperature': [0, 1],         # ë°°ê¹… ì˜¨ë„ (Bayesian bootstrapì¼ ë•Œë§Œ)
    'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS']  # ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ì‹
}
```
- **íŠ¹ì§•**: Bootstrap typeì— ë”°ë¼ `bagging_temperature` íŒŒë¼ë¯¸í„° ì¡°ê±´ë¶€ ì‚¬ìš©

**LightGBM**:
```python
{
    'num_leaves': [31, 255],               # ë¦¬í”„ ë…¸ë“œ ìˆ˜
    'learning_rate': [0.01, 0.3] (log),    # í•™ìŠµë¥  (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    'max_depth': [3, 12],                  # ìµœëŒ€ íŠ¸ë¦¬ ê¹Šì´
    'min_data_in_leaf': [20, 200],         # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ë°ì´í„° ìˆ˜
    'feature_fraction': [0.5, 1.0],        # í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨
    'bagging_fraction': [0.5, 1.0],        # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨
    'bagging_freq': [1, 7],                # ë°°ê¹… ë¹ˆë„
    'lambda_l1': [1e-3, 10.0] (log),       # L1 ì •ê·œí™” (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    'lambda_l2': [1e-3, 10.0] (log)        # L2 ì •ê·œí™” (ë¡œê·¸ ìŠ¤ì¼€ì¼)
}
```

**XGBoost**:
```python
{
    'max_depth': [3, 12],                  # ìµœëŒ€ íŠ¸ë¦¬ ê¹Šì´
    'learning_rate': [0.01, 0.3] (log),    # í•™ìŠµë¥  (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    'min_child_weight': [1, 10],           # ìì‹ ë…¸ë“œ ìµœì†Œ ê°€ì¤‘ì¹˜
    'subsample': [0.5, 1.0],               # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨
    'colsample_bytree': [0.5, 1.0],        # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨
    'gamma': [0, 1],                       # ìµœì†Œ ì†ì‹¤ ê°ì†ŒëŸ‰
    'reg_alpha': [1e-3, 10.0] (log),       # L1 ì •ê·œí™” (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    'reg_lambda': [1e-3, 10.0] (log)       # L2 ì •ê·œí™” (ë¡œê·¸ ìŠ¤ì¼€ì¼)
}
```

**ë°ì´í„° ìƒ˜í”Œë§** (`optuna_sample_size`):
- ëª©ì : ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ìµœì í™” ì†ë„ ê°œì„ 
- ë°©ë²•:
  - Classification: Stratified Sampling (í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)
  - Regression: Random Sampling
- ì˜ˆì‹œ: 10ë§Œ í–‰ â†’ 5ë§Œ í–‰ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ì—¬ ìµœì í™” ì†ë„ 2ë°° í–¥ìƒ

**ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§**:
- 1% ê°„ê²©ìœ¼ë¡œ Trial ì§„í–‰ ìƒí™© ì¶œë ¥
- Best Scoreì™€ Current Score ì‹¤ì‹œê°„ í‘œì‹œ
- ì˜ˆ: `Trial 50/50 ì™„ë£Œ | Best Score: 0.123456 | Current Score: 0.123789`

#### 3.2 ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© (`use_saved_params=True`)
- ì´ì „ Optuna ìµœì í™” ê²°ê³¼ë¥¼ JSON íŒŒì¼ì—ì„œ ë¡œë“œ
- íŒŒì¼ í˜•ì‹: `best_hyperparameters.json`
- ê° ëª¨ë¸ë³„ ìµœì  íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ë˜ì–´ ìˆìŒ

#### 3.3 ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš© (`use_saved_params=False`, íŒŒì¼ ì—†ìŒ)
- ì‚¬ì „ ì •ì˜ëœ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
- Optuna ìµœì í™” ì—†ì´ ë¹ ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥

### 4. ëª¨ë¸ í•™ìŠµ (`train_models()`)

ê° ëª¨ë¸ì— ëŒ€í•´ **5-Fold Cross-Validation**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

#### 4.1 ëª¨ë¸ë³„ íŠ¹ì„± ì‚¬ìš© ì „ëµ

**íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ ë¡œì§** (`get_features_for_model()`):

1. **CatBoost**:
   ```python
   feature_cols = [ëª¨ë“  ì»¬ëŸ¼] - [ì¸ì½”ë”©ëœ ì»¬ëŸ¼(*_encoded)]
   ```
   - ì‚¬ìš©: ì›ë³¸ ìˆ˜ì¹˜í˜• + ì›ë³¸ ë²”ì£¼í˜• + Feature Engineering ê²°ê³¼
   - ì œì™¸: One-Hot ì¸ì½”ë”© ì»¬ëŸ¼, Ordinal ì¸ì½”ë”© ì»¬ëŸ¼ (`*_encoded`)
   - ì´ìœ : CatBoostëŠ” ë²”ì£¼í˜•ì„ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¸ì½”ë”© ë¶ˆí•„ìš”
   - `cat_features` ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì— ì „ë‹¬

2. **LightGBM/XGBoost**:
   ```python
   feature_cols = [ëª¨ë“  ì»¬ëŸ¼] - [ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼]
   ```
   - ì‚¬ìš©: ìˆ˜ì¹˜í˜• + ì¸ì½”ë”©ëœ ë²”ì£¼í˜• (`*_encoded`) + One-Hot ì»¬ëŸ¼ + Feature Engineering ê²°ê³¼
   - ì œì™¸: ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ (ì´ë¯¸ ì¸ì½”ë”©ë¨)
   - ì´ìœ : Tree ê¸°ë°˜ ëª¨ë¸ì€ ë²”ì£¼í˜•ì„ ì§ì ‘ ì²˜ë¦¬í•˜ì§€ ëª»í•¨

#### 4.2 CatBoost í•™ìŠµ (`CatBoostModel`)

**íŠ¹ì§•**:
- **ë²”ì£¼í˜• ìë™ ì²˜ë¦¬**: ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë³„ë„ ì¸ì½”ë”© ì—†ì´ ì§ì ‘ ì‚¬ìš©
- **GPU ì§€ì›**: `task_type='GPU'`, `devices='0'`
- **ì¡°ê¸° ì¢…ë£Œ**: Overfitting ë°©ì§€ë¥¼ ìœ„í•œ ìë™ ì¡°ê¸° ì¢…ë£Œ
- **ëª©ì  í•¨ìˆ˜**: 
  - Regression: `RMSE`
  - Classification: `LogLoss`

**í•˜ì´í¼íŒŒë¼ë¯¸í„°** (ìµœì í™”ëœ ì˜ˆì‹œ):
```python
{
    'iterations': 479,
    'learning_rate': 0.271,
    'depth': 7,
    'l2_leaf_reg': 30.58,
    'random_strength': 0.511,
    'bootstrap_type': 'Bernoulli'
}
```

**í•™ìŠµ ê³¼ì •**:
1. ê° Foldë§ˆë‹¤ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
2. `cat_features` ì¸ë±ìŠ¤ ì „ë‹¬ë¡œ ë²”ì£¼í˜• ì²˜ë¦¬
3. ê²€ì¦ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ë° RMSE ê³„ì‚°
4. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ (5ê°œ Fold ëª¨ë¸ì˜ í‰ê· )

#### 4.3 LightGBM í•™ìŠµ (`LightGBMModel`)

**íŠ¹ì§•**:
- **ë¹ ë¥¸ í•™ìŠµ ì†ë„**: íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¹ ë¥¸ í•™ìŠµ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì **: í° ë°ì´í„°ì…‹ë„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
- **GPU ì§€ì›**: `device='gpu'`, `gpu_platform_id=0`, `gpu_device_id=0`
- **ëª©ì  í•¨ìˆ˜**:
  - Regression: `rmse`
  - Classification: `binary_logloss`

**í•˜ì´í¼íŒŒë¼ë¯¸í„°** (ìµœì í™”ëœ ì˜ˆì‹œ):
```python
{
    'num_leaves': 153,
    'learning_rate': 0.096,
    'max_depth': 11,
    'min_data_in_leaf': 90,
    'feature_fraction': 0.631,      # 63% í”¼ì²˜ë§Œ ì‚¬ìš©
    'bagging_fraction': 0.809,      # 81% ë°ì´í„°ë§Œ ì‚¬ìš©
    'bagging_freq': 5,
    'lambda_l1': 0.021,
    'lambda_l2': 0.002
}
```

**í•™ìŠµ ê³¼ì •**:
1. ì¸ì½”ë”©ëœ ë°ì´í„°ë¡œ í•™ìŠµ (ë²”ì£¼í˜•ì€ ì´ë¯¸ ìˆ˜ì¹˜í™”ë¨)
2. Early Stopping ì‚¬ìš© (ê²€ì¦ ë°ì´í„° ê¸°ì¤€)
3. Feature Fractionê³¼ Baggingìœ¼ë¡œ ê³¼ì í•© ë°©ì§€

#### 4.4 XGBoost í•™ìŠµ (`XGBoostModel`)

**íŠ¹ì§•**:
- **ê°•ë ¥í•œ ì •ê·œí™”**: L1, L2 ì •ê·œí™”ë¡œ ê³¼ì í•© ë°©ì§€
- **GPU ì§€ì›**: `tree_method='gpu_hist'`, `device='cuda:0'`
- **ìë™ í´ë°±**: GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ìë™ìœ¼ë¡œ CPUë¡œ ì „í™˜
- **ëª©ì  í•¨ìˆ˜**:
  - Regression: `reg:squarederror`
  - Classification: `binary:logistic`

**í•˜ì´í¼íŒŒë¼ë¯¸í„°** (ìµœì í™”ëœ ì˜ˆì‹œ):
```python
{
    'max_depth': 9,
    'learning_rate': 0.069,
    'min_child_weight': 10,
    'subsample': 0.843,              # 84% í–‰ ìƒ˜í”Œë§
    'colsample_bytree': 0.822,       # 82% ì—´ ìƒ˜í”Œë§
    'gamma': 0.190,
    'reg_alpha': 0.364,              # L1 ì •ê·œí™”
    'reg_lambda': 0.106              # L2 ì •ê·œí™”
}
```

**í•™ìŠµ ê³¼ì •**:
1. DMatrix í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
2. ì¸ì½”ë”©ëœ ë°ì´í„°ë¡œ í•™ìŠµ
3. ê²€ì¦ ë°ì´í„°ë¡œ ì¡°ê¸° ì¢…ë£Œ ì ìš©
4. GPU ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ CPUë¡œ ì „í™˜

#### 4.5 Cross-Validation í”„ë¡œì„¸ìŠ¤

**5-Fold CV êµ¬ì¡°**:
```
ì „ì²´ ë°ì´í„° â†’ [Fold 1, Fold 2, Fold 3, Fold 4, Fold 5]
ê° Foldë§ˆë‹¤:
  - Train Set (80%): ëª¨ë¸ í•™ìŠµ
  - Validation Set (20%): ì„±ëŠ¥ í‰ê°€
  - Test Set: ìµœì¢… ì˜ˆì¸¡
```

**ì¶œë ¥ ê²°ê³¼**:
- **OOF (Out-of-Fold) ì˜ˆì¸¡ê°’**: ê° ìƒ˜í”Œì´ validation setì— ì†í–ˆì„ ë•Œì˜ ì˜ˆì¸¡ê°’
  - 5ê°œ Fold ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„±
- **CV Score**: ê° Foldì˜ ì„±ëŠ¥ í‰ê·  ë° í‘œì¤€í¸ì°¨
  - Regression: RMSE í‰ê·  Â± í‘œì¤€í¸ì°¨
  - Classification: AUC í‰ê·  Â± í‘œì¤€í¸ì°¨
- **Foldë³„ ì ìˆ˜**: ê° Foldì˜ ê°œë³„ ì„±ëŠ¥ í™•ì¸ ê°€ëŠ¥

**ì˜ˆì¸¡ í†µí•©**:
- **í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡**: 5ê°œ Fold ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ í‰ê· í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡
- **OOF ì˜ˆì¸¡**: ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”ì— ì‚¬ìš©

### 5. ì•™ìƒë¸” ëª¨ë¸ ìƒì„± (`create_ensemble()`)

#### 5.1 ì•™ìƒë¸” ë°©ë²•

**Weighted Average (ê°€ì¤‘ í‰ê· )**:
- ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ì—¬ í•©ì‚°
- ê³µì‹: `pred_ensemble = w1*pred_catboost + w2*pred_lightgbm + w3*pred_xgboost`
- ì œì•½ ì¡°ê±´: `w1 + w2 + w3 = 1` (ê°€ì¤‘ì¹˜ í•© = 1)

#### 5.2 ê°€ì¤‘ì¹˜ ìµœì í™” (`_optimize_weights()`)

**ìµœì í™” ë°©ë²•**:
- **ì…ë ¥**: OOF ì˜ˆì¸¡ê°’ (3ê°œ ëª¨ë¸) + ì‹¤ì œ íƒ€ê²Ÿê°’
- **ëª©í‘œ**: 
  - Regression: ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ RMSE ìµœì†Œí™”
  - Classification: ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ AUC ìµœëŒ€í™”
- **ì•Œê³ ë¦¬ì¦˜**: Scipyì˜ `minimize` í•¨ìˆ˜ ì‚¬ìš©
- **ì œì•½ ì¡°ê±´**: 
  - ê°€ì¤‘ì¹˜ í•© = 1
  - ê° ê°€ì¤‘ì¹˜ â‰¥ 0 (ìŒìˆ˜ ê°€ì¤‘ì¹˜ ë°©ì§€)

**ìµœì í™” ê³¼ì •**:
1. ê° ëª¨ë¸ì˜ ê°œë³„ ì„±ëŠ¥ í™•ì¸ ë° ì¶œë ¥
2. ì´ˆê¸° ê°€ì¤‘ì¹˜: ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¹„ë¡€í•˜ì—¬ ì„¤ì •
3. ë°˜ë³µ ìµœì í™”: ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì†Œí™”/ìµœëŒ€í™”í•˜ëŠ” ê°€ì¤‘ì¹˜ íƒìƒ‰
4. ìµœì¢… ê°€ì¤‘ì¹˜: ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì €ì¥

**ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì˜ˆì‹œ**:
```python
{
    'catboost': 0.35,    # 35% ê°€ì¤‘ì¹˜
    'lightgbm': 0.40,    # 40% ê°€ì¤‘ì¹˜
    'xgboost': 0.25      # 25% ê°€ì¤‘ì¹˜
}
```

#### 5.3 ìµœì¢… ì˜ˆì¸¡

**í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡**:
- ê° ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ì— ìµœì  ê°€ì¤‘ì¹˜ ì ìš©
- ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ê³„ì‚°
- ì œì¶œ íŒŒì¼ì— ì €ì¥

### 6. ì œì¶œ íŒŒì¼ ìƒì„±
- **ì¶œë ¥**: `submission.csv` (Kaggle: `/kaggle/working/submission.csv`)
- **í˜•ì‹**: `id`, `exam_score` ì»¬ëŸ¼ í¬í•¨

## âš™ï¸ ì£¼ìš” ì„¤ì • ì˜µì…˜

### GPU ì‚¬ìš© (`USE_GPU`)
```python
USE_GPU = True  # GPU ì‚¬ìš© (CUDA ì§€ì› í•„ìš”)
```
- CatBoost: `task_type='GPU'`, `devices='0'`
- LightGBM: `device='gpu'`
- XGBoost: `tree_method='gpu_hist'`, `device='cuda:0'`
- GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ìë™ìœ¼ë¡œ CPUë¡œ ì „í™˜

### Optuna ìµœì í™” (`USE_OPTUNA`)
```python
USE_OPTUNA = True   # Optuna ìµœì í™” ì‚¬ìš©
N_TRIALS = 50       # ìµœì í™” ì‹œë„ íšŸìˆ˜
```
- ê° ëª¨ë¸ë³„ë¡œ `N_TRIALS`ë§Œí¼ ìµœì í™” ìˆ˜í–‰
- ì†ë„ ìµœì í™”: 3-fold CV, ì‘ì€ iterations ë²”ìœ„

### ë°ì´í„° ìƒ˜í”Œë§ (`OPTUNA_SAMPLE_SIZE`)
```python
OPTUNA_SAMPLE_SIZE = 50000  # 5ë§Œ ê°œë§Œ ìƒ˜í”Œë§
```
- ë°ì´í„°ê°€ ë§ì„ ë•Œ(10ë§Œ í–‰ ì´ìƒ) ì†ë„ ê°œì„ 
- Classification: Stratified Sampling
- Regression: Random Sampling

### ì €ì¥ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© (`USE_SAVED_PARAMS`)
```python
USE_SAVED_PARAMS = None  # ìë™ ê°ì§€
# None: íŒŒì¼ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
# True: ë°˜ë“œì‹œ íŒŒì¼ ì‚¬ìš© (ì—†ìœ¼ë©´ ì˜¤ë¥˜)
# False: íŒŒì¼ ìˆì–´ë„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
```

## ğŸ“Š ì¶œë ¥ ê²°ê³¼

### í•™ìŠµ ê²°ê³¼ ìš”ì•½ (`training_summary.txt`)
- ì‹¤í–‰ ì‹œê°„
- í™˜ê²½ ì •ë³´ (Kaggle/ë¡œì»¬)
- GPU ì‚¬ìš© ì—¬ë¶€
- ëª¨ë¸ë³„ CV Score (í‰ê· , í‘œì¤€í¸ì°¨, Foldë³„ ì ìˆ˜)
- ì•™ìƒë¸” ê°€ì¤‘ì¹˜
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ (`best_hyperparameters.json`)
- ëª¨ë¸ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
- ìµœì í™” ë©”íƒ€ë°ì´í„° (n_trials, random_state ë“±)

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ë¡œì»¬ í™˜ê²½
```python
# main.py ì‹¤í–‰
python main.py

# ë˜ëŠ” kaggle_submission.py ì§ì ‘ ì‹¤í–‰
python kaggle_submission.py
```

### Kaggle Notebook
```python
# kaggle_submission.py íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„
exec(open('/kaggle/working/kaggle_submission.py').read())
```

## ğŸ“ ì£¼ìš” íŒŒì¼ êµ¬ì¡°

```
202601_playground/
â”œâ”€â”€ kaggle_submission.py      # Kaggle ì œì¶œìš© í†µí•© ìŠ¤í¬ë¦½íŠ¸ (ëª¨ë“  ì½”ë“œ í¬í•¨)
â”œâ”€â”€ main.py                    # ë¡œì»¬ í™˜ê²½ ì‹¤í–‰ìš© ë©”ì¸ íŒŒì¼
â”œâ”€â”€ modeling/
â”‚   â”œâ”€â”€ model.py              # ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
â”‚   â””â”€â”€ train.py              # í•™ìŠµ ë¡œì§
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â””â”€â”€ sample_submission.csv
â”œâ”€â”€ best_hyperparameters.json # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ íŒŒì¼
â”œâ”€â”€ submission.csv            # ìµœì¢… ì œì¶œ íŒŒì¼
â””â”€â”€ training_summary.txt      # í•™ìŠµ ê²°ê³¼ ìš”ì•½
```

## ğŸ”§ ì£¼ìš” í´ë˜ìŠ¤ ë° í•¨ìˆ˜

### `HyperparameterOptimizer`
- `optimize_catboost()`: CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- `optimize_lightgbm()`: LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- `optimize_xgboost()`: XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

### `ModelTrainer`
- `train_with_cv()`: K-Fold Cross-Validationìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
- `predict_test()`: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
- `create_model()`: ëª¨ë¸ ìƒì„± (CatBoost/LightGBM/XGBoost)

### `EnsembleModel`
- `fit()`: ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
- `predict()`: ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰

## ğŸ’¡ ì„±ëŠ¥ ìµœì í™” íŒ

1. **ì†ë„ ê°œì„ **:
   - Optuna ìµœì í™” ì‹œ `OPTUNA_SAMPLE_SIZE` ì‚¬ìš© (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
   - GPU ì‚¬ìš© (`USE_GPU = True`)
   - Optuna ìµœì í™”ëŠ” 3-fold CV ì‚¬ìš© (ê¸°ë³¸ í•™ìŠµì€ 5-fold)

2. **ì •í™•ë„ ê°œì„ **:
   - Optuna ìµœì í™”ë¡œ ë” ë§ì€ trials ìˆ˜í–‰ (`N_TRIALS` ì¦ê°€)
   - ë” ë§ì€ ë°ì´í„° ì‚¬ìš© (ìƒ˜í”Œë§ í¬ê¸° ì¦ê°€)
   - Feature Engineering ê°•í™”

3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ì‚¬ìš©**:
   - í•œ ë²ˆ ìµœì í™”í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©
   - `USE_SAVED_PARAMS = True`ë¡œ ì„¤ì •

## ğŸ“ ì°¸ê³ ì‚¬í•­

- **Kaggle í™˜ê²½ ê°ì§€**: `/kaggle/input` ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ë¡œ ìë™ ê°ì§€
- **ë²”ì£¼í˜• ì²˜ë¦¬**: CatBoostëŠ” ì›ë³¸ ì‚¬ìš©, LightGBM/XGBoostëŠ” ì¸ì½”ë”© í•„ìš”
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìƒ˜í”Œë§ ì‚¬ìš© ê¶Œì¥
- **GPU ìë™ ì „í™˜**: GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ìë™ìœ¼ë¡œ CPUë¡œ ì „í™˜

## ğŸ› ë¬¸ì œ í•´ê²°

### GPU ì‚¬ìš© ë¶ˆê°€
- CUDA ì„¤ì¹˜ í™•ì¸: `nvidia-smi` ëª…ë ¹ì–´ë¡œ í™•ì¸
- GPU ì§€ì› íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”
- ìë™ìœ¼ë¡œ CPUë¡œ ì „í™˜ë˜ë¯€ë¡œ ì˜¤ë¥˜ ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥

### ë©”ëª¨ë¦¬ ë¶€ì¡±
- `OPTUNA_SAMPLE_SIZE`ë¡œ ë°ì´í„° í¬ê¸° ì œí•œ
- Feature Engineering ë‹¨ê³„ì—ì„œ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°

### Optuna ìµœì í™” ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
- `N_TRIALS` ê°ì†Œ
- `OPTUNA_SAMPLE_SIZE` ì„¤ì •
- GPU ì‚¬ìš© (`USE_GPU = True`)

## ğŸ“ˆ íŠ¹ì„± ìƒê´€ê´€ê³„ ë° ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„

### ë¶„ì„ ë„êµ¬ ì‚¬ìš©ë²•

#### 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¶„ì„ (ê¶Œì¥)
ì‹¤ì œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ ë¶„ì„:

```python
# analyze_features.py ì‹¤í–‰
python analyze_features.py
```

**ìƒì„±ë˜ëŠ” ê²°ê³¼ë¬¼**:
- `eda_results/correlation_heatmap_preprocessed.png`: ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
- `eda_results/high_correlations_preprocessed.csv`: ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ
- `eda_results/vif_preprocessed.csv`: VIF (ë‹¤ì¤‘ê³µì„ ì„±) ë¶„ì„ ê²°ê³¼
- `eda_results/vif_preprocessed.png`: VIF ë§‰ëŒ€ ê·¸ë˜í”„
- `eda_results/target_correlations_preprocessed.csv`: íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„
- `eda_results/target_correlation_preprocessed.png`: íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ê·¸ë˜í”„
- `eda_results/catboost_high_correlations.csv`: CatBoost íŠ¹ì„± ê°„ ë†’ì€ ìƒê´€ê´€ê³„
- `eda_results/lgbm_xgb_high_correlations.csv`: LightGBM/XGBoost íŠ¹ì„± ê°„ ë†’ì€ ìƒê´€ê´€ê³„

#### 2. ì›ë³¸ ë°ì´í„° ë¶„ì„
ì „ì²˜ë¦¬ ì „ ì›ë³¸ ë°ì´í„°ë¡œ ë¶„ì„:

```python
# eda/correlation_analysis.py ì‹¤í–‰
python eda/correlation_analysis.py
```

**ë¶„ì„ í•­ëª©**:
1. **ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ**: ëª¨ë“  ìˆ˜ì¹˜í˜• íŠ¹ì„± ê°„ì˜ ìƒê´€ê³„ìˆ˜ ì‹œê°í™”
2. **ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ**: ì„ê³„ê°’(0.6, 0.7, 0.8, 0.9) ì´ìƒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ íŠ¹ì„± ìŒ íƒìƒ‰
3. **VIF (Variance Inflation Factor)**: ë‹¤ì¤‘ê³µì„ ì„± ì¸¡ì •
   - VIF < 5: ë‹¤ì¤‘ê³µì„ ì„± ë‚®ìŒ (ì–‘í˜¸)
   - 5 â‰¤ VIF < 10: ë‹¤ì¤‘ê³µì„ ì„± ë³´í†µ (ì£¼ì˜)
   - VIF â‰¥ 10: ë‹¤ì¤‘ê³µì„ ì„± ë†’ìŒ (ë¬¸ì œ, ì œê±° ê³ ë ¤)
4. **íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„**: ê° íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê³„ìˆ˜ ë¶„ì„

### ë¶„ì„ ê²°ê³¼ í•´ì„

#### ìƒê´€ê´€ê³„ í•´ì„
- **|r| < 0.3**: ì•½í•œ ìƒê´€ê´€ê³„
- **0.3 â‰¤ |r| < 0.7**: ì¤‘ê°„ ìƒê´€ê´€ê³„
- **0.7 â‰¤ |r| < 0.9**: ë†’ì€ ìƒê´€ê´€ê³„ (íŠ¹ì„± ì„ íƒ ì‹œ ì£¼ì˜)
- **|r| â‰¥ 0.9**: ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„ (ì¤‘ë³µ íŠ¹ì„± ê°€ëŠ¥ì„±)

#### ë‹¤ì¤‘ê³µì„ ì„± í•´ì„
- **VIF < 5**: ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ì—†ìŒ
- **5 â‰¤ VIF < 10**: ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆìœ¼ë‚˜ í—ˆìš© ê°€ëŠ¥
- **VIF â‰¥ 10**: ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ, íŠ¹ì„± ì œê±° ë˜ëŠ” ë³€í™˜ ê³ ë ¤

### ê¶Œì¥ ì‚¬í•­

1. **ë†’ì€ ìƒê´€ê´€ê³„ (|r| â‰¥ 0.8) íŠ¹ì„± ìŒ ë°œê²¬ ì‹œ**:
   - í•œ íŠ¹ì„±ë§Œ ìœ ì§€í•˜ê±°ë‚˜
   - ë‘ íŠ¹ì„±ì„ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„± (ì˜ˆ: í‰ê· , ì°¨ì´)

2. **ë†’ì€ VIF (VIF â‰¥ 10) íŠ¹ì„± ë°œê²¬ ì‹œ**:
   - í•´ë‹¹ íŠ¹ì„± ì œê±° ê³ ë ¤
   - PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†Œ
   - ì •ê·œí™” ë˜ëŠ” ë³€ìˆ˜ ë³€í™˜

3. **íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ íŠ¹ì„± (|r| < 0.1)**:
   - íŠ¹ì„± ì¤‘ìš”ë„ì™€ í•¨ê»˜ ê²€í† í•˜ì—¬ ì œê±° ê³ ë ¤
   - ë‹¨, ë¹„ì„ í˜• ê´€ê³„ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸ í•„ìš”

## ğŸ”¬ ë°ì´í„° ë¶„ì„ ë° ê°œì„  ë°©í–¥

### í˜„ì¬ êµ¬í˜„ëœ Feature Engineering

1. **ì´ìƒì¹˜ ì²˜ë¦¬ (Clipping)**:
   - ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ê·¹ë‹¨ê°’ì„ ë¶„ìœ„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì œí•œ
   - ì ìš© ì»¬ëŸ¼: `study_hours`, `class_attendance`, `sleep_hours`, `age`
   - **ê°œì„  ê°€ëŠ¥**: IQR ê¸°ë°˜ ì´ìƒì¹˜ ì²˜ë¦¬, Z-score ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°

2. **ë¹ˆë„ ì¸ì½”ë”©**:
   - ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ë¹ˆë„ë¥¼ ìˆ˜ì¹˜í˜• íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜
   - **ê°œì„  ê°€ëŠ¥**: Target Encoding (íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì´ìš©í•œ ì¸ì½”ë”©), Leave-One-Out Encoding

3. **ìƒí˜¸ì‘ìš© íŠ¹ì„±**:
   - ë‘ ë³€ìˆ˜ ê°„ì˜ ê³±ì…ˆ ì—°ì‚°ë§Œ ìˆ˜í–‰
   - í˜„ì¬: `class_attendance Ã— sleep_hours`, `study_hours Ã— sleep_hours`
   - **ê°œì„  ê°€ëŠ¥**: 
     - ë” ë§ì€ ë³€ìˆ˜ ìŒ ì¡°í•© ì‹œë„
     - ë‚˜ëˆ—ì…ˆ, ë§ì…ˆ, ëº„ì…ˆ ì—°ì‚° ì¶”ê°€
     - 3ê°œ ì´ìƒ ë³€ìˆ˜ì˜ ë‹¤í•­ ìƒí˜¸ì‘ìš©

4. **ë¹„ìœ¨ íŠ¹ì„±**:
   - í˜„ì¬ ë¯¸ì‚¬ìš© ìƒíƒœ
   - **ê°œì„  ê°€ëŠ¥**: ì˜ë¯¸ ìˆëŠ” ë¹„ìœ¨ íŠ¹ì„± ì¶”ê°€ (ì˜ˆ: `study_hours / sleep_hours`)

### ì¶”ê°€ë¡œ ê°œì„  ê°€ëŠ¥í•œ Feature Engineering

#### 1. í†µê³„ì  íŠ¹ì„± ìƒì„±
- **ê·¸ë£¹ë³„ í†µê³„ëŸ‰**: 
  - ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ“ê°’, ìµœì†Ÿê°’
  - ì˜ˆ: `courseë³„ study_hours í‰ê· `, `genderë³„ age í‘œì¤€í¸ì°¨`
- **ìˆœìœ„ íŠ¹ì„±**: 
  - ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ìˆœìœ„ ë³€í™˜
  - ì˜ˆ: `study_hours_rank`

#### 2. ì‹œê°„/ìˆœì„œ ê´€ë ¨ íŠ¹ì„± (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
- **ì£¼ê¸°ì„± íŠ¹ì„±**: 
  - ì‹œê°„ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ìš”ì¼, ì›”, ê³„ì ˆ ë“± ì¶”ì¶œ
- **ëˆ„ì  íŠ¹ì„±**: 
  - ê³¼ê±° ë°ì´í„°ê°€ ìˆë‹¤ë©´ ëˆ„ì  í•©, ì´ë™ í‰ê·  ë“±

#### 3. ë²”ì£¼í˜• ì¸ì½”ë”© ê³ ë„í™”
- **Target Encoding**: 
  - íƒ€ê²Ÿ ë³€ìˆ˜ì˜ í‰ê· ê°’ìœ¼ë¡œ ë²”ì£¼ ì¸ì½”ë”©
  - Overfitting ë°©ì§€ë¥¼ ìœ„í•´ Cross-Validation ì‚¬ìš© í•„ìš”
- **Embedding**: 
  - ì‹ ê²½ë§ ê¸°ë°˜ ì„ë² ë”© (CatBoostì˜ ìë™ ì²˜ë¦¬ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ìœ ì—°)

#### 4. íŠ¹ì„± ì„ íƒ (Feature Selection)
- **ìƒê´€ê´€ê³„ ë¶„ì„**: 
  - íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ íŠ¹ì„± ì œê±°
  - íŠ¹ì„± ê°„ ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸ ë° ì²˜ë¦¬
- **ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ**: 
  - ëª¨ë¸ì˜ Feature Importanceë¥¼ í™œìš©í•˜ì—¬ ì¤‘ìš”ë„ê°€ ë‚®ì€ íŠ¹ì„± ì œê±°
  - Permutation Importance ì‚¬ìš©

#### 5. ë°ì´í„° ì¦ê°• (Data Augmentation)
- **SMOTE (Classification)**: 
  - ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œ ì¦ê°•
- **Noise Injection**: 
  - ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì— ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

### ëª¨ë¸ ê°œì„  ë°©í–¥

#### 1. ë‹¤ì–‘í•œ ëª¨ë¸ ì¶”ê°€
- **Neural Network**: 
  - TabNet, NODE (Neural Oblivious Decision Ensembles)
  - ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ì— ê°•ì 
- **Stacking**: 
  - Level 1: CatBoost, LightGBM, XGBoost
  - Level 2: Meta-learner (Linear Regression, Ridge ë“±)

#### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê°œì„ 
- **ë” ë„“ì€ íƒìƒ‰ ê³µê°„**: 
  - í˜„ì¬ iterations 100-500 â†’ ìµœì¢… í•™ìŠµ ì‹œ ë” í° ë²”ìœ„ ì‚¬ìš©
  - Optuna ìµœì í™”ì™€ ìµœì¢… í•™ìŠµì˜ íŒŒë¼ë¯¸í„° ë²”ìœ„ ë¶„ë¦¬
- **Pruning**: 
  - Optunaì˜ Median Pruner ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ì´ ë‚®ì€ trial ì¡°ê¸° ì¢…ë£Œ
  - ì†ë„ ê°œì„  ê°€ëŠ¥

#### 3. Cross-Validation ì „ëµ ê°œì„ 
- **Stratified K-Fold**: 
  - íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•œ ë¶„í•  (Regressionì—ì„œëŠ” quantile-based stratification)
- **Time Series Split**: 
  - ì‹œê°„ ìˆœì„œê°€ ìˆëŠ” ê²½ìš° ì‹œê°„ ê¸°ë°˜ ë¶„í• 

### ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ê°œì„  ì‚¬í•­

#### 1. EDA (Exploratory Data Analysis) ê°•í™”
- **ë¶„í¬ ì‹œê°í™”**: 
  - ê° ë³€ìˆ˜ì˜ íˆìŠ¤í† ê·¸ë¨, ë°•ìŠ¤í”Œë¡¯
  - íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ê´€ê³„ ì‹œê°í™”
- **ìƒê´€ê´€ê³„ ë¶„ì„**: 
  - ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
  - ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸
- **ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„**: 
  - ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì˜ íŠ¹ì§• ë¶„ì„
  - ê²°ì¸¡ì¹˜ê°€ ë¬´ì‘ìœ„ì¸ì§€ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸

#### 2. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
- **ëª¨ë¸ë³„ íŠ¹ì„± ì¤‘ìš”ë„**: 
  - ê° ëª¨ë¸ì˜ Feature Importance ì‹œê°í™”
  - ê³µí†µìœ¼ë¡œ ì¤‘ìš”í•œ íŠ¹ì„± ì‹ë³„
- **SHAP ê°’ ë¶„ì„**: 
  - ê° ì˜ˆì¸¡ì— ëŒ€í•œ íŠ¹ì„± ê¸°ì—¬ë„ ë¶„ì„
  - íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš© íš¨ê³¼ ì‹œê°í™”

#### 3. ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„
- **ì”ì°¨ ë¶„ì„**: 
  - ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ë¶„í¬ í™•ì¸
  - íŠ¹ì • ë²”ìœ„ì—ì„œ ì˜¤ì°¨ê°€ í°ì§€ í™•ì¸
- **ì˜ëª» ì˜ˆì¸¡í•œ ìƒ˜í”Œ ë¶„ì„**: 
  - ì˜¤ì°¨ê°€ í° ìƒ˜í”Œì˜ íŠ¹ì§• ë¶„ì„
  - íŒ¨í„´ ë°œê²¬ ë° íŠ¹ì„± ì¶”ê°€

#### 4. ê²€ì¦ ì „ëµ ê°œì„ 
- **Hold-out Set**: 
  - ìµœì¢… ê²€ì¦ì„ ìœ„í•œ ë³„ë„ Hold-out Set í™•ë³´
  - Public/Private Leaderboard ì°¨ì´ ì˜ˆì¸¡
- **ì‹œê°„ ê¸°ë°˜ ê²€ì¦**: 
  - ë°ì´í„°ì— ì‹œê°„ ì •ë³´ê°€ ìˆë‹¤ë©´ ì‹œê°„ ê¸°ë°˜ ê²€ì¦

### ì½”ë“œ êµ¬ì¡° ê°œì„ 

#### 1. ëª¨ë“ˆí™” ê°•í™”
- **Config íŒŒì¼ ë¶„ë¦¬**: 
  - í•˜ì´í¼íŒŒë¼ë¯¸í„°, Feature Engineering ì„¤ì •ì„ ë³„ë„ íŒŒì¼ë¡œ ê´€ë¦¬
- **Pipeline êµ¬ì¡°**: 
  - scikit-learnì˜ Pipeline ì‚¬ìš©ìœ¼ë¡œ ì „ì²˜ë¦¬ì™€ í•™ìŠµ í†µí•©

#### 2. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
- **ìì„¸í•œ ë¡œê·¸**: 
  - ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- **ì‹¤í—˜ ì¶”ì **: 
  - MLflow, Weights & Biases ë“±ìœ¼ë¡œ ì‹¤í—˜ ê²°ê³¼ ì¶”ì 

#### 3. ì¬í˜„ì„± ê°•í™”
- **Seed ê³ ì •**: 
  - ëª¨ë“  ëœë¤ ì—°ì‚°ì— seed ì„¤ì •
  - í™˜ê²½ë³„ ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡

### ì„±ëŠ¥ ê°œì„  ìš°ì„ ìˆœìœ„

1. **ë†’ì€ ìš°ì„ ìˆœìœ„** (ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥):
   - Target Encoding ì¶”ê°€
   - ë” ë§ì€ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±
   - íŠ¹ì„± ì¤‘ìš”ë„ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ

2. **ì¤‘ê°„ ìš°ì„ ìˆœìœ„** (ì‹œê°„ íˆ¬ì í•„ìš”):
   - Stacking ëª¨ë¸ ì¶”ê°€
   - SHAP ê°’ ë¶„ì„ìœ¼ë¡œ íŠ¹ì„± ì´í•´ë„ í–¥ìƒ
   - Hold-out Setìœ¼ë¡œ ê²€ì¦ ì „ëµ ê°œì„ 

3. **ë‚®ì€ ìš°ì„ ìˆœìœ„** (ì¥ê¸° ê°œì„ ):
   - Neural Network ëª¨ë¸ ì¶”ê°€
   - ìë™í™”ëœ Feature Engineering íŒŒì´í”„ë¼ì¸
   - ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•

## ğŸ“Š Feature Importance ë¶„ì„

### ìë™ ë¶„ì„ ê¸°ëŠ¥

ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ë©´ **ìë™ìœ¼ë¡œ Feature Importance ë¶„ì„**ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤.

**ë¶„ì„ ë‚´ìš©**:
1. **ëª¨ë¸ë³„ Feature Importance**: CatBoost, LightGBM, XGBoost ê° ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
2. **Foldë³„ í‰ê· **: 5-Fold CVì˜ ëª¨ë“  Foldì—ì„œ ì¶”ì¶œí•œ ì¤‘ìš”ë„ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
3. **ì‹œê°í™”**: ëª¨ë¸ë³„ ìƒìœ„ íŠ¹ì„± ë¹„êµ ê·¸ë˜í”„ ìƒì„±
4. **ê³µí†µ ì¤‘ìš” íŠ¹ì„±**: ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•œ íŠ¹ì„± ì‹ë³„

### ìƒì„±ë˜ëŠ” ê²°ê³¼ë¬¼

`feature_importance_results/` ë””ë ‰í† ë¦¬ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

1. **CSV íŒŒì¼**:
   - `catboost_feature_importance.csv`: CatBoost ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„
   - `lightgbm_feature_importance.csv`: LightGBM ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„
   - `xgboost_feature_importance.csv`: XGBoost ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„
   - `common_important_features.csv`: ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì¤‘ìš”í•œ íŠ¹ì„±

2. **ì‹œê°í™” íŒŒì¼**:
   - `feature_importance_comparison.png`: ëª¨ë¸ë³„ ìƒìœ„ íŠ¹ì„± ë¹„êµ (ê°œë³„ ê·¸ë˜í”„)
   - `feature_importance_combined.png`: ëª¨ë“  ëª¨ë¸ì˜ ìƒìœ„ íŠ¹ì„±ì„ í•˜ë‚˜ì˜ ê·¸ë˜í”„ë¡œ ë¹„êµ

### Feature Importance í•´ì„

**ì¤‘ìš”ë„ ê°’ì˜ ì˜ë¯¸**:
- ê° ëª¨ë¸ì˜ ì¤‘ìš”ë„ëŠ” **ì •ê·œí™”**ë˜ì–´ í•©ì´ 1ì´ ë©ë‹ˆë‹¤.
- ë†’ì€ ì¤‘ìš”ë„ = í•´ë‹¹ íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ë” ë§ì´ ê¸°ì—¬
- í‘œì¤€í¸ì°¨(Std) = Fold ê°„ ì¤‘ìš”ë„ì˜ ë³€ë™ì„± (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )

**í™œìš© ë°©ë²•**:
1. **íŠ¹ì„± ì„ íƒ**: ì¤‘ìš”ë„ê°€ ë‚®ì€ íŠ¹ì„± ì œê±° ê³ ë ¤
2. **íŠ¹ì„± Engineering**: ì¤‘ìš”ë„ê°€ ë†’ì€ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±
3. **ëª¨ë¸ í•´ì„**: ì–´ë–¤ íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì´í•´
4. **ê³µí†µ ì¤‘ìš” íŠ¹ì„±**: ëª¨ë“  ëª¨ë¸ì—ì„œ ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•œ íŠ¹ì„±ì€ í•µì‹¬ íŠ¹ì„±ìœ¼ë¡œ ê°„ì£¼

### Feature Importanceì™€ ìƒê´€ê´€ê³„ ë¶„ì„ì˜ ê´€ê³„

- **ìƒê´€ê´€ê³„ ë¶„ì„**: ì„ í˜• ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” **ì „ì²˜ë¦¬ ë‹¨ê³„**ì˜ ë¶„ì„
- **Feature Importance**: ì‹¤ì œ ëª¨ë¸ì´ í•™ìŠµí•œ **ë¹„ì„ í˜• ê´€ê³„ í¬í•¨**í•œ ì¤‘ìš”ë„

**ì˜ˆì‹œ**:
- ìƒê´€ê´€ê³„ê°€ ë‚®ì§€ë§Œ Feature Importanceê°€ ë†’ì€ íŠ¹ì„± â†’ ë¹„ì„ í˜• ê´€ê³„ ë˜ëŠ” ìƒí˜¸ì‘ìš© íš¨ê³¼
- ìƒê´€ê´€ê³„ê°€ ë†’ì§€ë§Œ Feature Importanceê°€ ë‚®ì€ íŠ¹ì„± â†’ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ë˜ëŠ” ì¤‘ë³µ íŠ¹ì„±

ë‘ ë¶„ì„ì„ **í•¨ê»˜ ê³ ë ¤**í•˜ì—¬ ìµœì¢… íŠ¹ì„± ì„ íƒì„ ê²°ì •í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.
