1회차 
Logistic Regression - Accuracy: 0.8982, AUC: 0.8896
LightGBM - Accuracy: 0.9118, AUC: 0.9220
XGBoost - Accuracy: 0.9103, AUC: 0.9205
Ensemble - Accuracy: 0.9097, AUC: 0.9152


2회차 - 모델 변경 CATB
catboost - Accuracy: 0.9122, AUC: 0.9203
LightGBM - Accuracy: 0.9122, AUC: 0.9220
XGBoost - Accuracy: 0.9102, AUC: 0.9199
Ensemble - Accuracy: 0.9118, AUC: 0.9213


3회차 - feature_engineering 제거
catboost - Accuracy: 0.9121, AUC: 0.9200
LightGBM - Accuracy: 0.9121, AUC: 0.9218
XGBoost - Accuracy: 0.9102, AUC: 0.9201
Ensemble - Accuracy: 0.9119, AUC: 0.9213


4회차 - 신규 feature 생성
df["head_grade"] = df["grade_subgrade"].astype(str).str.split('_').str[0]
df["sub_grade"] = df["grade_subgrade"].astype(str).str.split('_').str[1]
df["loan_amount_div_income"] = df["loan_amount"].astype(float) / (df["annual_income"].astype(float)+ 1e-6)
df["loan_amount_log"] = np.log1p(df["loan_amount"])
df["interest_rate_log"] = np.log1p(df["interest_rate"])
df["annual_income_log"] = np.log1p(df["annual_income"])

catboost - Accuracy: 0.9118, AUC: 0.9194
Ensemble - Accuracy: 0.9117, AUC: 0.9213
XGBoost - Accuracy: 0.9104, AUC: 0.9202
Ensemble - Accuracy: 0.9117, AUC: 0.9213


5회차
- 신규 feature에 대해서 정상적으로 모델에 전달 되도록 로직 변경
- 이상치 제거하는 순서를 feature_engineering이 종료된 이후에 다 같이 진행

catboost - Accuracy: 0.9121, AUC: 0.9201
LightGBM - Accuracy: 0.9119, AUC: 0.9218
XGBoost - Accuracy: 0.9101, AUC: 0.9194
Ensemble - Accuracy: 0.9117, AUC: 0.9210


6회차
- log 변환 이후 중복되는 컬럼 삭제
- interest_rate_to_dti 컬럼 추가

catboost - Accuracy: 0.9117, AUC: 0.9197
LightGBM - Accuracy: 0.9119, AUC: 0.9221
XGBoost - Accuracy: 0.9097, AUC: 0.9194
Ensemble - Accuracy: 0.9116, AUC: 0.9211


7회차 여러가지 feature 추가
catboost - Accuracy: 0.9115, AUC: 0.9192
LightGBM - Accuracy: 0.9112, AUC: 0.9207
XGBoost - Accuracy: 0.9100, AUC: 0.9189
Ensemble - Accuracy: 0.9111, AUC: 0.9204


8회차 importance 낮은 컬럼들 제거 / threshold 감소 0.5 -> 0.3
catboost - Accuracy: 0.9116, AUC: 0.9191
LightGBM - Accuracy: 0.9114, AUC: 0.9211
XGBoost - Accuracy: 0.9099, AUC: 0.9191
Ensemble - Accuracy: 0.9066, AUC: 0.9203


9회차 threshold 감소 0.3 -> 0.25
catboost - Accuracy: 0.9115, AUC: 0.9190
LightGBM - Accuracy: 0.9114, AUC: 0.9215
XGBoost - Accuracy: 0.9100, AUC: 0.9192
Ensemble - Accuracy: 0.9055, AUC: 0.9205


10회차 
- monthly_income, debt_to_monthly_income, monthly_income_interest_amount
- 하이퍼 파라메터 고정
catboost - Accuracy: 0.9118, AUC: 0.9193
LightGBM - Accuracy: 0.9115, AUC: 0.9212
XGBoost - Accuracy: 0.9098, AUC: 0.9193
Ensemble - Accuracy: 0.9116, AUC: 0.9205

catboost - Accuracy: 0.9117, AUC: 0.9197
LightGBM - Accuracy: 0.9114, AUC: 0.9213
XGBoost - Accuracy: 0.9098, AUC: 0.9193
Ensemble - Accuracy: 0.9113, AUC: 0.9207


11회차 이상치 비비제거 / threshold 0.5 -> 0.3
-> 이상치 제거 이유 (paid, non-paid 비율이 한쪽으로 치우쳐져 있으므로
이상치를 제거하는 경우(이상치에 non-paid의 비율이 높은 경우 데이터 편향은 더 심해질 수 밖에  없음)
이상치 데이터 중에서 non-paid의 비율을 확인하고 outlier에 대한 별도의 튜닝을 진행하거나
이상치 제거 과정 자체를 제외 시키는 것도 하나의 방법으로 볼 수 있을 것
catboost - Accuracy: 0.9058, AUC: 0.9219
LightGBM - Accuracy: 0.9056, AUC: 0.9233
XGBoost - Accuracy: 0.9047, AUC: 0.9219
Ensemble - Accuracy: 0.8985, AUC: 0.9229


12회차 목표 -> not-paid recall 수치에 대한 정확도 향상
- 추가적인 feature_engineering을 통해 명확한 기준을 만들어 보기